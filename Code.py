# -*- coding: utf-8 -*-
"""
Created on Mon Apr 14 16:33:10 2025

@author: andre
"""


#================================================================================
#================================================================================
# Code for paper "Knowledge interdependencies between metal-ion battery chemistries"
#================================================================================
#================================================================================


#==========================================================================================================================================
# Data preprocessing:
    # Data upload: Reading lens data Abisheks core patents, PATSTAT supplementary info data Abisheks core patents, and lens data Leos query
#==========================================================================================================================================

import os
import pandas as pd
from collections import Counter

# Helper function for comparing datasets
def compare_datasets(dataset_a, dataset_b, id_column="Simple Family Members", label=""):
    """
    Compares two datasets and provides information about common entries.
    
    Args:
        dataset_a: First dataset
        dataset_b: Second dataset
        id_column: Column name for comparison
        label: Descriptive text for output
    
    Returns:
        missing_entries: Entries from dataset_a that are not in dataset_b
    """
    common_ids = dataset_a[id_column].isin(dataset_b[id_column])
    num_common = common_ids.sum()
    total = len(dataset_a)
    percentage_common = (num_common / total) * 100
    
    # Extract missing documents
    missing_entries = dataset_a[~common_ids]
    
    # Output results
    print(f"{label} - Number of common entries: {num_common}")
    print(f"{label} - Percentage: {percentage_common:.2f}%")
    
    return missing_entries

# Paths to datasets
path_lens_Abishek = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Abishek_core_patents_lens"
path_lens_Leo = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leos_query_origdata"
path_lens_Leo_full_data = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data"
path_lens_Leo_IPC = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\full query IPC"
path_lens_Leo_CPC = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\full query CPC"

# 1. Load datasets
# 1.1 Abishek dataset
lens_Abishek = pd.read_excel(os.path.join(path_lens_Abishek, "lens_Abishek_core_patents.xlsx"), sheet_name="Tabelle1")

# 1.2 Leo dataset (filtered)
csv_file = [f for f in os.listdir(path_lens_Leo) if f.endswith('.csv')][0]
lens_Leo_simplefam_grant_forwcit_triad = pd.read_csv(os.path.join(path_lens_Leo, csv_file), encoding='utf-8')

# 1.3 Leo dataset (complete)
lens_Leo_full_data = pd.read_excel(os.path.join(path_lens_Leo_full_data, "Leo_full_dataset.xlsx"), sheet_name="Tabelle1")
lens_Leo_full_data = lens_Leo_full_data.drop_duplicates(subset=["Lens ID"], keep="first")

# 1.4 Leo IPC dataset
lens_Leo_IPC = pd.read_excel(os.path.join(path_lens_Leo_IPC, "Lens-full-data-Leo-IPC-20250228.xlsx"), sheet_name="Tabelle1")
lens_Leo_IPC["Earliest Priority Date"] = pd.to_datetime(lens_Leo_IPC["Earliest Priority Date"], errors='coerce')
lens_Leo_IPC = lens_Leo_IPC[lens_Leo_IPC["Earliest Priority Date"] < "2023-01-01"]

# 1.5 Leo CPC dataset
lens_Leo_CPC = pd.read_excel(os.path.join(path_lens_Leo_CPC, "Lens-full-data-Leo-CPC-20250228.xlsx"), sheet_name="Tabelle1")
lens_Leo_CPC["Earliest Priority Date"] = pd.to_datetime(lens_Leo_CPC["Earliest Priority Date"], errors='coerce')
lens_Leo_CPC = lens_Leo_CPC[lens_Leo_CPC["Earliest Priority Date"] < "2023-01-01"]

# 2. Dataset comparisons
# 2.1 Check Abishek vs. Leo Full
missing_core_patents = compare_datasets(lens_Abishek, lens_Leo_full_data, label="Abishek vs. Leo Full")

# 2.2 Check Leo Filtered vs. Leo Full
missing_patents_Leo = compare_datasets(lens_Leo_simplefam_grant_forwcit_triad, lens_Leo_full_data, 
                                     label="Leo Filtered vs. Leo Full")

# Analysis of missing datasets (CPC and IPC codes)
cpc_codes = Counter(code for codes in missing_patents_Leo["CPC Classifications"].dropna() for code in codes.split(";;"))
ipc_codes = Counter(code for codes in missing_patents_Leo["IPCR Classifications"].dropna() for code in codes.split(";;"))

# Create DataFrames for the counts
cpc_counts_missing = pd.DataFrame(cpc_codes.items(), columns=["CPC Code", "Count"])
ipc_counts_missing = pd.DataFrame(ipc_codes.items(), columns=["IPC Code", "Count"])

# 3. Comparison between IPC and CPC datasets
missing_CPC = compare_datasets(lens_Leo_CPC, lens_Leo_IPC, label="CPC in IPC")
missing_IPC = compare_datasets(lens_Leo_IPC, lens_Leo_CPC, label="IPC in CPC")

# 4. Create combined IPC and CPC dataset
lens_Leo_IPC_CPC = pd.concat([lens_Leo_IPC, missing_CPC], ignore_index=True)
lens_Leo_IPC_CPC = lens_Leo_IPC_CPC.drop_duplicates(subset="Simple Family Members", keep="first")

# 5. Check Abishek vs. Combined dataset
missing_core_patents_Abishek_in_full_data = compare_datasets(lens_Abishek, lens_Leo_IPC_CPC, 
                                                           label="Abishek vs. IPC_CPC Combined")

# 6. Add missing patents from Abishek to the combined dataset
missing_patents = lens_Abishek[~lens_Abishek["Simple Family Members"].isin(lens_Leo_IPC_CPC["Simple Family Members"])]
lens_Leo_IPC_CPC = pd.concat([lens_Leo_IPC_CPC, missing_patents], ignore_index=True)
print(f"New size of lens_Leo_IPC_CPC after addition: {lens_Leo_IPC_CPC.shape}")

# 7. Filter by forward citations
lens_Leo_IPC_CPC_forwcit = lens_Leo_IPC_CPC[lens_Leo_IPC_CPC["Cited by Patent Count"] > 0]
missing_core_patents_Abishek_in_forwcit_full_data = compare_datasets(lens_Abishek, lens_Leo_IPC_CPC_forwcit, 
                                                                    label="Abishek vs. IPC_CPC with citations")

# 8. Apply triadic filter (patents that were filed in at least one of the target countries)
target_countries = {"EP", "JP", "US"}
lens_Leo_IPC_CPC_triadic = lens_Leo_IPC_CPC[
    lens_Leo_IPC_CPC["Simple Family Member Jurisdictions"]
    .apply(lambda x: any(country in target_countries for country in str(x).split(";;")))
]

# 9. Check after triadic filter
missing_core_patents_Abishek_in_triadic_full_data = compare_datasets(lens_Abishek, lens_Leo_IPC_CPC_triadic, 
                                                                    label="Abishek vs. Triadic Filter")

# Additional analysis of missing patents
num_missing_with_target_countries = missing_core_patents_Abishek_in_triadic_full_data["Simple Family Member Jurisdictions"]\
    .apply(lambda x: any(country in target_countries for country in str(x).split(";;"))).sum()
print(f"Number of 'missing' patents that still contain a target country: {num_missing_with_target_countries}")

# 10. Combined filter: Triadic AND forward citations
lens_Leo_IPC_CPC_triadicforwcit = lens_Leo_IPC_CPC_triadic[lens_Leo_IPC_CPC_triadic["Cited by Patent Count"] > 0]
missing_core_patents_Abishek_in_triadicforwcit_full_data = compare_datasets(lens_Abishek, lens_Leo_IPC_CPC_triadicforwcit, 
                                                                           label="Abishek vs. Triadic+Citations")

# 11. Extract and save unique patent IDs
unique_patent_ids_triadic_one_out_of_three = set(
    patent_id.strip()
    for ids in lens_Leo_IPC_CPC_triadic["Simple Family Members"].dropna()
    for patent_id in ids.split(";;")
)

# Save the set of unique_patent_ids_triadic_one_out_of_three
pd.DataFrame({"Patent ID": list(unique_patent_ids_triadic_one_out_of_three)}).to_csv(
    "unique_patent_ids_triadic_one_out_of_three.csv", index=False)

# Save the DataFrame as an Excel file
output_path = "C:\\Users\\andre\\OneDrive\\0. Promotion\\Eigene Forschung\\ETH Zürich\\Dataset\\Lens\\Leo_full_data\\API data\\lens_Leo_IPC_CPC_triadic.xlsx"
lens_Leo_IPC_CPC_triadic.to_excel(output_path, index=False)


#%%

# Check if the json files include all lens_ids

import json
import pandas as pd
import os

# get files from a folder (id only)
fetch_folder_csv = 'C:\\Users\\andre\\OneDrive\\0. Promotion\\Eigene Forschung\\ETH Zürich\\Dataset\\Lens\\Leo_full_data\\API data\\CSV data'
files_id = [f for f in os.listdir(fetch_folder_csv) if f.endswith('.csv')]

# get lens_ids from each file
my_lens_ids = []
for file in files_id:
    file_path = os.path.join(fetch_folder_csv, file)  # Full path to the CSV file

    #Read the CSV file and extract 'lens_ids' column
    ids_missing_fam_temp = pd.read_csv(file_path)
    ids_missing_fam_temp = ids_missing_fam_temp['lens_ids'].tolist()  # Convert to list

    my_lens_ids = ids_missing_fam_temp + my_lens_ids

print(f'Unique Lens_ids searched:{len(set(my_lens_ids))}')

# get files from a folder (JSON info)
fetch_folder_csv = 'C:\\Users\\andre\\OneDrive\\0. Promotion\\Eigene Forschung\\ETH Zürich\\Dataset\\Lens\\Leo_full_data\\API data\\All JSON'
files_id = [f for f in os.listdir(fetch_folder_csv) if f.endswith('.json')]

info_lens_ids = []
for file in files_id:
    file_path = os.path.join(fetch_folder_csv, file)  # Full path to the CSV file
    with open(file_path, "r") as file:
        data = json.load(file)
    for patents in data:
        lens_id = str(patents.get("lens_id", None)) #to have everything in the dataframe as a string type
        info_lens_ids.append(lens_id)

print(f'Unique Lens_ids data collected for:{len(set(info_lens_ids))}')

not_collected_ids = list(set(my_lens_ids) - set(info_lens_ids))
print(f'IDs not collected:{len(not_collected_ids)}')

output_folder = r'C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\All JSON\missing ids'
batch_size = 10000
for i in range(0, len(not_collected_ids), batch_size):
    batch = not_collected_ids[i:i + batch_size]
    output_file = os.path.join(output_folder, f're_unique_patent_ids_triadic_one_out_of_three_{i//batch_size + 1}.csv')

    # Save batch to CSV
    df = pd.DataFrame(batch, columns=['lens_ids'])
    df.to_csv(output_file, index=False)

    print(f"Saved {len(batch)} lens_ids to {output_file}")



#%%


# Extract full dataset from json files (downloaded via API) into Excel file

import json
import pandas as pd
import numpy as np
import os
from pathlib import Path

# File paths
# Using raw string to avoid escape sequence issues
input_folder = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\All JSON"  # Path to folder containing JSON files
output_excel = "full_LIB_data_all_family_members.xlsx"  # Name of the output file

def extract_nested_value(obj, key_path, default=None):
    """Extracts values from nested dictionaries using a path of keys"""
    current = obj
    
    for key in key_path:
        if isinstance(current, dict) and key in current:
            current = current[key]
        else:
            return default
    
    return current

def extract_list_values(obj, key_path, extract_key=None, join_with=", "):
    """Extracts values from lists in nested dictionaries"""
    values = extract_nested_value(obj, key_path)
    
    if not isinstance(values, list):
        return None
    
    if extract_key:
        extracted = []
        for item in values:
            if isinstance(item, dict):
                if isinstance(extract_key, list):
                    value = extract_nested_value(item, extract_key)
                    if value:
                        extracted.append(value)
                else:
                    if extract_key in item:
                        extracted.append(item[extract_key])
        return join_with.join(extracted) if extracted else None
    else:
        return join_with.join(str(item) for item in values)

def extract_patent_data(patent):
    """Extracts relevant patent data from a patent object"""
    patent_data = {}
    
    # Basic information
    patent_data['lens_id'] = patent.get('lens_id')
    patent_data['date_published'] = patent.get('date_published')
    
    biblio = patent.get('biblio', {})
    
    # Publication reference
    pub_ref = extract_nested_value(biblio, ['publication_reference'], {})
    patent_data['pub_jurisdiction'] = extract_nested_value(pub_ref, ['jurisdiction'])
    patent_data['pub_doc_number'] = extract_nested_value(pub_ref, ['doc_number'])
    patent_data['pub_kind'] = extract_nested_value(pub_ref, ['kind'])
    patent_data['pub_date'] = extract_nested_value(pub_ref, ['date'])
    
    # Application reference
    app_ref = extract_nested_value(biblio, ['application_reference'], {})
    patent_data['app_jurisdiction'] = extract_nested_value(app_ref, ['jurisdiction'])
    patent_data['app_doc_number'] = extract_nested_value(app_ref, ['doc_number'])
    patent_data['app_kind'] = extract_nested_value(app_ref, ['kind'])
    patent_data['app_date'] = extract_nested_value(app_ref, ['date'])
    
    # Priority date and priority claims
    priority_claims = extract_nested_value(biblio, ['priority_claims'], {})
    patent_data['earliest_priority_date'] = extract_nested_value(priority_claims, ['earliest_claim', 'date'])
    
    # Extract all priority claims
    priority_claims_list = extract_nested_value(priority_claims, ['claims'], [])
    priority_claims_details = []
    
    for claim in priority_claims_list:
        jurisdiction = extract_nested_value(claim, ['jurisdiction'])
        doc_number = extract_nested_value(claim, ['doc_number'])
        kind = extract_nested_value(claim, ['kind'])
        date = extract_nested_value(claim, ['date'])
        sequence = extract_nested_value(claim, ['sequence'])
        
        if jurisdiction and doc_number:
            priority_claim_detail = f"{jurisdiction}{doc_number}"
            if kind:
                priority_claim_detail += f" ({kind})"
            if date:
                priority_claim_detail += f" [{date}]"
            if sequence:
                priority_claim_detail += f" Seq:{sequence}"
            
            priority_claims_details.append(priority_claim_detail)
    
    patent_data['priority_claims'] = ", ".join(priority_claims_details) if priority_claims_details else None
    
    # Title
    titles = extract_nested_value(biblio, ['invention_title'], [])
    if titles:
        for title in titles:
            if extract_nested_value(title, ['lang']) == 'en':
                patent_data['title_en'] = extract_nested_value(title, ['text'])
            else:
                patent_data['title_other'] = extract_nested_value(title, ['text'])
    
    # Parties
    parties = extract_nested_value(biblio, ['parties'], {})
    
    # Applicants
    applicants = extract_nested_value(parties, ['applicants'], [])
    applicant_names = []
    applicant_countries = []
    
    for applicant in applicants:
        if 'extracted_name' in applicant and 'value' in applicant['extracted_name']:
            applicant_names.append(applicant['extracted_name']['value'])
        if 'residence' in applicant:
            applicant_countries.append(applicant['residence'])
    
    patent_data['applicants'] = ", ".join(applicant_names) if applicant_names else None
    patent_data['applicant_countries'] = ", ".join(applicant_countries) if applicant_countries else None
    
    # Inventors
    inventors = extract_nested_value(parties, ['inventors'], [])
    inventor_names = []
    inventor_countries = []
    
    for inventor in inventors:
        if 'extracted_name' in inventor and 'value' in inventor['extracted_name']:
            inventor_names.append(inventor['extracted_name']['value'])
        if 'residence' in inventor:
            inventor_countries.append(inventor['residence'])
    
    patent_data['inventors'] = ", ".join(inventor_names) if inventor_names else None
    patent_data['inventor_countries'] = ", ".join(inventor_countries) if inventor_countries else None
    
    # Agents/Attorneys
    agents = extract_nested_value(parties, ['agents'], [])
    agent_names = []
    
    for agent in agents:
        if 'extracted_name' in agent and 'value' in agent['extracted_name']:
            agent_names.append(agent['extracted_name']['value'])
    
    patent_data['agents'] = ", ".join(agent_names) if agent_names else None
    
    # Extract examiners
    examiners = extract_nested_value(biblio, ['examiners'], {})
    primary_examiner = extract_nested_value(examiners, ['primary_examiner', 'extracted_name', 'value'])
    if primary_examiner:
        patent_data['primary_examiner'] = primary_examiner
    
    # Classifications
    # IPC
    ipc_classifications = extract_nested_value(biblio, ['classifications_ipcr', 'classifications'], [])
    ipc_symbols = []
    
    for cls in ipc_classifications:
        if 'symbol' in cls:
            ipc_symbols.append(cls['symbol'])
    
    patent_data['ipc_classifications'] = ", ".join(ipc_symbols) if ipc_symbols else None
    
    # CPC
    cpc_classifications = extract_nested_value(biblio, ['classifications_cpc', 'classifications'], [])
    cpc_symbols = []
    
    for cls in cpc_classifications:
        if 'symbol' in cls:
            cpc_symbols.append(cls['symbol'])
    
    patent_data['cpc_classifications'] = ", ".join(cpc_symbols) if cpc_symbols else None
    
    # National classifications
    nat_classifications = extract_nested_value(biblio, ['classifications_national', 'classifications'], [])
    nat_symbols = []
    
    for cls in nat_classifications:
        if 'symbol' in cls:
            nat_symbols.append(cls['symbol'])
    
    patent_data['national_classifications'] = ", ".join(nat_symbols) if nat_symbols else None
    
    # Cited patents
    citations = extract_nested_value(biblio, ['references_cited', 'citations'], [])
    patent_citations = []
    cited_lens_ids = []
    
    for citation in citations:
        patcit = extract_nested_value(citation, ['patcit'], {})
        doc_id = extract_nested_value(patcit, ['document_id'], {})
        lens_id = extract_nested_value(patcit, ['lens_id'])
        
        if doc_id:
            jurisdiction = extract_nested_value(doc_id, ['jurisdiction'])
            doc_number = extract_nested_value(doc_id, ['doc_number'])
            kind = extract_nested_value(doc_id, ['kind'])
            date = extract_nested_value(doc_id, ['date'])
            
            if jurisdiction and doc_number:
                citation_text = f"{jurisdiction}{doc_number}"
                if kind:
                    citation_text += f" ({kind})"
                if date:
                    citation_text += f" [{date}]"
                patent_citations.append(citation_text)
        
        if lens_id:
            cited_lens_ids.append(lens_id)
    
    patent_data['cited_patents'] = ", ".join(patent_citations) if patent_citations else None
    patent_data['cited_lens_ids'] = ", ".join(cited_lens_ids) if cited_lens_ids else None
    patent_data['cited_patents_count'] = extract_nested_value(biblio, ['references_cited', 'patent_count'])
    patent_data['cited_npl_count'] = extract_nested_value(biblio, ['references_cited', 'npl_count'])
    
    # Citing patents (Cited by)
    citing_patents = extract_nested_value(biblio, ['cited_by', 'patents'], [])
    citing_patent_ids = []
    citing_lens_ids = []
    
    for citing in citing_patents:
        doc_id = extract_nested_value(citing, ['document_id'], {})
        lens_id = extract_nested_value(citing, ['lens_id'])
        
        if doc_id:
            jurisdiction = extract_nested_value(doc_id, ['jurisdiction'])
            doc_number = extract_nested_value(doc_id, ['doc_number'])
            kind = extract_nested_value(doc_id, ['kind'])
            date = extract_nested_value(doc_id, ['date'])
            
            if jurisdiction and doc_number:
                citing_text = f"{jurisdiction}{doc_number}"
                if kind:
                    citing_text += f" ({kind})"
                if date:
                    citing_text += f" [{date}]"
                citing_patent_ids.append(citing_text)
        
        if lens_id:
            citing_lens_ids.append(lens_id)
    
    patent_data['citing_patents'] = ", ".join(citing_patent_ids) if citing_patent_ids else None
    patent_data['citing_lens_ids'] = ", ".join(citing_lens_ids) if citing_lens_ids else None
    patent_data['citing_patents_count'] = extract_nested_value(biblio, ['cited_by', 'patent_count'])
    
    # Patent families
    # Simple Family
    simple_family = extract_nested_value(patent, ['families', 'simple_family', 'members'], [])
    simple_family_size = extract_nested_value(patent, ['families', 'simple_family', 'size'])
    simple_family_ids = []
    simple_family_lens_ids = []
    
    for member in simple_family:
        doc_id = extract_nested_value(member, ['document_id'], {})
        lens_id = extract_nested_value(member, ['lens_id'])
        
        if doc_id:
            jurisdiction = extract_nested_value(doc_id, ['jurisdiction'])
            doc_number = extract_nested_value(doc_id, ['doc_number'])
            kind = extract_nested_value(doc_id, ['kind'])
            date = extract_nested_value(doc_id, ['date'])
            
            if jurisdiction and doc_number:
                family_text = f"{jurisdiction}{doc_number}"
                if kind:
                    family_text += f" ({kind})"
                if date:
                    family_text += f" [{date}]"
                simple_family_ids.append(family_text)
        
        if lens_id:
            simple_family_lens_ids.append(lens_id)
    
    patent_data['simple_family'] = ", ".join(simple_family_ids) if simple_family_ids else None
    patent_data['simple_family_lens_ids'] = ", ".join(simple_family_lens_ids) if simple_family_lens_ids else None
    patent_data['simple_family_size'] = simple_family_size
    
    # Extended Family
    extended_family = extract_nested_value(patent, ['families', 'extended_family', 'members'], [])
    extended_family_size = extract_nested_value(patent, ['families', 'extended_family', 'size'])
    extended_family_ids = []
    extended_family_lens_ids = []
    
    for member in extended_family:
        doc_id = extract_nested_value(member, ['document_id'], {})
        lens_id = extract_nested_value(member, ['lens_id'])
        
        if doc_id:
            jurisdiction = extract_nested_value(doc_id, ['jurisdiction'])
            doc_number = extract_nested_value(doc_id, ['doc_number'])
            kind = extract_nested_value(doc_id, ['kind'])
            date = extract_nested_value(doc_id, ['date'])
            
            if jurisdiction and doc_number:
                family_text = f"{jurisdiction}{doc_number}"
                if kind:
                    family_text += f" ({kind})"
                if date:
                    family_text += f" [{date}]"
                extended_family_ids.append(family_text)
        
        if lens_id:
            extended_family_lens_ids.append(lens_id)
    
    patent_data['extended_family'] = ", ".join(extended_family_ids) if extended_family_ids else None
    patent_data['extended_family_lens_ids'] = ", ".join(extended_family_lens_ids) if extended_family_lens_ids else None
    patent_data['extended_family_size'] = extended_family_size
    
    # Claims
    claims = extract_nested_value(patent, ['claims'], [])
    if claims and len(claims) > 0:
        claim_text_list = []
        for claim_group in claims:
            for claim in extract_nested_value(claim_group, ['claims'], []):
                claim_text = extract_nested_value(claim, ['claim_text'], [])
                if claim_text:
                    claim_text_list.extend(claim_text)
        
        if claim_text_list:
            patent_data['claims'] = " ".join(claim_text_list)
    
    return patent_data

def main():
    try:
        # Fix potential issues with the path
        safe_folder_path = Path(input_folder)
        
        # List all JSON files in the specified folder
        json_files = [f for f in os.listdir(safe_folder_path) if f.endswith('.json')]
        
        if not json_files:
            print(f"No JSON files found in folder {input_folder}.")
            return
        
        # For all patent data
        all_patent_data = []
        
        for json_file in json_files:
            json_file_path = safe_folder_path / json_file
            print(f"Processing {json_file}...")
            
            try:
                # Load JSON file
                with open(json_file_path, 'r', encoding='utf-8') as file:
                    data = json.load(file)
                
                # Extract data
                for patent in data:
                    patent_data = extract_patent_data(patent)
                    
                    # Add source file information
                    patent_data['source_file'] = json_file
                    
                    all_patent_data.append(patent_data)
                
                print(f"  {len(data)} patents successfully processed from {json_file}.")
                
            except Exception as e:
                print(f"  Error processing {json_file}: {str(e)}")
                continue
        
        if not all_patent_data:
            print("No patent data could be extracted.")
            return
        
        # Create DataFrame
        df = pd.DataFrame(all_patent_data)
        
        # Save Excel file
        df.to_excel(output_excel, index=False)
        
        print(f"\nData successfully saved to {output_excel}.")
        print(f"Number of extracted patents: {len(df)}")
        
        # Output a summary of the extracted data
        print("\nData Summary:")
        print(f"Columns: {', '.join(df.columns)}")
        print(f"Number of processed files: {len(json_files)}")
        
    except Exception as e:
        print(f"An error occurred: {str(e)}")

if __name__ == "__main__":
    main()

#%%


#======================================================================================================================================================
# Reduce the triadic LIB dataset to only families having at least one granted patent and one family-level forward citation
#======================================================================================================================================================

import pandas as pd
import numpy as np
import time
import re
import sys
from datetime import datetime

# Try importing tqdm for progress bars
try:
    from tqdm import tqdm
    tqdm.pandas()
except ImportError:
    print("Installing tqdm for progress bars...")
    import pip
    pip.main(['install', 'tqdm'])
    from tqdm import tqdm
    tqdm.pandas()

# File paths for LIB patent dataset
all_family_members_file = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\full_LIB_data_all_family_members.xlsx"
core_patents_file = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\lens_Leo_IPC_CPC_triadic.xlsx"
output_file_granted = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_patents_triad_granted.xlsx"
output_file_final = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_patents_triad_granted_forwcit.xlsx"

# New output files for excluded patents
output_file_excluded_not_granted = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_patents_triad_excluded_not_granted.xlsx"
output_file_excluded_no_citations = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_patents_triad_excluded_no_citations.xlsx"


def split_family_members(family_str):
    """Split family members string with proper error handling"""
    if pd.isna(family_str):
        return []
    
    try:
        # Try splitting by ";;"
        members = [member.strip() for member in family_str.split(";;")]
        return members
    except:
        try:
            # Fall back to comma splitting if ";;" fails
            members = [member.strip() for member in str(family_str).split(",")]
            return members
        except:
            # Return empty list if all splitting methods fail
            return []

def sum_family_citations(row, lib_full_family_members):
    """Sum citation counts for a single row's family members efficiently"""
    family_members = split_family_members(row["Simple Family Members"])
    
    if not family_members:
        return 0
    
    # Filter family members dataframe once for all members in this family
    matching_members = lib_full_family_members[lib_full_family_members["lens_id"].isin(family_members)]
    
    # Convert citation counts to numeric and sum, handling any non-numeric values
    citation_col = "citing_patents_count"
    if citation_col in matching_members.columns:
        # Convert to numeric, coercing errors to NaN
        numeric_counts = pd.to_numeric(matching_members[citation_col], errors='coerce')
        # Sum all non-NaN values
        return numeric_counts.sum(skipna=True)
    return 0

def extract_family_data(row, lib_full_family_members):
    """Extract kind codes, publication dates, and jurisdictions for all members of a patent family"""
    family_members = split_family_members(row["Simple Family Members"])
    
    if not family_members:
        return {
            "kind_codes": "",
            "pub_dates": "",
            "jurisdictions": ""
        }
    
    # Filter family members dataframe once for all members in this family
    matching_members = lib_full_family_members[lib_full_family_members["lens_id"].isin(family_members)]
    
    # Extract publication kind codes, publication dates, and jurisdictions using the exact column names
    kind_codes = []
    pub_dates = []
    jurisdictions = []
    
    # Use the exact column names provided
    if "pub_kind" in matching_members.columns:
        kind_codes = matching_members["pub_kind"].dropna().tolist()
    
    if "pub_date" in matching_members.columns:
        pub_dates = matching_members["pub_date"].dropna().tolist()
    
    if "pub_jurisdiction" in matching_members.columns:
        jurisdictions = matching_members["pub_jurisdiction"].dropna().tolist()
    
    # Join data with ";;" separator
    return {
        "kind_codes": ";;" if not kind_codes else ";;".join(map(str, kind_codes)),
        "pub_dates": ";;" if not pub_dates else ";;".join(map(str, pub_dates)),
        "jurisdictions": ";;" if not jurisdictions else ";;".join(map(str, jurisdictions))
    }

def contains_granted_patent(family_data):
    """
    Check if a patent family contains at least one granted patent based on specific criteria:
    
    1. US patents:
       - Before Jan 2, 2001: Kind Code A* = granted
       - After Jan 2, 2001: Kind Codes B*, C*, E* = granted
    2. JP patents: Kind Codes B1, B2, B6 = granted
    3. EP patents: Kind Codes B*, C* = granted
    """
    if not isinstance(family_data, dict):
        return False
    
    # Get kind codes, pub dates, and jurisdictions
    kind_codes_str = family_data.get("kind_codes", "")
    pub_dates_str = family_data.get("pub_dates", "")
    jurisdictions_str = family_data.get("jurisdictions", "")
    
    if pd.isna(kind_codes_str) or kind_codes_str == "":
        return False
    
    # Split the data
    kind_codes = kind_codes_str.split(";;") if ';;' in kind_codes_str else [kind_codes_str]
    
    # Handle different date formats and separators for pub_dates
    pub_dates = []
    if not pd.isna(pub_dates_str) and pub_dates_str != "":
        if ';;' in pub_dates_str:
            pub_dates = pub_dates_str.split(";;")
        else:
            pub_dates = [pub_dates_str]
    
    # Handle different formats for jurisdictions
    jurisdictions = []
    if not pd.isna(jurisdictions_str) and jurisdictions_str != "":
        if ';;' in jurisdictions_str:
            jurisdictions = jurisdictions_str.split(";;")
        else:
            jurisdictions = [jurisdictions_str]
    
    # Define the cutoff date for US patents (January 2, 2001)
    us_cutoff_date = datetime(2001, 1, 2)
    
    # If we have different lengths, we need to align the data properly
    # Ideally, we'd have the same number of kind codes, pub dates, and jurisdictions
    max_length = max(len(kind_codes), len(pub_dates), len(jurisdictions))
    min_length = min(len(kind_codes), len(pub_dates), len(jurisdictions))
    
    # If all lists are empty, there's nothing to check
    if max_length == 0:
        return False
    
    # If we have mismatched list lengths but at least some data in each category,
    # use the shortest length to ensure we're matching related data
    check_length = min_length if min_length > 0 else max_length
    
    # If one of the lists is empty (e.g., no jurisdictions), but we have kind codes,
    # check if any kind code is a clear indicator of a granted patent
    if len(kind_codes) > 0 and (len(pub_dates) == 0 or len(jurisdictions) == 0):
        for kind_code in kind_codes:
            kind_code = str(kind_code).strip().upper()
            # B1, B2, etc. are common indicators of granted patents across jurisdictions
            if (re.match(r'B\d', kind_code) or 
                re.match(r'C\d', kind_code)):
                return True
    
    # Check each patent in the family up to the determined length
    for i in range(check_length):
        # Get data safely
        kind_code = str(kind_codes[i]).strip().upper() if i < len(kind_codes) else ""
        jurisdiction = str(jurisdictions[i]).strip().upper() if i < len(jurisdictions) else ""
        
        try:
            pub_date = pd.to_datetime(pub_dates[i]) if i < len(pub_dates) else None
        except:
            pub_date = None
        
        # Check US patents
        if jurisdiction == "US":
            # Before Jan 2, 2001: A* = granted
            if pub_date is not None and pub_date < us_cutoff_date:
                if kind_code.startswith('A'):
                    return True
            # After Jan 2, 2001: B*, C*, E* = granted
            else:
                if (re.match(r'B\d', kind_code) or 
                    re.match(r'C\d', kind_code) or 
                    re.match(r'E\d?', kind_code)):
                    return True
        
        # Check JP patents
        elif jurisdiction == "JP":
            if kind_code in ["B1", "B2", "B6"]:
                return True
        
        # Check EP patents
        elif jurisdiction == "EP":
            if (re.match(r'B\d', kind_code) or re.match(r'C\d', kind_code)):
                return True
    
    return False

def main():
    try:
        total_start_time = time.time()
        
        # STEP 1: Load the datasets
        print("\n==== STEP 1: LOADING DATASETS ====")
        
        # Load the full family members dataset
        print("Loading full family members dataset...")
        load_start = time.time()
        lib_full_family_members = pd.read_excel(all_family_members_file)
        print(f"Full family members dataset loaded successfully with {len(lib_full_family_members)} rows in {time.time() - load_start:.2f} seconds.")
        
        # Print column names to help with debugging
        print("\nColumns in full family members dataset:")
        print(", ".join(lib_full_family_members.columns.tolist()))
        
        # Load the full LIB dataset
        print("\nLoading full LIB dataset...")
        load_start = time.time()
        lib_full_data = pd.read_excel(core_patents_file)
        original_count = len(lib_full_data)
        print(f"Full LIB dataset loaded successfully with {original_count} rows in {time.time() - load_start:.2f} seconds.")
        
        # Print column names to help with debugging
        print("\nColumns in full LIB dataset:")
        print(", ".join(lib_full_data.columns.tolist()))
        
        # Check if "Simple Family Members" column exists
        if "Simple Family Members" not in lib_full_data.columns:
            possible_columns = [col for col in lib_full_data.columns if 'family' in col.lower() or 'member' in col.lower()]
            if possible_columns:
                print(f"\nWARNING: 'Simple Family Members' column not found. Similar columns: {', '.join(possible_columns)}")
                # Try to use the first similar column instead
                family_column = possible_columns[0]
                print(f"Using '{family_column}' instead of 'Simple Family Members'")
                lib_full_data = lib_full_data.rename(columns={family_column: "Simple Family Members"})
            else:
                print("\nERROR: 'Simple Family Members' column not found and no similar columns available.")
                raise ValueError("'Simple Family Members' column is required but not found in the dataset")
        
        # Remove duplicates in Simple Family Members column
        print("\nRemoving duplicates based on 'Simple Family Members'...")
        lib_full_data = lib_full_data.drop_duplicates(subset=["Simple Family Members"], keep="first")
        print(f"Dataset after duplicate removal: {len(lib_full_data)} rows (removed {original_count - len(lib_full_data)} duplicates).")
        
        # STEP 2: Extract family data and filter for granted patents
        print("\n==== STEP 2: EXTRACTING FAMILY DATA AND FILTERING FOR GRANTED PATENTS ====")
        print("Extracting publication kind codes, dates, and jurisdictions for all family members...")
        
        # Initialize tqdm for pandas
        tqdm.pandas(desc="Extracting family data")
        
        # Create new columns for family data with progress bar
        start_time = time.time()
        
        # Process the full dataset with the correct column names
        print("\nProcessing full dataset using columns: pub_kind, pub_date, pub_jurisdiction")
        
        family_data = lib_full_data.progress_apply(
            lambda row: extract_family_data(row, lib_full_family_members),
            axis=1
        )
        
        # Extract data from dictionaries and add to dataframe
        lib_full_data["Family kind codes"] = family_data.apply(lambda x: x.get("kind_codes", ""))
        lib_full_data["Family pub dates"] = family_data.apply(lambda x: x.get("pub_dates", ""))
        lib_full_data["Family jurisdictions"] = family_data.apply(lambda x: x.get("jurisdictions", ""))
        
        print(f"Family data extraction completed in {time.time() - start_time:.2f} seconds.")
        
        # Create composite family data for granted patent check
        family_data_for_check = family_data.copy()
        
        # Count patents before filtering
        before_filtering = len(lib_full_data)
        print(f"Patents before filtering for granted status: {before_filtering}")
        
        # Create a mask for patents with granted patents in their family
        tqdm.pandas(desc="Checking for granted patents")
        granted_mask = family_data_for_check.progress_apply(contains_granted_patent)
        
        # Filter for granted patents only
        print("Filtering for patent families with at least one granted patent...")
        lib_full_data_granted = lib_full_data[granted_mask]
        
        # Get excluded patents (those without granted patents in their family)
        lib_full_data_excluded_not_granted = lib_full_data[~granted_mask]
        
        # Save excluded patents to Excel
        lib_full_data_excluded_not_granted.to_excel(output_file_excluded_not_granted, index=False)
        print(f"Excluded patents (no granted patents) saved to: {output_file_excluded_not_granted}")
        
        # Count patents after filtering
        after_filtering = len(lib_full_data_granted)
        print(f"Patents after filtering for granted status: {after_filtering}")
        print(f"Removed {before_filtering - after_filtering} patents without granted family members.")
        
        # Save intermediate result
        lib_full_data_granted.to_excel(output_file_granted, index=False)
        print(f"Granted patents dataset saved to: {output_file_granted}")
        
        # STEP 3: Calculate family-level citation counts and filter by citations
        print("\n==== STEP 3: CALCULATING FAMILY-LEVEL CITATION COUNTS AND FILTERING BY CITATIONS ====")
        print("Calculating family level citation counts...")
        
        # Initialize tqdm for pandas again
        tqdm.pandas(desc="Counting citations")
        
        # Create new column for family level citation count with progress bar
        citation_start = time.time()
        lib_full_data_granted["Family level citing count"] = lib_full_data_granted.progress_apply(
            lambda row: sum_family_citations(row, lib_full_family_members),
            axis=1
        )
        
        citation_time = time.time() - citation_start
        print(f"Citation counting completed in {citation_time:.2f} seconds.")
        
        # Count patents with citations
        patents_with_citations = (lib_full_data_granted["Family level citing count"] > 0).sum()
        print(f"Patents with family-level citations: {patents_with_citations} out of {len(lib_full_data_granted)}")
        print(f"Total citations across all patent families: {lib_full_data_granted['Family level citing count'].sum()}")
        
        # Count patents before citation filtering
        before_citation_filtering = len(lib_full_data_granted)
        print(f"Patents before filtering by citation count: {before_citation_filtering}")
        
        # Create a mask for patents with at least one citation
        citation_mask = lib_full_data_granted["Family level citing count"] > 0
        
        # Filter patents with at least one citation
        print("Filtering for patents with at least one citation...")
        lib_full_data_triad_granted_forwcit = lib_full_data_granted[citation_mask]
        
        # Get excluded patents (those without citations)
        lib_full_data_excluded_no_citations = lib_full_data_granted[~citation_mask]
        
        # Save excluded patents to Excel
        lib_full_data_excluded_no_citations.to_excel(output_file_excluded_no_citations, index=False)
        print(f"Excluded patents (no citations) saved to: {output_file_excluded_no_citations}")
        
        # Count patents after citation filtering
        after_citation_filtering = len(lib_full_data_triad_granted_forwcit)
        print(f"Patents after filtering by citation count: {after_citation_filtering}")
        print(f"Removed {before_citation_filtering - after_citation_filtering} patents without citations.")
        
        # Save final result
        lib_full_data_triad_granted_forwcit.to_excel(output_file_final, index=False)
        
        # Calculate total processing time
        total_time = time.time() - total_start_time
        print(f"\nFinal dataset saved to: {output_file_final}")
        print(f"Total processing time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
        
        # SUMMARY: Print filtering statistics
        print("\n==== SUMMARY OF FILTERING STEPS ====")
        print(f"1. Initial dataset: {original_count} patents")
        print(f"2. After duplicate removal: {before_filtering} patents (-{original_count - before_filtering})")
        print(f"3. After filtering for granted patents: {after_filtering} patents (-{before_filtering - after_filtering})")
        print(f"   - Excluded patents without granted status saved to: {output_file_excluded_not_granted}")
        print(f"4. After filtering for citations: {after_citation_filtering} patents (-{before_citation_filtering - after_citation_filtering})")
        print(f"   - Excluded patents without citations saved to: {output_file_excluded_no_citations}")
        print(f"Final dataset contains {after_citation_filtering} patents ({after_citation_filtering/original_count*100:.2f}% of original)")
    
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()



#======================================================================================================================================================
# Adding claim data to our filtered LIB full patent family dataset - OPTIMIZED VERSION
#======================================================================================================================================================

import pandas as pd
import numpy as np
import re
import time
import sys
from datetime import datetime

# Try importing tqdm for progress bars
try:
    from tqdm import tqdm
    tqdm.pandas()
except ImportError:
    print("Installing tqdm for progress bars...")
    import pip
    pip.main(['install', 'tqdm'])
    from tqdm import tqdm
    tqdm.pandas()

# Try importing langdetect for better language detection
try:
    from langdetect import detect
    from langdetect.lang_detect_exception import LangDetectException
    has_langdetect = True
    print("Using langdetect for language detection")
except ImportError:
    has_langdetect = False
    print("Warning: langdetect not installed. Using simpler language detection.")

# 1. Reading the datasets with raw strings
lib_patents_cleaned_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_patents_triad_granted_forwcit.xlsx"
full_lib_data_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\full_LIB_data_all_family_members.xlsx"

# Reading the data
print("Reading datasets...")
start_time = time.time()
lib_patents_cleaned = pd.read_excel(lib_patents_cleaned_path)
full_lib_data = pd.read_excel(full_lib_data_path)
print(f"Datasets loaded in {time.time() - start_time:.2f} seconds")

# Create a lookup dictionary for claims and application dates
print("Creating lookup dictionaries for claims and application dates...")
claims_dict = {}
app_date_dict = {}

# Process all rows in full_lib_data to populate our dictionaries
for idx, row in tqdm(full_lib_data.iterrows(), total=len(full_lib_data), desc="Processing family members"):
    lens_id = str(row['lens_id'])
    claim = row.get('claims', None)
    app_date = row.get('app_date', None)
    
    # Store claim if it exists and is a string
    if pd.notna(claim) and isinstance(claim, str):
        claims_dict[lens_id] = claim
    
    # Store application date if it exists
    if pd.notna(app_date):
        # Convert to datetime object if it's not already
        if not isinstance(app_date, pd.Timestamp) and not isinstance(app_date, datetime):
            try:
                # Try to parse common date formats
                app_date = pd.to_datetime(app_date)
            except:
                print(f"Warning: Could not parse app_date for {lens_id}: {app_date}")
                app_date = None
        
        if app_date is not None:
            app_date_dict[lens_id] = app_date

# Enhanced function to detect if text is in English
def is_english(text):
    """
    Detect if text is in English language with higher accuracy.
    Returns True if English, False otherwise or if detection fails.
    """
    if pd.isna(text) or not isinstance(text, str) or len(text.strip()) < 10:
        return False
        
    try:
        # More comprehensive list of English words/patterns
        english_pattern = r'\b(the|and|of|to|in|is|for|with|as|on|at|by|an|this|that|which|claim|claims|wherein|comprising|comprises|consists|method|system|device|apparatus|according)\b'
        matches = re.findall(english_pattern, text.lower())
        
        # Check for non-English characters (common in European languages)
        non_english_chars = r'[äöüßéèêëàáâäãåçñøæœ]'
        non_english_matches = re.findall(non_english_chars, text.lower())
        
        # Check for language-specific patterns
        german_pattern = r'\b(der|die|das|ein|eine|zu|für|mit|und|oder|wobei|anspruch|verfahren|vorrichtung|gemäß)\b'
        french_pattern = r'\b(le|la|les|un|une|pour|avec|et|ou|selon|revendication|procédé|dispositif|caractérisé)\b'
        
        german_matches = re.findall(german_pattern, text.lower())
        french_matches = re.findall(french_pattern, text.lower())
        
        # Stronger heuristics for English detection
        is_likely_english = len(matches) >= 7 and len(non_english_matches) < 3 and len(german_matches) < 3 and len(french_matches) < 3
        
        # If using langdetect and our heuristics suggest it might be English, confirm with langdetect
        if has_langdetect and (is_likely_english or (len(matches) > 5 and len(non_english_matches) < 5)):
            try:
                # Use a sample of the text to speed up detection
                # Take multiple samples from different parts for better accuracy
                start_sample = text[:min(len(text), 500)]
                mid_sample = text[len(text)//2:len(text)//2 + min(500, len(text)-len(text)//2)] if len(text) > 1000 else ""
                end_sample = text[-min(500, len(text)):] if len(text) > 1000 else ""
                
                # Combine samples
                sample_text = start_sample
                if mid_sample:
                    sample_text += " " + mid_sample
                if end_sample and end_sample != start_sample:
                    sample_text += " " + end_sample
                
                detected_lang = detect(sample_text)
                return detected_lang == 'en'
            except LangDetectException:
                # If langdetect fails, fall back to pattern matching
                return is_likely_english
        else:
            # Without langdetect: more stringent checks
            return is_likely_english
            
    except Exception as e:
        print(f"Error in language detection: {str(e)}")
        # If language detection fails, be conservative and return False
        return False

# Helper function to split family members with proper error handling
def split_family_members(family_str):
    """Split family members string with proper error handling"""
    if pd.isna(family_str):
        return []
    
    try:
        # Try splitting by ";;"
        members = [member.strip() for member in family_str.split(";;")]
        # Filter out empty strings
        members = [m for m in members if m]
        return members
    except:
        try:
            # Fall back to comma splitting if ";;" fails
            members = [member.strip() for member in str(family_str).split(",")]
            # Filter out empty strings
            members = [m for m in members if m]
            return members
        except:
            # Return empty list if all splitting methods fail
            return []

# New function to find the most recent patent with English claims in a family
def find_newest_english_claim_for_family(family_members_str):
    """
    Find the most recent patent with English claims from the family members.
    Returns a tuple of (claim, source_id, app_date).
    Returns (np.nan, np.nan, np.nan) if no English claim is found.
    """
    if pd.isna(family_members_str):
        return np.nan, np.nan, np.nan
    
    # Split family members
    family_members = split_family_members(family_members_str)
    
    if not family_members:
        return np.nan, np.nan, np.nan
    
    # Track all members with English claims and their application dates
    english_claims_members = []
    
    # First pass: identify all members with English claims
    for member in family_members:
        member = member.strip()  # Remove any whitespace
        
        # Skip if member not in either dictionary
        if member not in claims_dict:
            continue
            
        claim = claims_dict[member]
        
        # Skip if claim is not a string or is too short
        if not isinstance(claim, str) or len(claim.strip()) < 10:
            continue
            
        # Check if the claim is in English
        if is_english(claim):
            # Add this member to our list of English claims sources
            # We'll need to sort by app_date later
            app_date = app_date_dict.get(member, None)
            english_claims_members.append((member, claim, app_date))
    
    # If we have members with English claims, sort by application date (newest first)
    if english_claims_members:
        # Filter to only members that have a valid application date
        dated_members = [(m, c, d) for m, c, d in english_claims_members if d is not None]
        
        if dated_members:
            # Sort by date (newest first)
            sorted_members = sorted(dated_members, key=lambda x: x[2], reverse=True)
            newest_member, newest_claim, newest_date = sorted_members[0]
            return newest_claim, newest_member, newest_date
        else:
            # If none have dates, just take the first one
            member, claim, _ = english_claims_members[0]
            return claim, member, np.nan
    
    # Return np.nan if no English claim is found
    return np.nan, np.nan, np.nan

# 2. Process all rows in LIB_patents_cleaned and add claims with progress bar
print("Adding newest English claims to patent families...")
start_time = time.time()

# Create a function to apply that returns all three values
def process_family(family_str):
    claim, source_id, app_date = find_newest_english_claim_for_family(family_str)
    return pd.Series([claim, source_id, app_date])

# Use tqdm for progress tracking
tqdm.pandas(desc="Finding newest English claims")
results = lib_patents_cleaned['Simple Family Members'].progress_apply(process_family)

# Assign the results to new columns
lib_patents_cleaned['claims'] = results[0]
lib_patents_cleaned['claims_source_id'] = results[1]
lib_patents_cleaned['claims_source_date'] = results[2]

# Add a column to indicate the language
lib_patents_cleaned['claims_language'] = lib_patents_cleaned['claims'].apply(
    lambda x: 'en' if pd.notna(x) and is_english(x) else 'none'
)

# Verify language detection on a sample of claims
if lib_patents_cleaned['claims'].notna().sum() > 0:
    print("\nVerifying language detection on 5 random samples:")
    sample_claims = lib_patents_cleaned[lib_patents_cleaned['claims'].notna()].sample(min(5, lib_patents_cleaned['claims'].notna().sum()))
    for i, (_, row) in enumerate(sample_claims.iterrows()):
        claim_preview = row['claims'][:150] + "..." if len(row['claims']) > 150 else row['claims']
        source_id = row['claims_source_id']
        source_date = row['claims_source_date']
        print(f"Sample {i+1} (source: {source_id}, date: {source_date}):\n{claim_preview}\n")

# Diagnostic check - find any non-English claims that might have slipped through
non_english_claims = lib_patents_cleaned[(lib_patents_cleaned['claims'].notna()) & 
                                       (lib_patents_cleaned['claims_language'] == 'none')]

if len(non_english_claims) > 0:
    print(f"\nWARNING: Found {len(non_english_claims)} claims that may not be in English")
    for i, (_, row) in enumerate(non_english_claims.head(3).iterrows()):
        claim_preview = row['claims'][:150] + "..." if len(row['claims']) > 150 else row['claims']
        print(f"Non-English sample {i+1}:\n{claim_preview}\n")
    
    # Remove non-English claims
    print("Removing non-English claims...")
    lib_patents_cleaned.loc[lib_patents_cleaned['claims_language'] == 'none', 'claims'] = np.nan
    lib_patents_cleaned.loc[lib_patents_cleaned['claims_language'] == 'none', 'claims_source_id'] = np.nan
    lib_patents_cleaned.loc[lib_patents_cleaned['claims_language'] == 'none', 'claims_source_date'] = np.nan

# Split the dataframe into two: one with claims and one without
patents_with_claims = lib_patents_cleaned[lib_patents_cleaned['claims'].notna()]
patents_without_claims = lib_patents_cleaned[lib_patents_cleaned['claims'].isna()]

# Save the updated data to two separate Excel files
with_claims_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_patents_cleaned_with_claims.xlsx"
no_claims_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_patents_cleaned_no_claims_newest.xlsx"

print(f"\nSaving dataset with English claims from newest patents to: {with_claims_path}")
patents_with_claims.to_excel(with_claims_path, index=False)

print(f"Saving dataset without English claims to: {no_claims_path}")
patents_without_claims.to_excel(no_claims_path, index=False)

# Report detailed statistics
processing_time = time.time() - start_time
claims_count = lib_patents_cleaned['claims'].notna().sum()
total_count = len(lib_patents_cleaned)

# Additional statistics about dates
date_available = lib_patents_cleaned['claims_source_date'].notna().sum()

print("\n===== SUMMARY =====")
print(f"Processing time: {processing_time:.2f} seconds ({processing_time/60:.2f} minutes)")
print(f"Total patent families processed: {total_count}")
print(f"Patents with English claims: {claims_count} ({claims_count/total_count*100:.2f}%)")
print(f"Patents with English claims and date info: {date_available} ({date_available/claims_count*100:.2f}% of patents with claims)")
print(f"Patents without English claims: {total_count - claims_count} ({(total_count - claims_count)/total_count*100:.2f}%)")
print("=====================")




#===================================================================================
# Adding Citing Lens IDs at family level
#===================================================================================

import pandas as pd

# Load both datasets
def process_library_data(path_to_main_file, path_to_family_members_file):
    """
    Process LIB patent data and extract citing patents from family members.
    
    Args:
        path_to_main_file: Path to the main Excel file with LIB patent data
        path_to_family_members_file: Path to the Excel file with family members data
    
    Returns:
        DataFrame with processed data including new 'family_citing_lens_id' column
    """
    # Read the Excel files
    print("Reading input files...")
    LIB_full_data = pd.read_excel(path_to_main_file)
    LIB_full_data_all_family_members = pd.read_excel(path_to_family_members_file)
    
    # Create a dictionary for faster lookup of citing patents by lens_id
    print("Creating lookup dictionary for citing patents...")
    citing_patents_dict = {}
    for index, row in LIB_full_data_all_family_members.iterrows():
        citing_patents_dict[row['lens_id']] = row['citing_lens_ids']
    
    # Initialize new column for family citing lens IDs
    LIB_full_data['family_citing_lens_id'] = ''
    
    # Process each row in the main dataset
    print("Processing simple family members for each patent...")
    for index, row in LIB_full_data.iterrows():
        if pd.notna(row['Simple Family Members']):  # Check if value is not NaN
            # Split the family members by ";;" and strip whitespace
            family_members = [member.strip() for member in str(row['Simple Family Members']).split(';;')]
            
            # Collect all citing patents for these family members
            all_citing_patents = []
            for member in family_members:
                if member in citing_patents_dict and pd.notna(citing_patents_dict[member]):
                    # Add citing patents to the collection if they exist
                    all_citing_patents.append(str(citing_patents_dict[member]))
            
            # Join all citing patents with commas and assign to the new column
            LIB_full_data.at[index, 'family_citing_lens_id'] = ', '.join(all_citing_patents)
        
        # Print progress for every 1000 records
        if index % 1000 == 0:
            print(f"Processed {index} records...")
    
    print("Processing complete!")
    return LIB_full_data

# Example usage
if __name__ == "__main__":
    # Update these paths to your actual file locations
    main_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_patents_cleaned_with_claims.xlsx"
    family_members_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\full_LIB_data_all_family_members.xlsx"
    
    # Process the data
    result_df = process_library_data(main_file_path, family_members_file_path)
    
    # Save the result to a new Excel file
    output_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_patents_final.xlsx"
    result_df.to_excel(output_path, index=False)
    
    print(f"Results saved to: {output_path}")

#%%

#========================================================================================================
#========================================================================================================
# Sodium-ion batteries - triadic filter criterium
#========================================================================================================
#========================================================================================================

#============================================================================================
# Data upload:Handling Sodium-ion battery dataset from lens.org
#============================================================================================ 


import pandas as pd
import os

# 1. Read Excel file
file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_dataset_08042025.xlsx"
SIB_data = pd.read_excel(file_path)

# 2. Filter for triadic patents (EP, US or JP)
# Create empty list for indices of rows to keep
triadic_indices = []

# Jurisdictions to filter for
target_jurisdictions = {"EP", "US", "JP"}

# Iterate through each row in the "Simple Family Member Jurisdictions" column
for idx, cell_content in enumerate(SIB_data["Simple Family Member Jurisdictions"]):
    # Check if the cell content is a string (to skip NaN values)
    if isinstance(cell_content, str):
        # Split the cell at ";;"
        jurisdictions = cell_content.split(";;")
        # Check if at least one of the target jurisdictions is present
        if any(jurisdiction in target_jurisdictions for jurisdiction in jurisdictions):
            triadic_indices.append(idx)

# Create filtered DataFrame
SIB_data_triadic = SIB_data.iloc[triadic_indices].reset_index(drop=True)
print(f"Original dataset: {len(SIB_data)} rows")
print(f"Filtered dataset (EP, US or JP): {len(SIB_data_triadic)} rows")

# 3. Create set for all lens_ids from filtered patents
set_SIB_full_patents = set()

# Iterate through each row in the "Simple Family Members" column of the filtered DataFrame
for cell_content in SIB_data_triadic["Simple Family Members"]:
    # Check if the cell content is a string (to skip NaN values)
    if isinstance(cell_content, str):
        # Split the cell at ";;"
        lens_ids = cell_content.split(";;")
        # Add each ID to the set
        for lens_id in lens_ids:
            set_SIB_full_patents.add(lens_id)

# 4. Print the number of unique IDs
print(f"Number of unique patent IDs from triadic patent families: {len(set_SIB_full_patents)}")

# Define output path (in the same directory as the Excel file)
output_dir = os.path.dirname(file_path)
output_file = os.path.join(output_dir, "All_SIB_patents.txt")

# Write IDs to a text file (each ID on a new line)
with open(output_file, 'w') as f:
    for lens_id in set_SIB_full_patents:
        f.write(f"{lens_id}\n")

print(f"The list of patent IDs has been successfully saved to: {output_file}")

# 5. Save the filtered DataFrame as an Excel file
triadic_output_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\sib_data_triadic.xlsx"
SIB_data_triadic.to_excel(triadic_output_path, index=False)
print(f"The filtered dataset has been successfully saved to: {triadic_output_path}")


#%%

#========================================================================================================
#========================================================================================================
# Sodium-ion batteries - Chinese filter criterium
#========================================================================================================
#========================================================================================================

#============================================================================================
# Data upload:Handling Sodium-ion battery dataset from lens.org
#============================================================================================ 

import pandas as pd
import os

# 1. Read Excel file
file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_dataset_08042025.xlsx"
SIB_data = pd.read_excel(file_path)

# 2. Filter for Chinese patents (CN)
# Create empty list for indices of rows to keep
chinese_indices = []

# Jurisdictions to filter for
target_jurisdictions = {"CN"}

# Iterate through each row in the "Simple Family Member Jurisdictions" column
for idx, cell_content in enumerate(SIB_data["Simple Family Member Jurisdictions"]):
    # Check if the cell content is a string (to skip NaN values)
    if isinstance(cell_content, str):
        # Split the cell at ";;"
        jurisdictions = cell_content.split(";;")
        # Check if at least one of the target jurisdictions is present
        if any(jurisdiction in target_jurisdictions for jurisdiction in jurisdictions):
            chinese_indices.append(idx)

# Create filtered DataFrame
SIB_data_CN = SIB_data.iloc[chinese_indices].reset_index(drop=True)
print(f"Original dataset: {len(SIB_data)} rows")
print(f"Filtered dataset (CN): {len(SIB_data_CN)} rows")

# 3. Create set for all lens_ids from filtered patents
set_SIB_CN_patents = set()

# Iterate through each row in the "Simple Family Members" column of the filtered DataFrame
for cell_content in SIB_data_CN["Simple Family Members"]:
    # Check if the cell content is a string (to skip NaN values)
    if isinstance(cell_content, str):
        # Split the cell at ";;"
        lens_ids = cell_content.split(";;")
        # Add each ID to the set
        for lens_id in lens_ids:
            set_SIB_CN_patents.add(lens_id)

# 4. Print the number of unique IDs
print(f"Number of unique patent IDs from Chinese patent families: {len(set_SIB_CN_patents)}")

# Define output path (in the same directory as the Excel file)
output_dir = os.path.dirname(file_path)
output_file = os.path.join(output_dir, "All_SIB_CN_patents.txt")

# Write IDs to a text file (each ID on a new line)
with open(output_file, 'w') as f:
    for lens_id in set_SIB_CN_patents:
        f.write(f"{lens_id}\n")
print(f"The list of patent IDs has been successfully saved to: {output_file}")

# 5. Save the filtered DataFrame as an Excel file
CN_output_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\sib_data_CN.xlsx"
SIB_data_CN.to_excel(CN_output_path, index=False)
print(f"The filtered dataset has been successfully saved to: {CN_output_path}")

#%%

# Check if the json files include all lens_ids

import json
import pandas as pd
import os

# get files from a folder (id only)
fetch_folder_csv = r'C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\CSV data'
files_id = [f for f in os.listdir(fetch_folder_csv) if f.endswith('.csv')]

# get lens_ids from each file
my_lens_ids = []
for file in files_id:
    file_path = os.path.join(fetch_folder_csv, file)  # Full path to the CSV file

    #Read the CSV file and extract 'lens_ids' column
    ids_missing_fam_temp = pd.read_csv(file_path)
    ids_missing_fam_temp = ids_missing_fam_temp['lens_ids'].tolist()  # Convert to list

    my_lens_ids = ids_missing_fam_temp + my_lens_ids

print(f'Unique Lens_ids searched:{len(set(my_lens_ids))}')

# get files from a folder (JSON info)
fetch_folder_csv = r'C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\output'
files_id = [f for f in os.listdir(fetch_folder_csv) if f.endswith('.json')]

info_lens_ids = []
for file in files_id:
    file_path = os.path.join(fetch_folder_csv, file)  # Full path to the CSV file
    with open(file_path, "r") as file:
        data = json.load(file)
    for patents in data:
        lens_id = str(patents.get("lens_id", None)) #to have everything in the dataframe as a string type
        info_lens_ids.append(lens_id)

print(f'Unique Lens_ids data collected for:{len(set(info_lens_ids))}')

not_collected_ids = list(set(my_lens_ids) - set(info_lens_ids))
print(f'IDs not collected:{len(not_collected_ids)}')

output_folder = r'C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\All JSON\missing ids'
batch_size = 10000
for i in range(0, len(not_collected_ids), batch_size):
    batch = not_collected_ids[i:i + batch_size]
    output_file = os.path.join(output_folder, f're_unique_patent_ids_triadic_one_out_of_three_{i//batch_size + 1}.csv')

    # Save batch to CSV
    df = pd.DataFrame(batch, columns=['lens_ids'])
    df.to_csv(output_file, index=False)

    print(f"Saved {len(batch)} lens_ids to {output_file}")


#%%

# Extract full dataset from json files (downloaded via API) into Excel file

import json
import pandas as pd
import numpy as np
import os
from pathlib import Path

# File paths
# Using raw string to avoid escape sequence issues
input_folder = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\output\SIB_EP_US_JP"  # Path to folder containing JSON files
output_excel = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\output\SIB_EP_US_JP\full_SIB_US_EP_JP_data_all_family_members.xlsx"  # Full path to the output file

def extract_nested_value(obj, key_path, default=None):
    """Extracts values from nested dictionaries using a path of keys"""
    current = obj
    
    for key in key_path:
        if isinstance(current, dict) and key in current:
            current = current[key]
        else:
            return default
    
    return current

def extract_list_values(obj, key_path, extract_key=None, join_with=", "):
    """Extracts values from lists in nested dictionaries"""
    values = extract_nested_value(obj, key_path)
    
    if not isinstance(values, list):
        return None
    
    if extract_key:
        extracted = []
        for item in values:
            if isinstance(item, dict):
                if isinstance(extract_key, list):
                    value = extract_nested_value(item, extract_key)
                    if value:
                        extracted.append(value)
                else:
                    if extract_key in item:
                        extracted.append(item[extract_key])
        return join_with.join(extracted) if extracted else None
    else:
        return join_with.join(str(item) for item in values)

def extract_patent_data(patent):
    """Extracts relevant patent data from a patent object"""
    patent_data = {}
    
    # Basic information
    patent_data['lens_id'] = patent.get('lens_id')
    patent_data['date_published'] = patent.get('date_published')
    
    biblio = patent.get('biblio', {})
    
    # Publication reference
    pub_ref = extract_nested_value(biblio, ['publication_reference'], {})
    patent_data['pub_jurisdiction'] = extract_nested_value(pub_ref, ['jurisdiction'])
    patent_data['pub_doc_number'] = extract_nested_value(pub_ref, ['doc_number'])
    patent_data['pub_kind'] = extract_nested_value(pub_ref, ['kind'])
    patent_data['pub_date'] = extract_nested_value(pub_ref, ['date'])
    
    # Application reference
    app_ref = extract_nested_value(biblio, ['application_reference'], {})
    patent_data['app_jurisdiction'] = extract_nested_value(app_ref, ['jurisdiction'])
    patent_data['app_doc_number'] = extract_nested_value(app_ref, ['doc_number'])
    patent_data['app_kind'] = extract_nested_value(app_ref, ['kind'])
    patent_data['app_date'] = extract_nested_value(app_ref, ['date'])
    
    # Priority date and priority claims
    priority_claims = extract_nested_value(biblio, ['priority_claims'], {})
    patent_data['earliest_priority_date'] = extract_nested_value(priority_claims, ['earliest_claim', 'date'])
    
    # Extract all priority claims
    priority_claims_list = extract_nested_value(priority_claims, ['claims'], [])
    priority_claims_details = []
    
    for claim in priority_claims_list:
        jurisdiction = extract_nested_value(claim, ['jurisdiction'])
        doc_number = extract_nested_value(claim, ['doc_number'])
        kind = extract_nested_value(claim, ['kind'])
        date = extract_nested_value(claim, ['date'])
        sequence = extract_nested_value(claim, ['sequence'])
        
        if jurisdiction and doc_number:
            priority_claim_detail = f"{jurisdiction}{doc_number}"
            if kind:
                priority_claim_detail += f" ({kind})"
            if date:
                priority_claim_detail += f" [{date}]"
            if sequence:
                priority_claim_detail += f" Seq:{sequence}"
            
            priority_claims_details.append(priority_claim_detail)
    
    patent_data['priority_claims'] = ", ".join(priority_claims_details) if priority_claims_details else None
    
    # Title
    titles = extract_nested_value(biblio, ['invention_title'], [])
    if titles:
        for title in titles:
            if extract_nested_value(title, ['lang']) == 'en':
                patent_data['title_en'] = extract_nested_value(title, ['text'])
            else:
                patent_data['title_other'] = extract_nested_value(title, ['text'])
    
    # Parties
    parties = extract_nested_value(biblio, ['parties'], {})
    
    # Applicants
    applicants = extract_nested_value(parties, ['applicants'], [])
    applicant_names = []
    applicant_countries = []
    
    for applicant in applicants:
        if 'extracted_name' in applicant and 'value' in applicant['extracted_name']:
            applicant_names.append(applicant['extracted_name']['value'])
        if 'residence' in applicant:
            applicant_countries.append(applicant['residence'])
    
    patent_data['applicants'] = ", ".join(applicant_names) if applicant_names else None
    patent_data['applicant_countries'] = ", ".join(applicant_countries) if applicant_countries else None
    
    # Inventors
    inventors = extract_nested_value(parties, ['inventors'], [])
    inventor_names = []
    inventor_countries = []
    
    for inventor in inventors:
        if 'extracted_name' in inventor and 'value' in inventor['extracted_name']:
            inventor_names.append(inventor['extracted_name']['value'])
        if 'residence' in inventor:
            inventor_countries.append(inventor['residence'])
    
    patent_data['inventors'] = ", ".join(inventor_names) if inventor_names else None
    patent_data['inventor_countries'] = ", ".join(inventor_countries) if inventor_countries else None
    
    # Agents/Attorneys
    agents = extract_nested_value(parties, ['agents'], [])
    agent_names = []
    
    for agent in agents:
        if 'extracted_name' in agent and 'value' in agent['extracted_name']:
            agent_names.append(agent['extracted_name']['value'])
    
    patent_data['agents'] = ", ".join(agent_names) if agent_names else None
    
    # Extract examiners
    examiners = extract_nested_value(biblio, ['examiners'], {})
    primary_examiner = extract_nested_value(examiners, ['primary_examiner', 'extracted_name', 'value'])
    if primary_examiner:
        patent_data['primary_examiner'] = primary_examiner
    
    # Classifications
    # IPC
    ipc_classifications = extract_nested_value(biblio, ['classifications_ipcr', 'classifications'], [])
    ipc_symbols = []
    
    for cls in ipc_classifications:
        if 'symbol' in cls:
            ipc_symbols.append(cls['symbol'])
    
    patent_data['ipc_classifications'] = ", ".join(ipc_symbols) if ipc_symbols else None
    
    # CPC
    cpc_classifications = extract_nested_value(biblio, ['classifications_cpc', 'classifications'], [])
    cpc_symbols = []
    
    for cls in cpc_classifications:
        if 'symbol' in cls:
            cpc_symbols.append(cls['symbol'])
    
    patent_data['cpc_classifications'] = ", ".join(cpc_symbols) if cpc_symbols else None
    
    # National classifications
    nat_classifications = extract_nested_value(biblio, ['classifications_national', 'classifications'], [])
    nat_symbols = []
    
    for cls in nat_classifications:
        if 'symbol' in cls:
            nat_symbols.append(cls['symbol'])
    
    patent_data['national_classifications'] = ", ".join(nat_symbols) if nat_symbols else None
    
    # Cited patents
    citations = extract_nested_value(biblio, ['references_cited', 'citations'], [])
    patent_citations = []
    cited_lens_ids = []
    
    for citation in citations:
        patcit = extract_nested_value(citation, ['patcit'], {})
        doc_id = extract_nested_value(patcit, ['document_id'], {})
        lens_id = extract_nested_value(patcit, ['lens_id'])
        
        if doc_id:
            jurisdiction = extract_nested_value(doc_id, ['jurisdiction'])
            doc_number = extract_nested_value(doc_id, ['doc_number'])
            kind = extract_nested_value(doc_id, ['kind'])
            date = extract_nested_value(doc_id, ['date'])
            
            if jurisdiction and doc_number:
                citation_text = f"{jurisdiction}{doc_number}"
                if kind:
                    citation_text += f" ({kind})"
                if date:
                    citation_text += f" [{date}]"
                patent_citations.append(citation_text)
        
        if lens_id:
            cited_lens_ids.append(lens_id)
    
    patent_data['cited_patents'] = ", ".join(patent_citations) if patent_citations else None
    patent_data['cited_lens_ids'] = ", ".join(cited_lens_ids) if cited_lens_ids else None
    patent_data['cited_patents_count'] = extract_nested_value(biblio, ['references_cited', 'patent_count'])
    patent_data['cited_npl_count'] = extract_nested_value(biblio, ['references_cited', 'npl_count'])
    
    # Citing patents (Cited by)
    citing_patents = extract_nested_value(biblio, ['cited_by', 'patents'], [])
    citing_patent_ids = []
    citing_lens_ids = []
    
    for citing in citing_patents:
        doc_id = extract_nested_value(citing, ['document_id'], {})
        lens_id = extract_nested_value(citing, ['lens_id'])
        
        if doc_id:
            jurisdiction = extract_nested_value(doc_id, ['jurisdiction'])
            doc_number = extract_nested_value(doc_id, ['doc_number'])
            kind = extract_nested_value(doc_id, ['kind'])
            date = extract_nested_value(doc_id, ['date'])
            
            if jurisdiction and doc_number:
                citing_text = f"{jurisdiction}{doc_number}"
                if kind:
                    citing_text += f" ({kind})"
                if date:
                    citing_text += f" [{date}]"
                citing_patent_ids.append(citing_text)
        
        if lens_id:
            citing_lens_ids.append(lens_id)
    
    patent_data['citing_patents'] = ", ".join(citing_patent_ids) if citing_patent_ids else None
    patent_data['citing_lens_ids'] = ", ".join(citing_lens_ids) if citing_lens_ids else None
    patent_data['citing_patents_count'] = extract_nested_value(biblio, ['cited_by', 'patent_count'])
    
    # Patent families
    # Simple Family
    simple_family = extract_nested_value(patent, ['families', 'simple_family', 'members'], [])
    simple_family_size = extract_nested_value(patent, ['families', 'simple_family', 'size'])
    simple_family_ids = []
    simple_family_lens_ids = []
    
    for member in simple_family:
        doc_id = extract_nested_value(member, ['document_id'], {})
        lens_id = extract_nested_value(member, ['lens_id'])
        
        if doc_id:
            jurisdiction = extract_nested_value(doc_id, ['jurisdiction'])
            doc_number = extract_nested_value(doc_id, ['doc_number'])
            kind = extract_nested_value(doc_id, ['kind'])
            date = extract_nested_value(doc_id, ['date'])
            
            if jurisdiction and doc_number:
                family_text = f"{jurisdiction}{doc_number}"
                if kind:
                    family_text += f" ({kind})"
                if date:
                    family_text += f" [{date}]"
                simple_family_ids.append(family_text)
        
        if lens_id:
            simple_family_lens_ids.append(lens_id)
    
    patent_data['simple_family'] = ", ".join(simple_family_ids) if simple_family_ids else None
    patent_data['simple_family_lens_ids'] = ", ".join(simple_family_lens_ids) if simple_family_lens_ids else None
    patent_data['simple_family_size'] = simple_family_size
    
    # Extended Family
    extended_family = extract_nested_value(patent, ['families', 'extended_family', 'members'], [])
    extended_family_size = extract_nested_value(patent, ['families', 'extended_family', 'size'])
    extended_family_ids = []
    extended_family_lens_ids = []
    
    for member in extended_family:
        doc_id = extract_nested_value(member, ['document_id'], {})
        lens_id = extract_nested_value(member, ['lens_id'])
        
        if doc_id:
            jurisdiction = extract_nested_value(doc_id, ['jurisdiction'])
            doc_number = extract_nested_value(doc_id, ['doc_number'])
            kind = extract_nested_value(doc_id, ['kind'])
            date = extract_nested_value(doc_id, ['date'])
            
            if jurisdiction and doc_number:
                family_text = f"{jurisdiction}{doc_number}"
                if kind:
                    family_text += f" ({kind})"
                if date:
                    family_text += f" [{date}]"
                extended_family_ids.append(family_text)
        
        if lens_id:
            extended_family_lens_ids.append(lens_id)
    
    patent_data['extended_family'] = ", ".join(extended_family_ids) if extended_family_ids else None
    patent_data['extended_family_lens_ids'] = ", ".join(extended_family_lens_ids) if extended_family_lens_ids else None
    patent_data['extended_family_size'] = extended_family_size
    
    # Claims
    claims = extract_nested_value(patent, ['claims'], [])
    if claims and len(claims) > 0:
        claim_text_list = []
        for claim_group in claims:
            for claim in extract_nested_value(claim_group, ['claims'], []):
                claim_text = extract_nested_value(claim, ['claim_text'], [])
                if claim_text:
                    claim_text_list.extend(claim_text)
        
        if claim_text_list:
            patent_data['claims'] = " ".join(claim_text_list)
    
    return patent_data

def main():
    try:
        # Fix potential issues with the path
        safe_folder_path = Path(input_folder)
        
        # List all JSON files in the specified folder
        json_files = [f for f in os.listdir(safe_folder_path) if f.endswith('.json')]
        
        if not json_files:
            print(f"No JSON files found in folder {input_folder}.")
            return
        
        # For all patent data
        all_patent_data = []
        
        for json_file in json_files:
            json_file_path = safe_folder_path / json_file
            print(f"Processing {json_file}...")
            
            try:
                # Load JSON file
                with open(json_file_path, 'r', encoding='utf-8') as file:
                    data = json.load(file)
                
                # Extract data
                for patent in data:
                    patent_data = extract_patent_data(patent)
                    
                    # Add source file information
                    patent_data['source_file'] = json_file
                    
                    all_patent_data.append(patent_data)
                
                print(f"  {len(data)} patents successfully processed from {json_file}.")
                
            except Exception as e:
                print(f"  Error processing {json_file}: {str(e)}")
                continue
        
        if not all_patent_data:
            print("No patent data could be extracted.")
            return
        
        # Create DataFrame
        df = pd.DataFrame(all_patent_data)
        
        # Save Excel file
        df.to_excel(output_excel, index=False)
        
        print(f"\nData successfully saved to {output_excel}.")
        print(f"Number of extracted patents: {len(df)}")
        
        # Output a summary of the extracted data
        print("\nData Summary:")
        print(f"Columns: {', '.join(df.columns)}")
        print(f"Number of processed files: {len(json_files)}")
        
    except Exception as e:
        print(f"An error occurred: {str(e)}")

if __name__ == "__main__":
    main()


#%%


#======================================================================================================================================================
# Reduce the SIB dataset to only families having at least one granted patent and one family-level forward citation
#======================================================================================================================================================

import pandas as pd
import numpy as np
import time
import re
import sys
from datetime import datetime

# Try importing tqdm for progress bars
try:
    from tqdm import tqdm
    tqdm.pandas()
except ImportError:
    print("Installing tqdm for progress bars...")
    import pip
    pip.main(['install', 'tqdm'])
    from tqdm import tqdm
    tqdm.pandas()

# File paths for SIB patent dataset
all_family_members_file = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\full_SIB_data_all_family_members.xlsx"
core_patents_file = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\sib_data_triadic.xlsx"
output_file_granted = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_patents_triadic_granted.xlsx"
output_file_final = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_patents_triadic_granted_forwcit.xlsx"

# New output files for excluded patents
output_file_excluded_not_granted = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_patents_triadic_excluded_not_granted.xlsx"
output_file_excluded_no_citations = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_patents_triadic_excluded_no_citations.xlsx"


def split_family_members(family_str):
    """Split family members string with proper error handling"""
    if pd.isna(family_str):
        return []
    
    try:
        # Try splitting by ";;"
        members = [member.strip() for member in family_str.split(";;")]
        return members
    except:
        try:
            # Fall back to comma splitting if ";;" fails
            members = [member.strip() for member in str(family_str).split(",")]
            return members
        except:
            # Return empty list if all splitting methods fail
            return []

def sum_family_citations(row, sib_full_family_members):
    """Sum citation counts for a single row's family members efficiently"""
    family_members = split_family_members(row["Simple Family Members"])
    
    if not family_members:
        return 0
    
    # Filter family members dataframe once for all members in this family
    matching_members = sib_full_family_members[sib_full_family_members["lens_id"].isin(family_members)]
    
    # Convert citation counts to numeric and sum, handling any non-numeric values
    citation_col = "citing_patents_count"
    if citation_col in matching_members.columns:
        # Convert to numeric, coercing errors to NaN
        numeric_counts = pd.to_numeric(matching_members[citation_col], errors='coerce')
        # Sum all non-NaN values
        return numeric_counts.sum(skipna=True)
    return 0

def extract_family_data(row, sib_full_family_members):
    """Extract kind codes, publication dates, and jurisdictions for all members of a patent family"""
    family_members = split_family_members(row["Simple Family Members"])
    
    if not family_members:
        return {
            "kind_codes": "",
            "pub_dates": "",
            "jurisdictions": ""
        }
    
    # Filter family members dataframe once for all members in this family
    matching_members = sib_full_family_members[sib_full_family_members["lens_id"].isin(family_members)]
    
    # Extract publication kind codes, publication dates, and jurisdictions using the exact column names
    kind_codes = []
    pub_dates = []
    jurisdictions = []
    
    # Use the exact column names provided
    if "pub_kind" in matching_members.columns:
        kind_codes = matching_members["pub_kind"].dropna().tolist()
    
    if "pub_date" in matching_members.columns:
        pub_dates = matching_members["pub_date"].dropna().tolist()
    
    if "pub_jurisdiction" in matching_members.columns:
        jurisdictions = matching_members["pub_jurisdiction"].dropna().tolist()
    
    # Join data with ";;" separator
    return {
        "kind_codes": ";;" if not kind_codes else ";;".join(map(str, kind_codes)),
        "pub_dates": ";;" if not pub_dates else ";;".join(map(str, pub_dates)),
        "jurisdictions": ";;" if not jurisdictions else ";;".join(map(str, jurisdictions))
    }


#=============================================================================
# Checking for granted patents in traidic dataset
#=============================================================================

def contains_granted_patent(family_data):
    """
    Check if a patent family contains at least one granted patent based on specific criteria:
    
    1. US patents:
       - Before Jan 2, 2001: Kind Code A* = granted
       - After Jan 2, 2001: Kind Codes B*, C*, E* = granted
    2. JP patents: Kind Codes B1, B2, B6 = granted
    3. EP patents: Kind Codes B*, C* = granted
    """
    if not isinstance(family_data, dict):
        return False
    
    # Get kind codes, pub dates, and jurisdictions
    kind_codes_str = family_data.get("kind_codes", "")
    pub_dates_str = family_data.get("pub_dates", "")
    jurisdictions_str = family_data.get("jurisdictions", "")
    
    if pd.isna(kind_codes_str) or kind_codes_str == "":
        return False
    
    # Split the data
    kind_codes = kind_codes_str.split(";;") if ';;' in kind_codes_str else [kind_codes_str]
    
    # Handle different date formats and separators for pub_dates
    pub_dates = []
    if not pd.isna(pub_dates_str) and pub_dates_str != "":
        if ';;' in pub_dates_str:
            pub_dates = pub_dates_str.split(";;")
        else:
            pub_dates = [pub_dates_str]
    
    # Handle different formats for jurisdictions
    jurisdictions = []
    if not pd.isna(jurisdictions_str) and jurisdictions_str != "":
        if ';;' in jurisdictions_str:
            jurisdictions = jurisdictions_str.split(";;")
        else:
            jurisdictions = [jurisdictions_str]
    
    # Define the cutoff date for US patents (January 2, 2001)
    us_cutoff_date = datetime(2001, 1, 2)
    
    # If we have different lengths, we need to align the data properly
    # Ideally, we'd have the same number of kind codes, pub dates, and jurisdictions
    max_length = max(len(kind_codes), len(pub_dates), len(jurisdictions))
    min_length = min(len(kind_codes), len(pub_dates), len(jurisdictions))
    
    # If all lists are empty, there's nothing to check
    if max_length == 0:
        return False
    
    # If we have mismatched list lengths but at least some data in each category,
    # use the shortest length to ensure we're matching related data
    check_length = min_length if min_length > 0 else max_length
    
    # If one of the lists is empty (e.g., no jurisdictions), but we have kind codes,
    # check if any kind code is a clear indicator of a granted patent
    if len(kind_codes) > 0 and (len(pub_dates) == 0 or len(jurisdictions) == 0):
        for kind_code in kind_codes:
            kind_code = str(kind_code).strip().upper()
            # B1, B2, etc. are common indicators of granted patents across jurisdictions
            if (re.match(r'B\d', kind_code) or 
                re.match(r'C\d', kind_code)):
                return True
    
    # Check each patent in the family up to the determined length
    for i in range(check_length):
        # Get data safely
        kind_code = str(kind_codes[i]).strip().upper() if i < len(kind_codes) else ""
        jurisdiction = str(jurisdictions[i]).strip().upper() if i < len(jurisdictions) else ""
        
        try:
            pub_date = pd.to_datetime(pub_dates[i]) if i < len(pub_dates) else None
        except:
            pub_date = None
        
        # Check US patents
        if jurisdiction == "US":
            # Before Jan 2, 2001: A* = granted
            if pub_date is not None and pub_date < us_cutoff_date:
                if kind_code.startswith('A'):
                    return True
            # After Jan 2, 2001: B*, C*, E* = granted
            else:
                if (re.match(r'B\d', kind_code) or 
                    re.match(r'C\d', kind_code) or 
                    re.match(r'E\d?', kind_code)):
                    return True
        
        # Check JP patents
        elif jurisdiction == "JP":
            if kind_code in ["B1", "B2", "B6"]:
                return True
        
        # Check EP patents
        elif jurisdiction == "EP":
            if (re.match(r'B\d', kind_code) or re.match(r'C\d', kind_code)):
                return True
    
    return False




def main():
    try:
        total_start_time = time.time()
        
        # STEP 1: Load the datasets
        print("\n==== STEP 1: LOADING DATASETS ====")
        
        # Load the full family members dataset
        print("Loading full family members dataset...")
        load_start = time.time()
        sib_full_family_members = pd.read_excel(all_family_members_file)
        print(f"Full family members dataset loaded successfully with {len(sib_full_family_members)} rows in {time.time() - load_start:.2f} seconds.")
        
        # Print column names to help with debugging
        print("\nColumns in full family members dataset:")
        print(", ".join(sib_full_family_members.columns.tolist()))
        
        # Load the full SIB dataset
        print("\nLoading full SIB dataset...")
        load_start = time.time()
        sib_full_data = pd.read_excel(core_patents_file)
        original_count = len(sib_full_data)
        print(f"Full SIB dataset loaded successfully with {original_count} rows in {time.time() - load_start:.2f} seconds.")
        
        # Print column names to help with debugging
        print("\nColumns in full SIB dataset:")
        print(", ".join(sib_full_data.columns.tolist()))
        
        # Check if "Simple Family Members" column exists
        if "Simple Family Members" not in sib_full_data.columns:
            possible_columns = [col for col in sib_full_data.columns if 'family' in col.lower() or 'member' in col.lower()]
            if possible_columns:
                print(f"\nWARNING: 'Simple Family Members' column not found. Similar columns: {', '.join(possible_columns)}")
                # Try to use the first similar column instead
                family_column = possible_columns[0]
                print(f"Using '{family_column}' instead of 'Simple Family Members'")
                sib_full_data = sib_full_data.rename(columns={family_column: "Simple Family Members"})
            else:
                print("\nERROR: 'Simple Family Members' column not found and no similar columns available.")
                raise ValueError("'Simple Family Members' column is required but not found in the dataset")
        
        # Remove duplicates in Simple Family Members column
        print("\nRemoving duplicates based on 'Simple Family Members'...")
        sib_full_data = sib_full_data.drop_duplicates(subset=["Simple Family Members"], keep="first")
        print(f"Dataset after duplicate removal: {len(sib_full_data)} rows (removed {original_count - len(sib_full_data)} duplicates).")
        
        # STEP 2: Extract family data and filter for granted patents
        print("\n==== STEP 2: EXTRACTING FAMILY DATA AND FILTERING FOR GRANTED PATENTS ====")
        print("Extracting publication kind codes, dates, and jurisdictions for all family members...")
        
        # Initialize tqdm for pandas
        tqdm.pandas(desc="Extracting family data")
        
        # Create new columns for family data with progress bar
        start_time = time.time()
        
        # Process the full dataset with the correct column names
        print("\nProcessing full dataset using columns: pub_kind, pub_date, pub_jurisdiction")
        
        family_data = sib_full_data.progress_apply(
            lambda row: extract_family_data(row, sib_full_family_members),
            axis=1
        )
        

        # Extract data from dictionaries and add to dataframe
        sib_full_data["Family kind codes"] = family_data.apply(lambda x: x.get("kind_codes", ""))
        sib_full_data["Family pub dates"] = family_data.apply(lambda x: x.get("pub_dates", ""))
        sib_full_data["Family jurisdictions"] = family_data.apply(lambda x: x.get("jurisdictions", ""))
        
        print(f"Family data extraction completed in {time.time() - start_time:.2f} seconds.")
        
        # Create composite family data for granted patent check
        family_data_for_check = family_data.copy()
        
        # Count patents before filtering
        before_filtering = len(sib_full_data)
        print(f"Patents before filtering for granted status: {before_filtering}")
        
        
        # Vor der Anwendung der contains_granted_patent-Funktion
        print("\nSample of family_data_for_check (first 5 records):")
        print(family_data_for_check.head().to_string())
        
        # Create a mask for patents with granted patents in their family
        tqdm.pandas(desc="Checking for granted patents")
        granted_mask = family_data_for_check.progress_apply(contains_granted_patent)
        
        # Filter for granted patents only
        print("Filtering for patent families with at least one granted patent...")
        sib_full_data_granted = sib_full_data[granted_mask]
        
        # Get excluded patents (those without granted patents in their family)
        sib_full_data_excluded_not_granted = sib_full_data[~granted_mask]
        
        # Save excluded patents to Excel
        sib_full_data_excluded_not_granted.to_excel(output_file_excluded_not_granted, index=False)
        print(f"Excluded patents (no granted patents) saved to: {output_file_excluded_not_granted}")
        
        # Count patents after filtering
        after_filtering = len(sib_full_data_granted)
        print(f"Patents after filtering for granted status: {after_filtering}")
        print(f"Removed {before_filtering - after_filtering} patents without granted family members.")
        
        # Save intermediate result
        sib_full_data_granted.to_excel(output_file_granted, index=False)
        print(f"Granted patents dataset saved to: {output_file_granted}")
        
        # STEP 3: Calculate family-level citation counts and filter by citations
        print("\n==== STEP 3: CALCULATING FAMILY-LEVEL CITATION COUNTS AND FILTERING BY CITATIONS ====")
        print("Calculating family level citation counts...")
        
        # Initialize tqdm for pandas again
        tqdm.pandas(desc="Counting citations")
        
        # Create new column for family level citation count with progress bar
        citation_start = time.time()
        sib_full_data_granted["Family level citing count"] = sib_full_data_granted.progress_apply(
            lambda row: sum_family_citations(row, sib_full_family_members),
            axis=1
        )
        
        citation_time = time.time() - citation_start
        print(f"Citation counting completed in {citation_time:.2f} seconds.")
        
        # Count patents with citations
        patents_with_citations = (sib_full_data_granted["Family level citing count"] > 0).sum()
        print(f"Patents with family-level citations: {patents_with_citations} out of {len(sib_full_data_granted)}")
        print(f"Total citations across all patent families: {sib_full_data_granted['Family level citing count'].sum()}")
        
        # Count patents before citation filtering
        before_citation_filtering = len(sib_full_data_granted)
        print(f"Patents before filtering by citation count: {before_citation_filtering}")
        
        # Create a mask for patents with at least one citation
        citation_mask = sib_full_data_granted["Family level citing count"] > 0
        
        # Filter patents with at least one citation
        print("Filtering for patents with at least one citation...")
        sib_full_data_triad_granted_forwcit = sib_full_data_granted[citation_mask]
        
        # Get excluded patents (those without citations)
        sib_full_data_excluded_no_citations = sib_full_data_granted[~citation_mask]
        
        # Save excluded patents to Excel
        sib_full_data_excluded_no_citations.to_excel(output_file_excluded_no_citations, index=False)
        print(f"Excluded patents (no citations) saved to: {output_file_excluded_no_citations}")
        
        # Count patents after citation filtering
        after_citation_filtering = len(sib_full_data_triad_granted_forwcit)
        print(f"Patents after filtering by citation count: {after_citation_filtering}")
        print(f"Removed {before_citation_filtering - after_citation_filtering} patents without citations.")
        
        # Save final result
        sib_full_data_triad_granted_forwcit.to_excel(output_file_final, index=False)
        
        # Calculate total processing time
        total_time = time.time() - total_start_time
        print(f"\nFinal dataset saved to: {output_file_final}")
        print(f"Total processing time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
        
        # SUMMARY: Print filtering statistics
        print("\n==== SUMMARY OF FILTERING STEPS ====")
        print(f"1. Initial dataset: {original_count} patents")
        print(f"2. After duplicate removal: {before_filtering} patents (-{original_count - before_filtering})")
        print(f"3. After filtering for granted patents: {after_filtering} patents (-{before_filtering - after_filtering})")
        print(f"   - Excluded patents without granted status saved to: {output_file_excluded_not_granted}")
        print(f"4. After filtering for citations: {after_citation_filtering} patents (-{before_citation_filtering - after_citation_filtering})")
        print(f"   - Excluded patents without citations saved to: {output_file_excluded_no_citations}")
        print(f"Final dataset contains {after_citation_filtering} patents ({after_citation_filtering/original_count*100:.2f}% of original)")
    
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()



#======================================================================================================================================================
# Adding claim data to our filtered SIB full patent family dataset - OPTIMIZED VERSION
#======================================================================================================================================================

import pandas as pd
import numpy as np
import re
import time
import sys
from datetime import datetime

# Try importing tqdm for progress bars
try:
    from tqdm import tqdm
    tqdm.pandas()
except ImportError:
    print("Installing tqdm for progress bars...")
    import pip
    pip.main(['install', 'tqdm'])
    from tqdm import tqdm
    tqdm.pandas()

# Try importing langdetect for better language detection
try:
    from langdetect import detect
    from langdetect.lang_detect_exception import LangDetectException
    has_langdetect = True
    print("Using langdetect for language detection")
except ImportError:
    has_langdetect = False
    print("Warning: langdetect not installed. Using simpler language detection.")

# 1. Reading the datasets with raw strings
sib_patents_cleaned_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_patents_triadic_granted_forwcit.xlsx"
full_sib_data_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\full_SIB_data_all_family_members.xlsx"

# Reading the data
print("Reading datasets...")
start_time = time.time()
sib_patents_cleaned = pd.read_excel(sib_patents_cleaned_path)
full_sib_data = pd.read_excel(full_sib_data_path)
print(f"Datasets loaded in {time.time() - start_time:.2f} seconds")

# Create a lookup dictionary for claims and application dates
print("Creating lookup dictionaries for claims and application dates...")
claims_dict = {}
app_date_dict = {}

# Process all rows in full_sib_data to populate our dictionaries
for idx, row in tqdm(full_sib_data.iterrows(), total=len(full_sib_data), desc="Processing family members"):
    lens_id = str(row['lens_id'])
    claim = row.get('claims', None)
    app_date = row.get('app_date', None)
    
    # Store claim if it exists and is a string
    if pd.notna(claim) and isinstance(claim, str):
        claims_dict[lens_id] = claim
    
    # Store application date if it exists
    if pd.notna(app_date):
        # Convert to datetime object if it's not already
        if not isinstance(app_date, pd.Timestamp) and not isinstance(app_date, datetime):
            try:
                # Try to parse common date formats
                app_date = pd.to_datetime(app_date)
            except:
                print(f"Warning: Could not parse app_date for {lens_id}: {app_date}")
                app_date = None
        
        if app_date is not None:
            app_date_dict[lens_id] = app_date

# Enhanced function to detect if text is in English
def is_english(text):
    """
    Detect if text is in English language with higher accuracy.
    Returns True if English, False otherwise or if detection fails.
    """
    if pd.isna(text) or not isinstance(text, str) or len(text.strip()) < 10:
        return False
        
    try:
        # More comprehensive list of English words/patterns
        english_pattern = r'\b(the|and|of|to|in|is|for|with|as|on|at|by|an|this|that|which|claim|claims|wherein|comprising|comprises|consists|method|system|device|apparatus|according)\b'
        matches = re.findall(english_pattern, text.lower())
        
        # Check for non-English characters (common in European languages)
        non_english_chars = r'[äöüßéèêëàáâäãåçñøæœ]'
        non_english_matches = re.findall(non_english_chars, text.lower())
        
        # Check for language-specific patterns
        german_pattern = r'\b(der|die|das|ein|eine|zu|für|mit|und|oder|wobei|anspruch|verfahren|vorrichtung|gemäß)\b'
        french_pattern = r'\b(le|la|les|un|une|pour|avec|et|ou|selon|revendication|procédé|dispositif|caractérisé)\b'
        
        german_matches = re.findall(german_pattern, text.lower())
        french_matches = re.findall(french_pattern, text.lower())
        
        # Stronger heuristics for English detection
        is_likely_english = len(matches) >= 7 and len(non_english_matches) < 3 and len(german_matches) < 3 and len(french_matches) < 3
        
        # If using langdetect and our heuristics suggest it might be English, confirm with langdetect
        if has_langdetect and (is_likely_english or (len(matches) > 5 and len(non_english_matches) < 5)):
            try:
                # Use a sample of the text to speed up detection
                # Take multiple samples from different parts for better accuracy
                start_sample = text[:min(len(text), 500)]
                mid_sample = text[len(text)//2:len(text)//2 + min(500, len(text)-len(text)//2)] if len(text) > 1000 else ""
                end_sample = text[-min(500, len(text)):] if len(text) > 1000 else ""
                
                # Combine samples
                sample_text = start_sample
                if mid_sample:
                    sample_text += " " + mid_sample
                if end_sample and end_sample != start_sample:
                    sample_text += " " + end_sample
                
                detected_lang = detect(sample_text)
                return detected_lang == 'en'
            except LangDetectException:
                # If langdetect fails, fall back to pattern matching
                return is_likely_english
        else:
            # Without langdetect: more stringent checks
            return is_likely_english
            
    except Exception as e:
        print(f"Error in language detection: {str(e)}")
        # If language detection fails, be conservative and return False
        return False

# Helper function to split family members with proper error handling
def split_family_members(family_str):
    """Split family members string with proper error handling"""
    if pd.isna(family_str):
        return []
    
    try:
        # Try splitting by ";;"
        members = [member.strip() for member in family_str.split(";;")]
        # Filter out empty strings
        members = [m for m in members if m]
        return members
    except:
        try:
            # Fall back to comma splitting if ";;" fails
            members = [member.strip() for member in str(family_str).split(",")]
            # Filter out empty strings
            members = [m for m in members if m]
            return members
        except:
            # Return empty list if all splitting methods fail
            return []

# New function to find the most recent patent with English claims in a family
def find_newest_english_claim_for_family(family_members_str):
    """
    Find the most recent patent with English claims from the family members.
    Returns a tuple of (claim, source_id, app_date).
    Returns (np.nan, np.nan, np.nan) if no English claim is found.
    """
    if pd.isna(family_members_str):
        return np.nan, np.nan, np.nan
    
    # Split family members
    family_members = split_family_members(family_members_str)
    
    if not family_members:
        return np.nan, np.nan, np.nan
    
    # Track all members with English claims and their application dates
    english_claims_members = []
    
    # First pass: identify all members with English claims
    for member in family_members:
        member = member.strip()  # Remove any whitespace
        
        # Skip if member not in either dictionary
        if member not in claims_dict:
            continue
            
        claim = claims_dict[member]
        
        # Skip if claim is not a string or is too short
        if not isinstance(claim, str) or len(claim.strip()) < 10:
            continue
            
        # Check if the claim is in English
        if is_english(claim):
            # Add this member to our list of English claims sources
            # We'll need to sort by app_date later
            app_date = app_date_dict.get(member, None)
            english_claims_members.append((member, claim, app_date))
    
    # If we have members with English claims, sort by application date (newest first)
    if english_claims_members:
        # Filter to only members that have a valid application date
        dated_members = [(m, c, d) for m, c, d in english_claims_members if d is not None]
        
        if dated_members:
            # Sort by date (newest first)
            sorted_members = sorted(dated_members, key=lambda x: x[2], reverse=True)
            newest_member, newest_claim, newest_date = sorted_members[0]
            return newest_claim, newest_member, newest_date
        else:
            # If none have dates, just take the first one
            member, claim, _ = english_claims_members[0]
            return claim, member, np.nan
    
    # Return np.nan if no English claim is found
    return np.nan, np.nan, np.nan

# 2. Process all rows in SIB_patents_cleaned and add claims with progress bar
print("Adding newest English claims to patent families...")
start_time = time.time()

# Create a function to apply that returns all three values
def process_family(family_str):
    claim, source_id, app_date = find_newest_english_claim_for_family(family_str)
    return pd.Series([claim, source_id, app_date])

# Use tqdm for progress tracking
tqdm.pandas(desc="Finding newest English claims")
results = sib_patents_cleaned['Simple Family Members'].progress_apply(process_family)

# Assign the results to new columns
sib_patents_cleaned['claims'] = results[0]
sib_patents_cleaned['claims_source_id'] = results[1]
sib_patents_cleaned['claims_source_date'] = results[2]

# Add a column to indicate the language
sib_patents_cleaned['claims_language'] = sib_patents_cleaned['claims'].apply(
    lambda x: 'en' if pd.notna(x) and is_english(x) else 'none'
)

# Verify language detection on a sample of claims
if sib_patents_cleaned['claims'].notna().sum() > 0:
    print("\nVerifying language detection on 5 random samples:")
    sample_claims = sib_patents_cleaned[sib_patents_cleaned['claims'].notna()].sample(min(5, sib_patents_cleaned['claims'].notna().sum()))
    for i, (_, row) in enumerate(sample_claims.iterrows()):
        claim_preview = row['claims'][:150] + "..." if len(row['claims']) > 150 else row['claims']
        source_id = row['claims_source_id']
        source_date = row['claims_source_date']
        print(f"Sample {i+1} (source: {source_id}, date: {source_date}):\n{claim_preview}\n")

# Diagnostic check - find any non-English claims that might have slipped through
non_english_claims = sib_patents_cleaned[(sib_patents_cleaned['claims'].notna()) & 
                                       (sib_patents_cleaned['claims_language'] == 'none')]

if len(non_english_claims) > 0:
    print(f"\nWARNING: Found {len(non_english_claims)} claims that may not be in English")
    for i, (_, row) in enumerate(non_english_claims.head(3).iterrows()):
        claim_preview = row['claims'][:150] + "..." if len(row['claims']) > 150 else row['claims']
        print(f"Non-English sample {i+1}:\n{claim_preview}\n")
    
    # Remove non-English claims
    print("Removing non-English claims...")
    sib_patents_cleaned.loc[sib_patents_cleaned['claims_language'] == 'none', 'claims'] = np.nan
    sib_patents_cleaned.loc[sib_patents_cleaned['claims_language'] == 'none', 'claims_source_id'] = np.nan
    sib_patents_cleaned.loc[sib_patents_cleaned['claims_language'] == 'none', 'claims_source_date'] = np.nan

# Split the dataframe into two: one with claims and one without
patents_with_claims = sib_patents_cleaned[sib_patents_cleaned['claims'].notna()]
patents_without_claims = sib_patents_cleaned[sib_patents_cleaned['claims'].isna()]

# Save the updated data to two separate Excel files
with_claims_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_patents_cleaned_with_claims.xlsx"
no_claims_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_patents_cleaned_no_claims_newest.xlsx"

print(f"\nSaving dataset with English claims from newest patents to: {with_claims_path}")
patents_with_claims.to_excel(with_claims_path, index=False)

print(f"Saving dataset without English claims to: {no_claims_path}")
patents_without_claims.to_excel(no_claims_path, index=False)

# Report detailed statistics
processing_time = time.time() - start_time
claims_count = sib_patents_cleaned['claims'].notna().sum()
total_count = len(sib_patents_cleaned)

# Additional statistics about dates
date_available = sib_patents_cleaned['claims_source_date'].notna().sum()

print("\n===== SUMMARY =====")
print(f"Processing time: {processing_time:.2f} seconds ({processing_time/60:.2f} minutes)")
print(f"Total patent families processed: {total_count}")
print(f"Patents with English claims: {claims_count} ({claims_count/total_count*100:.2f}%)")
print(f"Patents with English claims and date info: {date_available} ({date_available/claims_count*100:.2f}% of patents with claims)")
print(f"Patents without English claims: {total_count - claims_count} ({(total_count - claims_count)/total_count*100:.2f}%)")
print("=====================")





#===================================================================================
# Adding Citing Lens IDs at family level
#===================================================================================

import pandas as pd

# Load both datasets
def process_library_data(path_to_main_file, path_to_family_members_file):
    """
    Process LIB patent data and extract citing patents from family members.
    
    Args:
        path_to_main_file: Path to the main Excel file with LIB patent data
        path_to_family_members_file: Path to the Excel file with family members data
    
    Returns:
        DataFrame with processed data including new 'family_citing_lens_id' column
    """
    # Read the Excel files
    print("Reading input files...")
    SIB_full_data = pd.read_excel(path_to_main_file)
    SIB_full_data_all_family_members = pd.read_excel(path_to_family_members_file)
    
    # Create a dictionary for faster lookup of citing patents by lens_id
    print("Creating lookup dictionary for citing patents...")
    citing_patents_dict = {}
    for index, row in SIB_full_data_all_family_members.iterrows():
        citing_patents_dict[row['lens_id']] = row['citing_lens_ids']
    
    # Initialize new column for family citing lens IDs
    SIB_full_data['family_citing_lens_id'] = ''
    
    # Process each row in the main dataset
    print("Processing simple family members for each patent...")
    for index, row in SIB_full_data.iterrows():
        if pd.notna(row['Simple Family Members']):  # Check if value is not NaN
            # Split the family members by ";;" and strip whitespace
            family_members = [member.strip() for member in str(row['Simple Family Members']).split(';;')]
            
            # Collect all citing patents for these family members
            all_citing_patents = []
            for member in family_members:
                if member in citing_patents_dict and pd.notna(citing_patents_dict[member]):
                    # Add citing patents to the collection if they exist
                    all_citing_patents.append(str(citing_patents_dict[member]))
            
            # Join all citing patents with commas and assign to the new column
            SIB_full_data.at[index, 'family_citing_lens_id'] = ', '.join(all_citing_patents)
        
        # Print progress for every 1000 records
        if index % 1000 == 0:
            print(f"Processed {index} records...")
    
    print("Processing complete!")
    return SIB_full_data

# Example usage
if __name__ == "__main__":
    # Update these paths to your actual file locations
    main_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_patents_cleaned_with_claims.xlsx"
    family_members_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\full_SIB_data_all_family_members.xlsx"
    
    # Process the data
    result_df = process_library_data(main_file_path, family_members_file_path)
    
    # Save the result to a new Excel file
    output_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_patents_final.xlsx"
    result_df.to_excel(output_path, index=False)
    
    print(f"Results saved to: {output_path}")


#%%


#======================================================================================================================================================
# Reduce the SIB dataset to only families having at least one granted patent and one family-level forward citation
#======================================================================================================================================================

import pandas as pd
import numpy as np
import time
import re
import sys
from datetime import datetime

# Try importing tqdm for progress bars
try:
    from tqdm import tqdm
    tqdm.pandas()
except ImportError:
    print("Installing tqdm for progress bars...")
    import pip
    pip.main(['install', 'tqdm'])
    from tqdm import tqdm
    tqdm.pandas()

# File paths for SIB patent dataset
all_family_members_file = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\full_SIB_data_all_family_members.xlsx"
core_patents_file = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\sib_data_CN.xlsx"
output_file_granted = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_patents_CN_granted.xlsx"
output_file_final = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_patents_CN_granted_forwcit.xlsx"

# New output files for excluded patents
output_file_excluded_not_granted = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_patents_CN_excluded_not_granted.xlsx"
output_file_excluded_no_citations = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_patents_CN_excluded_no_citations.xlsx"
debug_file = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\debug_cn_patents.xlsx"

# Global variables for debugging
cn_patent_data = []
kind_code_data = []
jurisdiction_data = []
family_data_examples = []

def split_family_members(family_str):
    """Split family members string with proper error handling"""
    if pd.isna(family_str):
        return []
    
    try:
        # Try splitting by ";;"
        members = [member.strip() for member in family_str.split(";;")]
        return members
    except:
        try:
            # Fall back to comma splitting if ";;" fails
            members = [member.strip() for member in str(family_str).split(",")]
            return members
        except:
            # Return empty list if all splitting methods fail
            return []

def sum_family_citations(row, sib_full_family_members):
    """Sum citation counts for a single row's family members efficiently"""
    family_members = split_family_members(row["Simple Family Members"])
    
    if not family_members:
        return 0
    
    # Filter family members dataframe once for all members in this family
    matching_members = sib_full_family_members[sib_full_family_members["lens_id"].isin(family_members)]
    
    # Convert citation counts to numeric and sum, handling any non-numeric values
    citation_col = "citing_patents_count"
    if citation_col in matching_members.columns:
        # Convert to numeric, coercing errors to NaN
        numeric_counts = pd.to_numeric(matching_members[citation_col], errors='coerce')
        # Sum all non-NaN values
        return numeric_counts.sum(skipna=True)
    return 0

def extract_family_data(row, sib_full_family_members):
    """Extract kind codes, publication dates, and jurisdictions for all members of a patent family"""
    family_members = split_family_members(row["Simple Family Members"])
    
    if not family_members:
        return {
            "kind_codes": "",
            "pub_dates": "",
            "jurisdictions": ""
        }
    
    # Filter family members dataframe once for all members in this family
    matching_members = sib_full_family_members[sib_full_family_members["lens_id"].isin(family_members)]
    
    # Extract publication kind codes, publication dates, and jurisdictions using the exact column names
    kind_codes = []
    pub_dates = []
    jurisdictions = []
    
    # Use the exact column names provided
    if "pub_kind" in matching_members.columns:
        kind_codes = matching_members["pub_kind"].dropna().tolist()
    
    if "pub_date" in matching_members.columns:
        pub_dates = matching_members["pub_date"].dropna().tolist()
    
    if "pub_jurisdiction" in matching_members.columns:
        jurisdictions = matching_members["pub_jurisdiction"].dropna().tolist()
    
    # Join data with ";;" separator
    return {
        "kind_codes": ";;" if not kind_codes else ";;".join(map(str, kind_codes)),
        "pub_dates": ";;" if not pub_dates else ";;".join(map(str, pub_dates)),
        "jurisdictions": ";;" if not jurisdictions else ";;".join(map(str, jurisdictions))
    }

def extract_family_data_with_debug(row, sib_full_family_members):
    """Extract family data and store examples for debugging"""
    result = extract_family_data(row, sib_full_family_members)
    
    # Speichere einige Beispiele für die manuelle Inspektion
    global family_data_examples
    if len(family_data_examples) < 100:  # Begrenze auf 100 Beispiele
        family_data_examples.append(result)
    
    return result

def contains_granted_patent_with_debug(family_data):
    """
    Debug-Version der contains_granted_patent Funktion mit globalen Variablen.
    Prüft, ob eine Patentfamilie mindestens ein erteiltes Patent enthält.
    CN Patente: Kind Codes B*, C* = erteilt
    """
    global cn_patent_data, kind_code_data, jurisdiction_data
    
    # Überprüfe, ob family_data ein Dictionary ist
    if not isinstance(family_data, dict):
        return False
    
    # Hole Kind-Codes, Veröffentlichungsdaten und Jurisdiktionen
    kind_codes_str = family_data.get("kind_codes", "")
    pub_dates_str = family_data.get("pub_dates", "")
    jurisdictions_str = family_data.get("jurisdictions", "")
    
    # Speichere Daten für CN-Patente im globalen Scope für Debugging
    if "CN" in str(jurisdictions_str):
        cn_patent_data.append(family_data.copy())
    
    # Grundlegende Validierung
    if pd.isna(kind_codes_str) or kind_codes_str == "":
        return False
    
    # Teile die Daten auf
    kind_codes = kind_codes_str.split(";;") if ';;' in kind_codes_str else [kind_codes_str]
    
    # Verarbeite verschiedene Datumsformate
    pub_dates = []
    if not pd.isna(pub_dates_str) and pub_dates_str != "":
        if ';;' in pub_dates_str:
            pub_dates = pub_dates_str.split(";;")
        else:
            pub_dates = [pub_dates_str]
    
    # Verarbeite verschiedene Jurisdiktionsformate
    jurisdictions = []
    if not pd.isna(jurisdictions_str) and jurisdictions_str != "":
        if ';;' in jurisdictions_str:
            jurisdictions = jurisdictions_str.split(";;")
        else:
            jurisdictions = [jurisdictions_str]
    
    # Für CN-Patente speichere Kind-Codes und Jurisdictions zum Debuggen
    for i, jurisdiction in enumerate(jurisdictions):
        if "CN" in str(jurisdiction).upper():
            if i < len(kind_codes):
                kind_code_data.append(kind_codes[i])
                jurisdiction_data.append(jurisdiction)
    
    # Wenn Listen unterschiedliche Längen haben, bestimme, wie viele Elemente zu prüfen sind
    max_length = max(len(kind_codes), len(pub_dates), len(jurisdictions))
    min_length = min(len(kind_codes), len(pub_dates), len(jurisdictions))
    
    if max_length == 0:
        return False
    
    check_length = min_length if min_length > 0 else max_length
    
    # Fallback: Wenn eine der Listen leer ist (z.B. keine Jurisdictions), aber Kind-Codes da sind,
    # prüfe, ob irgendein Kind-Code ein klarer Indikator für ein erteiltes Patent ist
    if len(kind_codes) > 0 and (len(pub_dates) == 0 or len(jurisdictions) == 0):
        for kind_code in kind_codes:
            kind_code = str(kind_code).strip().upper()
            # B1, B2, etc. sind übliche Indikatoren für erteilte Patente
            if kind_code.startswith('B') or kind_code.startswith('C'):  # Vereinfachte Prüfung
                return True
    
    # Prüfe jedes Patent in der Familie
    for i in range(check_length):
        # Hole Daten sicher
        kind_code = str(kind_codes[i]).strip().upper() if i < len(kind_codes) else ""
        jurisdiction = str(jurisdictions[i]).strip().upper() if i < len(jurisdictions) else ""
        
        try:
            pub_date = pd.to_datetime(pub_dates[i]) if i < len(pub_dates) else None
        except:
            pub_date = None
        
        # Prüfe CN-Patente
        if jurisdiction == "CN":
            # Vereinfachte Prüfung ohne reguläre Ausdrücke
            if kind_code.startswith('B') or kind_code.startswith('C'):
                return True
    
    return False

def check_cn_patents_directly(family_data_series):
    """Direkte Überprüfung der CN-Patente ohne die normale Funktion"""
    print("\n=== DIREKTE CN-PATENTPRÜFUNG ===")
    cn_patents_found = 0
    granted_cn_patents = 0
    
    for i, data in enumerate(family_data_series):
        if not isinstance(data, dict):
            continue
            
        jurisdictions_str = data.get("jurisdictions", "")
        if "CN" not in str(jurisdictions_str):
            continue
            
        cn_patents_found += 1
        
        # Parse die Daten
        kind_codes_str = data.get("kind_codes", "")
        kind_codes = kind_codes_str.split(";;") if ';;' in kind_codes_str else [kind_codes_str]
        jurisdictions = jurisdictions_str.split(";;") if ';;' in jurisdictions_str else [jurisdictions_str]
        
        # Finde CN-Patente mit B* oder C* Kind-Codes
        for j, jur in enumerate(jurisdictions):
            if "CN" in str(jur).upper() and j < len(kind_codes):
                kind_code = str(kind_codes[j]).strip().upper()
                if kind_code.startswith('B') or kind_code.startswith('C'):
                    granted_cn_patents += 1
                    # Nur die ersten 5 ausgeben
                    if granted_cn_patents <= 5:
                        print(f"  CN Patent gefunden: jurisdiction={jur}, kind_code={kind_code}")
    
    print(f"CN Patente gefunden: {cn_patents_found}")
    print(f"Davon erteilte CN Patente (B*/C*): {granted_cn_patents}")
    
    return cn_patents_found, granted_cn_patents

def analyze_patent_data(family_data_series):
    """Analysiert die Patentdaten im Detail"""
    print("\n=== DETAILANALYSE DER PATENTDATEN ===")
    
    # Zähle Patente nach Jurisdiction
    jurisdiction_counts = {}
    kind_code_counts = {}
    jurisdiction_kind_pairs = []
    
    for data in family_data_series:
        if not isinstance(data, dict):
            continue
            
        jurisdictions_str = data.get("jurisdictions", "")
        kind_codes_str = data.get("kind_codes", "")
        
        if pd.isna(jurisdictions_str) or jurisdictions_str == "":
            continue
            
        jurisdictions = jurisdictions_str.split(";;") if ';;' in jurisdictions_str else [jurisdictions_str]
        kind_codes = kind_codes_str.split(";;") if ';;' in kind_codes_str else [kind_codes_str]
        
        # Zähle Jurisdictions
        for jur in jurisdictions:
            jur = str(jur).strip().upper()
            jurisdiction_counts[jur] = jurisdiction_counts.get(jur, 0) + 1
        
        # Zähle Kind-Codes
        for code in kind_codes:
            code = str(code).strip().upper()
            kind_code_counts[code] = kind_code_counts.get(code, 0) + 1
        
        # Speichere Jurisdiction-Kind-Code-Paare
        for i, jur in enumerate(jurisdictions):
            if i < len(kind_codes):
                jur = str(jur).strip().upper()
                code = str(kind_codes[i]).strip().upper()
                jurisdiction_kind_pairs.append((jur, code))
    
    # Ausgabe der häufigsten Jurisdiktionen
    print("\nTop 10 Jurisdiktionen:")
    for jur, count in sorted(jurisdiction_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"  {jur}: {count}")
    
    # Ausgabe der häufigsten Kind-Codes
    print("\nTop 10 Kind-Codes:")
    for code, count in sorted(kind_code_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"  {code}: {count}")
    
    # Ausgabe der CN-bezogenen Kind-Codes
    print("\nCN-bezogene Kind-Codes:")
    cn_kind_codes = {}
    for jur, code in jurisdiction_kind_pairs:
        if jur == "CN":
            cn_kind_codes[code] = cn_kind_codes.get(code, 0) + 1
    
    for code, count in sorted(cn_kind_codes.items(), key=lambda x: x[1], reverse=True):
        print(f"  {code}: {count}")
    
    # Überprüfe, ob es B*/C* Kind-Codes für CN gibt
    cn_granted_count = 0
    for code, count in cn_kind_codes.items():
        if code.startswith('B') or code.startswith('C'):
            cn_granted_count += count
    
    print(f"\nAnzahl der CN Patente mit B*/C* Kind-Codes: {cn_granted_count}")
    
    return jurisdiction_counts, kind_code_counts, cn_kind_codes

def main():
    try:
        total_start_time = time.time()
        
        # STEP 1: Load the datasets
        print("\n==== STEP 1: LOADING DATASETS ====")
        
        # Load the full family members dataset
        print("Loading full family members dataset...")
        load_start = time.time()
        sib_full_family_members = pd.read_excel(all_family_members_file)
        print(f"Full family members dataset loaded successfully with {len(sib_full_family_members)} rows in {time.time() - load_start:.2f} seconds.")
        
        # Print column names to help with debugging
        print("\nColumns in full family members dataset:")
        print(", ".join(sib_full_family_members.columns.tolist()))
        
        # Load the full SIB dataset
        print("\nLoading full SIB dataset...")
        load_start = time.time()
        sib_full_data = pd.read_excel(core_patents_file)
        original_count = len(sib_full_data)
        print(f"Full SIB dataset loaded successfully with {original_count} rows in {time.time() - load_start:.2f} seconds.")
        
        # Print column names to help with debugging
        print("\nColumns in full SIB dataset:")
        print(", ".join(sib_full_data.columns.tolist()))
        
        # Check if "Simple Family Members" column exists
        if "Simple Family Members" not in sib_full_data.columns:
            possible_columns = [col for col in sib_full_data.columns if 'family' in col.lower() or 'member' in col.lower()]
            if possible_columns:
                print(f"\nWARNING: 'Simple Family Members' column not found. Similar columns: {', '.join(possible_columns)}")
                # Try to use the first similar column instead
                family_column = possible_columns[0]
                print(f"Using '{family_column}' instead of 'Simple Family Members'")
                sib_full_data = sib_full_data.rename(columns={family_column: "Simple Family Members"})
            else:
                print("\nERROR: 'Simple Family Members' column not found and no similar columns available.")
                raise ValueError("'Simple Family Members' column is required but not found in the dataset")
        
        # Remove duplicates in Simple Family Members column
        print("\nRemoving duplicates based on 'Simple Family Members'...")
        sib_full_data = sib_full_data.drop_duplicates(subset=["Simple Family Members"], keep="first")
        print(f"Dataset after duplicate removal: {len(sib_full_data)} rows (removed {original_count - len(sib_full_data)} duplicates).")
        
        # STEP 2: Extract family data and filter for granted patents
        print("\n==== STEP 2: EXTRACTING FAMILY DATA AND FILTERING FOR GRANTED PATENTS ====")
        print("Extracting publication kind codes, dates, and jurisdictions for all family members...")
        
        # Initialize tqdm for pandas
        tqdm.pandas(desc="Extracting family data")
        
        # Create new columns for family data with progress bar
        start_time = time.time()
        
        # Process the full dataset with the correct column names and debugging
        print("\nProcessing full dataset using columns: pub_kind, pub_date, pub_jurisdiction")
        
        family_data = sib_full_data.progress_apply(
            lambda row: extract_family_data_with_debug(row, sib_full_family_members),
            axis=1
        )
        
        # Debugging: Sample der family_data
        print("\nSample of family_data (first 3 records):")
        for i, data in enumerate(family_data.head(3)):
            print(f"Record {i+1}:")
            for key, value in data.items():
                print(f"  {key}: {value[:100]}..." if isinstance(value, str) and len(value) > 100 else f"  {key}: {value}")
        
        # Extract data from dictionaries and add to dataframe
        sib_full_data["Family kind codes"] = family_data.apply(lambda x: x.get("kind_codes", ""))
        sib_full_data["Family pub dates"] = family_data.apply(lambda x: x.get("pub_dates", ""))
        sib_full_data["Family jurisdictions"] = family_data.apply(lambda x: x.get("jurisdictions", ""))
        
        print(f"Family data extraction completed in {time.time() - start_time:.2f} seconds.")
        
        # Debugging: Direkte Prüfung der CN-Patente
        cn_count, granted_cn_count = check_cn_patents_directly(family_data)
        
        # Debugging: Detaillierte Analyse der Patentdaten
        jurisdiction_counts, kind_code_counts, cn_kind_codes = analyze_patent_data(family_data)
        
        # Create composite family data for granted patent check
        family_data_for_check = family_data.copy()
        
        # Count patents before filtering
        before_filtering = len(sib_full_data)
        print(f"Patents before filtering for granted status: {before_filtering}")
        
        # Create a mask for patents with granted patents in their family
        tqdm.pandas(desc="Checking for granted patents")
        granted_mask = family_data_for_check.progress_apply(contains_granted_patent_with_debug)
        
        # Debugging: Speichere gesammelte CN-Patentdaten
        print(f"\nDEBUG: Collected {len(cn_patent_data)} CN patent records")
        
        # Speichere die CN-Patentdaten in Dataframes für einfachere Inspektion
        if len(cn_patent_data) > 0:
            cn_patents_df = pd.DataFrame(cn_patent_data)
            print(f"DEBUG: Saved {len(cn_patents_df)} CN patents to variable 'cn_patents_df'")
            
            # Speichere die CN-Patentdaten auch in eine Excel-Datei für externe Inspektion
            cn_patents_df.to_excel(debug_file, index=False)
            print(f"DEBUG: Saved CN patents to: {debug_file}")
        
        # Erstelle einen Dataframe mit Jurisdictions und Kind-Codes
        if len(kind_code_data) > 0 and len(jurisdiction_data) > 0:
            # Stelle sicher, dass sie gleich lang sind
            min_len = min(len(kind_code_data), len(jurisdiction_data))
            code_jur_df = pd.DataFrame({
                'jurisdiction': jurisdiction_data[:min_len],
                'kind_code': kind_code_data[:min_len]
            })
            print(f"DEBUG: Saved {len(code_jur_df)} kind code/jurisdiction pairs to variable 'code_jur_df'")
            
            # Zähle die Anzahl der B*/C* Kind-Codes für CN-Jurisdiktionen
            cn_entries = code_jur_df[code_jur_df['jurisdiction'].str.contains('CN', case=False)]
            granted_cn_entries = cn_entries[
                cn_entries['kind_code'].str.startswith('B') | 
                cn_entries['kind_code'].str.startswith('C')
            ]
            
            print(f"DEBUG: Found {len(cn_entries)} CN entries in code_jur_df")
            print(f"DEBUG: Of these, {len(granted_cn_entries)} have B*/C* kind codes")
            
            # Ausgabe der ersten 10 CN-Einträge mit B*/C* Kind-Codes
            if len(granted_cn_entries) > 0:
                print("\nDEBUG: First 10 CN entries with B*/C* kind codes:")
                print(granted_cn_entries.head(10))
        
        # Filter for granted patents only
        print("Filtering for patent families with at least one granted patent...")
        sib_full_data_granted = sib_full_data[granted_mask]
        
        # Get excluded patents (those without granted patents in their family)
        sib_full_data_excluded_not_granted = sib_full_data[~granted_mask]
        
        # Save excluded patents to Excel
        sib_full_data_excluded_not_granted.to_excel(output_file_excluded_not_granted, index=False)
        print(f"Excluded patents (no granted patents) saved to: {output_file_excluded_not_granted}")
        
        # Count patents after filtering
        after_filtering = len(sib_full_data_granted)
        print(f"Patents after filtering for granted status: {after_filtering}")
        print(f"Removed {before_filtering - after_filtering} patents without granted family members.")
        
        # Save intermediate result
        sib_full_data_granted.to_excel(output_file_granted, index=False)
        print(f"Granted patents dataset saved to: {output_file_granted}")
        
        # STEP 3: Calculate family-level citation counts and filter by citations
        print("\n==== STEP 3: CALCULATING FAMILY-LEVEL CITATION COUNTS AND FILTERING BY CITATIONS ====")
        print("Calculating family level citation counts...")
        
        # Initialize tqdm for pandas again
        tqdm.pandas(desc="Counting citations")
        
        # Create new column for family level citation count with progress bar
        citation_start = time.time()
        sib_full_data_granted["Family level citing count"] = sib_full_data_granted.progress_apply(
            lambda row: sum_family_citations(row, sib_full_family_members),
            axis=1
        )
        
        citation_time = time.time() - citation_start
        print(f"Citation counting completed in {citation_time:.2f} seconds.")
        
        # Count patents with citations
        patents_with_citations = (sib_full_data_granted["Family level citing count"] > 0).sum()
        print(f"Patents with family-level citations: {patents_with_citations} out of {len(sib_full_data_granted)}")
        print(f"Total citations across all patent families: {sib_full_data_granted['Family level citing count'].sum()}")
        
        # Count patents before citation filtering
        before_citation_filtering = len(sib_full_data_granted)
        print(f"Patents before filtering by citation count: {before_citation_filtering}")
        
        # Create a mask for patents with at least one citation
        citation_mask = sib_full_data_granted["Family level citing count"] > 0
        
        # Filter patents with at least one citation
        print("Filtering for patents with at least one citation...")
        sib_full_data_CN_granted_forwcit = sib_full_data_granted[citation_mask]
        
        # Get excluded patents (those without citations)
        sib_full_data_excluded_no_citations = sib_full_data_granted[~citation_mask]
        
        # Save excluded patents to Excel
        sib_full_data_excluded_no_citations.to_excel(output_file_excluded_no_citations, index=False)
        print(f"Excluded patents (no citations) saved to: {output_file_excluded_no_citations}")
        
        # Count patents after citation filtering
        after_citation_filtering = len(sib_full_data_CN_granted_forwcit)
        print(f"Patents after filtering by citation count: {after_citation_filtering}")
        print(f"Removed {before_citation_filtering - after_citation_filtering} patents without citations.")
        
        # Save final result
        sib_full_data_CN_granted_forwcit.to_excel(output_file_final, index=False)
        
        # Calculate total processing time
        total_time = time.time() - total_start_time
        print(f"\nFinal dataset saved to: {output_file_final}")
        print(f"Total processing time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
        
        # SUMMARY: Print filtering statistics
        print("\n==== SUMMARY OF FILTERING STEPS ====")
        print(f"1. Initial dataset: {original_count} patents")
        print(f"2. After duplicate removal: {before_filtering} patents (-{original_count - before_filtering})")
        print(f"3. After filtering for granted patents: {after_filtering} patents (-{before_filtering - after_filtering})")
        print(f"   - Excluded patents without granted status saved to: {output_file_excluded_not_granted}")
        print(f"4. After filtering for citations: {after_citation_filtering} patents (-{before_citation_filtering - after_citation_filtering})")
        print(f"   - Excluded patents without citations saved to: {output_file_excluded_no_citations}")
        print(f"Final dataset contains {after_citation_filtering} patents ({after_citation_filtering/original_count*100:.2f}% of original)")
        
        # Debugging Summary
        print("\n==== DEBUGGING SUMMARY ====")
        print(f"CN patents found: {cn_count}")
        print(f"CN patents with B*/C* kind codes: {granted_cn_count}")
        print(f"CN patents recognized by function: {after_filtering}")
        
        # Zeige, ob es ein Problem mit der Erkennung gibt
        if granted_cn_count > 0 and after_filtering == 0:
            print("\nWARNING: CN patents with B*/C* kind codes were found, but none were recognized by the function!")
            print("Possible issue with the contains_granted_patent_with_debug function or data alignment.")
        elif granted_cn_count == 0:
            print("\nINFO: No CN patents with B*/C* kind codes were found in the dataset.")
        else:
            print(f"\nINFO: {after_filtering} patents with granted status were recognized.")
    
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()





#======================================================================================================================================================
# Adding claim data to our filtered SIB full patent family dataset - OPTIMIZED VERSION
#======================================================================================================================================================

import pandas as pd
import numpy as np
import re
import time
import sys
from datetime import datetime

# Try importing tqdm for progress bars
try:
    from tqdm import tqdm
    tqdm.pandas()
except ImportError:
    print("Installing tqdm for progress bars...")
    import pip
    pip.main(['install', 'tqdm'])
    from tqdm import tqdm
    tqdm.pandas()

# Try importing langdetect for better language detection
try:
    from langdetect import detect
    from langdetect.lang_detect_exception import LangDetectException
    has_langdetect = True
    print("Using langdetect for language detection")
except ImportError:
    has_langdetect = False
    print("Warning: langdetect not installed. Using simpler language detection.")

# 1. Reading the datasets with raw strings
sib_patents_cleaned_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_patents_CN_granted_forwcit.xlsx"
full_sib_data_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\full_SIB_data_all_family_members.xlsx"

# Reading the data
print("Reading datasets...")
start_time = time.time()
sib_patents_cleaned = pd.read_excel(sib_patents_cleaned_path)
full_sib_data = pd.read_excel(full_sib_data_path)
print(f"Datasets loaded in {time.time() - start_time:.2f} seconds")

# Create a lookup dictionary for claims and application dates
print("Creating lookup dictionaries for claims and application dates...")
claims_dict = {}
app_date_dict = {}

# Process all rows in full_sib_data to populate our dictionaries
for idx, row in tqdm(full_sib_data.iterrows(), total=len(full_sib_data), desc="Processing family members"):
    lens_id = str(row['lens_id'])
    claim = row.get('claims', None)
    app_date = row.get('app_date', None)
    
    # Store claim if it exists and is a string
    if pd.notna(claim) and isinstance(claim, str):
        claims_dict[lens_id] = claim
    
    # Store application date if it exists
    if pd.notna(app_date):
        # Convert to datetime object if it's not already
        if not isinstance(app_date, pd.Timestamp) and not isinstance(app_date, datetime):
            try:
                # Try to parse common date formats
                app_date = pd.to_datetime(app_date)
            except:
                print(f"Warning: Could not parse app_date for {lens_id}: {app_date}")
                app_date = None
        
        if app_date is not None:
            app_date_dict[lens_id] = app_date

# Enhanced function to detect if text is in English
def is_english(text):
    """
    Detect if text is in English language with higher accuracy.
    Returns True if English, False otherwise or if detection fails.
    """
    if pd.isna(text) or not isinstance(text, str) or len(text.strip()) < 10:
        return False
        
    try:
        # More comprehensive list of English words/patterns
        english_pattern = r'\b(the|and|of|to|in|is|for|with|as|on|at|by|an|this|that|which|claim|claims|wherein|comprising|comprises|consists|method|system|device|apparatus|according)\b'
        matches = re.findall(english_pattern, text.lower())
        
        # Check for non-English characters (common in European languages)
        non_english_chars = r'[äöüßéèêëàáâäãåçñøæœ]'
        non_english_matches = re.findall(non_english_chars, text.lower())
        
        # Check for language-specific patterns
        german_pattern = r'\b(der|die|das|ein|eine|zu|für|mit|und|oder|wobei|anspruch|verfahren|vorrichtung|gemäß)\b'
        french_pattern = r'\b(le|la|les|un|une|pour|avec|et|ou|selon|revendication|procédé|dispositif|caractérisé)\b'
        
        german_matches = re.findall(german_pattern, text.lower())
        french_matches = re.findall(french_pattern, text.lower())
        
        # Stronger heuristics for English detection
        is_likely_english = len(matches) >= 7 and len(non_english_matches) < 3 and len(german_matches) < 3 and len(french_matches) < 3
        
        # If using langdetect and our heuristics suggest it might be English, confirm with langdetect
        if has_langdetect and (is_likely_english or (len(matches) > 5 and len(non_english_matches) < 5)):
            try:
                # Use a sample of the text to speed up detection
                # Take multiple samples from different parts for better accuracy
                start_sample = text[:min(len(text), 500)]
                mid_sample = text[len(text)//2:len(text)//2 + min(500, len(text)-len(text)//2)] if len(text) > 1000 else ""
                end_sample = text[-min(500, len(text)):] if len(text) > 1000 else ""
                
                # Combine samples
                sample_text = start_sample
                if mid_sample:
                    sample_text += " " + mid_sample
                if end_sample and end_sample != start_sample:
                    sample_text += " " + end_sample
                
                detected_lang = detect(sample_text)
                return detected_lang == 'en'
            except LangDetectException:
                # If langdetect fails, fall back to pattern matching
                return is_likely_english
        else:
            # Without langdetect: more stringent checks
            return is_likely_english
            
    except Exception as e:
        print(f"Error in language detection: {str(e)}")
        # If language detection fails, be conservative and return False
        return False

# Helper function to split family members with proper error handling
def split_family_members(family_str):
    """Split family members string with proper error handling"""
    if pd.isna(family_str):
        return []
    
    try:
        # Try splitting by ";;"
        members = [member.strip() for member in family_str.split(";;")]
        # Filter out empty strings
        members = [m for m in members if m]
        return members
    except:
        try:
            # Fall back to comma splitting if ";;" fails
            members = [member.strip() for member in str(family_str).split(",")]
            # Filter out empty strings
            members = [m for m in members if m]
            return members
        except:
            # Return empty list if all splitting methods fail
            return []

# New function to find the most recent patent with English claims in a family
def find_newest_english_claim_for_family(family_members_str):
    """
    Find the most recent patent with English claims from the family members.
    Returns a tuple of (claim, source_id, app_date).
    Returns (np.nan, np.nan, np.nan) if no English claim is found.
    """
    if pd.isna(family_members_str):
        return np.nan, np.nan, np.nan
    
    # Split family members
    family_members = split_family_members(family_members_str)
    
    if not family_members:
        return np.nan, np.nan, np.nan
    
    # Track all members with English claims and their application dates
    english_claims_members = []
    
    # First pass: identify all members with English claims
    for member in family_members:
        member = member.strip()  # Remove any whitespace
        
        # Skip if member not in either dictionary
        if member not in claims_dict:
            continue
            
        claim = claims_dict[member]
        
        # Skip if claim is not a string or is too short
        if not isinstance(claim, str) or len(claim.strip()) < 10:
            continue
            
        # Check if the claim is in English
        if is_english(claim):
            # Add this member to our list of English claims sources
            # We'll need to sort by app_date later
            app_date = app_date_dict.get(member, None)
            english_claims_members.append((member, claim, app_date))
    
    # If we have members with English claims, sort by application date (newest first)
    if english_claims_members:
        # Filter to only members that have a valid application date
        dated_members = [(m, c, d) for m, c, d in english_claims_members if d is not None]
        
        if dated_members:
            # Sort by date (newest first)
            sorted_members = sorted(dated_members, key=lambda x: x[2], reverse=True)
            newest_member, newest_claim, newest_date = sorted_members[0]
            return newest_claim, newest_member, newest_date
        else:
            # If none have dates, just take the first one
            member, claim, _ = english_claims_members[0]
            return claim, member, np.nan
    
    # Return np.nan if no English claim is found
    return np.nan, np.nan, np.nan

# 2. Process all rows in SIB_patents_cleaned and add claims with progress bar
print("Adding newest English claims to patent families...")
start_time = time.time()

# Create a function to apply that returns all three values
def process_family(family_str):
    claim, source_id, app_date = find_newest_english_claim_for_family(family_str)
    return pd.Series([claim, source_id, app_date])

# Use tqdm for progress tracking
tqdm.pandas(desc="Finding newest English claims")
results = sib_patents_cleaned['Simple Family Members'].progress_apply(process_family)

# Assign the results to new columns
sib_patents_cleaned['claims'] = results[0]
sib_patents_cleaned['claims_source_id'] = results[1]
sib_patents_cleaned['claims_source_date'] = results[2]

# Add a column to indicate the language
sib_patents_cleaned['claims_language'] = sib_patents_cleaned['claims'].apply(
    lambda x: 'en' if pd.notna(x) and is_english(x) else 'none'
)

# Verify language detection on a sample of claims
if sib_patents_cleaned['claims'].notna().sum() > 0:
    print("\nVerifying language detection on 5 random samples:")
    sample_claims = sib_patents_cleaned[sib_patents_cleaned['claims'].notna()].sample(min(5, sib_patents_cleaned['claims'].notna().sum()))
    for i, (_, row) in enumerate(sample_claims.iterrows()):
        claim_preview = row['claims'][:150] + "..." if len(row['claims']) > 150 else row['claims']
        source_id = row['claims_source_id']
        source_date = row['claims_source_date']
        print(f"Sample {i+1} (source: {source_id}, date: {source_date}):\n{claim_preview}\n")

# Diagnostic check - find any non-English claims that might have slipped through
non_english_claims = sib_patents_cleaned[(sib_patents_cleaned['claims'].notna()) & 
                                       (sib_patents_cleaned['claims_language'] == 'none')]

if len(non_english_claims) > 0:
    print(f"\nWARNING: Found {len(non_english_claims)} claims that may not be in English")
    for i, (_, row) in enumerate(non_english_claims.head(3).iterrows()):
        claim_preview = row['claims'][:150] + "..." if len(row['claims']) > 150 else row['claims']
        print(f"Non-English sample {i+1}:\n{claim_preview}\n")
    
    # Remove non-English claims
    print("Removing non-English claims...")
    sib_patents_cleaned.loc[sib_patents_cleaned['claims_language'] == 'none', 'claims'] = np.nan
    sib_patents_cleaned.loc[sib_patents_cleaned['claims_language'] == 'none', 'claims_source_id'] = np.nan
    sib_patents_cleaned.loc[sib_patents_cleaned['claims_language'] == 'none', 'claims_source_date'] = np.nan

# Split the dataframe into two: one with claims and one without
patents_with_claims = sib_patents_cleaned[sib_patents_cleaned['claims'].notna()]
patents_without_claims = sib_patents_cleaned[sib_patents_cleaned['claims'].isna()]

# Save the updated data to two separate Excel files
with_claims_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_patents_cleaned_with_claims.xlsx"
no_claims_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_patents_cleaned_no_claims_newest.xlsx"

print(f"\nSaving dataset with English claims from newest patents to: {with_claims_path}")
patents_with_claims.to_excel(with_claims_path, index=False)

print(f"Saving dataset without English claims to: {no_claims_path}")
patents_without_claims.to_excel(no_claims_path, index=False)

# Report detailed statistics
processing_time = time.time() - start_time
claims_count = sib_patents_cleaned['claims'].notna().sum()
total_count = len(sib_patents_cleaned)

# Additional statistics about dates
date_available = sib_patents_cleaned['claims_source_date'].notna().sum()

print("\n===== SUMMARY =====")
print(f"Processing time: {processing_time:.2f} seconds ({processing_time/60:.2f} minutes)")
print(f"Total patent families processed: {total_count}")
print(f"Patents with English claims: {claims_count} ({claims_count/total_count*100:.2f}%)")
print(f"Patents with English claims and date info: {date_available} ({date_available/claims_count*100:.2f}% of patents with claims)")
print(f"Patents without English claims: {total_count - claims_count} ({(total_count - claims_count)/total_count*100:.2f}%)")
print("=====================")





#===================================================================================
# Adding Citing Lens IDs at family level
#===================================================================================

import pandas as pd

# Load both datasets
def process_library_data(path_to_main_file, path_to_family_members_file):
    """
    Process LIB patent data and extract citing patents from family members.
    
    Args:
        path_to_main_file: Path to the main Excel file with LIB patent data
        path_to_family_members_file: Path to the Excel file with family members data
    
    Returns:
        DataFrame with processed data including new 'family_citing_lens_id' column
    """
    # Read the Excel files
    print("Reading input files...")
    SIB_full_data = pd.read_excel(path_to_main_file)
    SIB_full_data_all_family_members = pd.read_excel(path_to_family_members_file)
    
    # Create a dictionary for faster lookup of citing patents by lens_id
    print("Creating lookup dictionary for citing patents...")
    citing_patents_dict = {}
    for index, row in SIB_full_data_all_family_members.iterrows():
        citing_patents_dict[row['lens_id']] = row['citing_lens_ids']
    
    # Initialize new column for family citing lens IDs
    SIB_full_data['family_citing_lens_id'] = ''
    
    # Process each row in the main dataset
    print("Processing simple family members for each patent...")
    for index, row in SIB_full_data.iterrows():
        if pd.notna(row['Simple Family Members']):  # Check if value is not NaN
            # Split the family members by ";;" and strip whitespace
            family_members = [member.strip() for member in str(row['Simple Family Members']).split(';;')]
            
            # Collect all citing patents for these family members
            all_citing_patents = []
            for member in family_members:
                if member in citing_patents_dict and pd.notna(citing_patents_dict[member]):
                    # Add citing patents to the collection if they exist
                    all_citing_patents.append(str(citing_patents_dict[member]))
            
            # Join all citing patents with commas and assign to the new column
            SIB_full_data.at[index, 'family_citing_lens_id'] = ', '.join(all_citing_patents)
        
        # Print progress for every 1000 records
        if index % 1000 == 0:
            print(f"Processed {index} records...")
    
    print("Processing complete!")
    return SIB_full_data

# Example usage
if __name__ == "__main__":
    # Update these paths to your actual file locations
    main_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_patents_cleaned_with_claims.xlsx"
    family_members_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\full_SIB_data_all_family_members.xlsx"
    
    # Process the data
    result_df = process_library_data(main_file_path, family_members_file_path)
    
    # Save the result to a new Excel file
    output_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_patents_final.xlsx"
    result_df.to_excel(output_path, index=False)
    
    print(f"Results saved to: {output_path}")


#%%


#===============================================================================================
# Process priority numbers of CN SIB patents without claims to download claim data from Derwent
#===============================================================================================

import pandas as pd
import os

def process_priority_numbers(excel_file_path, max_chunk_size=10000):
    """
    Process priority numbers from Excel file and split into chunks.
    
    Args:
        excel_file_path (str): Path to the Excel file
        max_chunk_size (int): Maximum characters per chunk (default: 5000)
    """
    
    # Read the Excel file
    print("Reading Excel file...")
    df = pd.read_excel(excel_file_path)
    
    # Get the first column (assuming priority numbers are in the first column)
    # If they're in a specific column, replace df.iloc[:, 0] with df['column_name']
    priority_numbers = df.iloc[:, 0].astype(str)
    
    # Strip whitespace from each priority number and filter out empty/invalid entries
    print("Processing priority numbers...")
    cleaned_priorities = []
    
    for pn in priority_numbers:
        if pd.notna(pn):  # Check if not NaN
            cleaned = str(pn).strip()
            # Only include non-empty strings that don't contain operators
            if cleaned and cleaned not in ['', 'nan', 'NaN', 'OR', 'AND', 'NOT']:
                # Remove any existing operators that might be within the priority number
                cleaned = cleaned.replace(' OR ', ' ').replace(' AND ', ' ').replace(' NOT ', ' ')
                cleaned = ' '.join(cleaned.split())  # Remove extra whitespace
                if cleaned:  # Final check after cleaning
                    cleaned_priorities.append(cleaned)
    
    print(f"Found {len(cleaned_priorities)} priority numbers")
    
    # Join all priority numbers with " OR " and validate the result
    full_string = " OR ".join(cleaned_priorities)
    
    # Clean up any potential consecutive operators
    while " OR  OR " in full_string:
        full_string = full_string.replace(" OR  OR ", " OR ")
    while "  " in full_string:
        full_string = full_string.replace("  ", " ")
    
    # Remove leading/trailing OR operators
    full_string = full_string.strip()
    if full_string.startswith("OR "):
        full_string = full_string[3:]
    if full_string.endswith(" OR"):
        full_string = full_string[:-3]
    
    print(f"Total length of combined string: {len(full_string)} characters")
    
    # Validate that we don't have consecutive operators
    if " OR OR " in full_string or full_string.startswith("OR OR") or full_string.endswith("OR OR"):
        print("Warning: Found consecutive OR operators in the string. This will cause database errors.")
        return None
    
    # Split into chunks without breaking priority numbers
    chunks = []
    current_chunk = ""
    
    # Split the full string by " OR " to work with complete priority numbers
    priority_parts = full_string.split(" OR ")
    
    for i, part in enumerate(priority_parts):
        # Calculate what the length would be if we add this part
        if current_chunk:
            test_length = len(current_chunk) + len(" OR ") + len(part)
        else:
            test_length = len(part)
        
        # If adding this part would exceed the limit, save current chunk and start new one
        if test_length > max_chunk_size and current_chunk:
            chunks.append(current_chunk)
            current_chunk = part
        else:
            # Add the part to current chunk
            if current_chunk:
                current_chunk += " OR " + part
            else:
                current_chunk = part
    
    # Don't forget the last chunk
    if current_chunk:
        chunks.append(current_chunk)
    
    # Create output directory
    output_dir = os.path.dirname(excel_file_path)
    base_filename = os.path.splitext(os.path.basename(excel_file_path))[0]
    
    # Save chunks to separate text files
    print(f"Saving {len(chunks)} chunks...")
    for i, chunk in enumerate(chunks, 1):
        # Final validation of each chunk before saving
        if " OR OR " in chunk or chunk.startswith("OR ") or chunk.endswith(" OR"):
            print(f"Warning: Chunk {i} contains formatting issues that may cause database errors")
        
        output_filename = f"{base_filename}_chunk_{i:03d}.txt"
        output_path = os.path.join(output_dir, output_filename)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(chunk)
        
        print(f"Saved chunk {i}: {len(chunk)} characters -> {output_filename}")
        
        # Show first few characters of each chunk for debugging
        preview = chunk[:100] + "..." if len(chunk) > 100 else chunk
        print(f"  Preview: {preview}")
    
    print("Processing completed successfully!")
    print("\nIMPORTANT: Before using in Derwent database:")
    print("1. Check that no chunk starts or ends with 'OR'")
    print("2. Verify there are no consecutive 'OR OR' operators")
    print("3. Test with a small chunk first to ensure compatibility")
    
    return chunks

# Usage
if __name__ == "__main__":
    # Path to your Excel file
    excel_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\Get Claims Derwent\SIB_priority_numbers_without_claims2.xlsx"
    
    # Process the file
    try:
        chunks = process_priority_numbers(excel_file_path)
        print(f"\nSummary:")
        print(f"- Total chunks created: {len(chunks)}")
        for i, chunk in enumerate(chunks, 1):
            print(f"- Chunk {i}: {len(chunk)} characters")
    
    except FileNotFoundError:
        print(f"Error: Could not find the Excel file at: {excel_file_path}")
        print("Please check the file path and try again.")
    
    except Exception as e:
        print(f"An error occurred: {str(e)}")




#%%
#===============================================================================================
# Adding Derwent claims to Lens Excel file
#===============================================================================================

import pandas as pd
import os
import glob
import re
from pathlib import Path

def normalize_priority_number(priority_num):
    """
    Normalize priority numbers to a consistent format for comparison.
    Converts both "CN200610096873A" and "CN 200610096873 A" to "CN200610096873A"
    """
    if pd.isna(priority_num):
        return ""
    
    # Convert to string and strip whitespace
    pn = str(priority_num).strip()
    
    # Remove all spaces to create a consistent format
    normalized = re.sub(r'\s+', '', pn)
    
    return normalized.upper()

def merge_patent_claims():
    """
    Merge claims from SIB_CN_chunk files into the main patents file.
    """
    
    # Define file paths
    chunk_files_dir = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\Get Claims Derwent"
    main_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_patents_cleaned_no_claims_newest.xlsx"
    
    # Load the main patents file
    print("Loading main patents file...")
    try:
        main_df = pd.read_excel(main_file_path)
        print(f"Loaded main file with {len(main_df)} rows")
    except FileNotFoundError:
        print(f"Error: Main file not found at {main_file_path}")
        return
    except Exception as e:
        print(f"Error loading main file: {str(e)}")
        return
    
    # Check if required columns exist in main file
    if 'Priority Numbers' not in main_df.columns:
        print("Error: 'Priority Numbers' column not found in main file")
        print(f"Available columns: {list(main_df.columns)}")
        return
    
    if 'claims' not in main_df.columns:
        print("Warning: 'claims' column not found in main file. Creating it...")
        main_df['claims'] = ""
    
    # Normalize priority numbers in main file for faster lookup
    print("Normalizing priority numbers in main file...")
    main_df['normalized_priority'] = main_df['Priority Numbers'].apply(normalize_priority_number)
    
    # Find all chunk files
    chunk_pattern = os.path.join(chunk_files_dir, "SIB_CN_chunk*.xlsx")
    chunk_files = glob.glob(chunk_pattern)
    
    if not chunk_files:
        print(f"No chunk files found matching pattern: {chunk_pattern}")
        return
    
    print(f"Found {len(chunk_files)} chunk files to process")
    
    # Create a dictionary to store all claims data for faster lookup
    claims_dict = {}
    
    # Process each chunk file
    for chunk_file in chunk_files:
        chunk_filename = os.path.basename(chunk_file)
        print(f"\nProcessing {chunk_filename}...")
        
        try:
            # Read chunk file, skipping first row, using second row as headers
            chunk_df = pd.read_excel(chunk_file, skiprows=1)
            print(f"  Loaded {len(chunk_df)} rows from {chunk_filename}")
            
            # Check if required columns exist
            if 'Priority Number' not in chunk_df.columns:
                print(f"  Warning: 'Priority Number' column not found in {chunk_filename}")
                print(f"  Available columns: {list(chunk_df.columns)}")
                continue
                
            if 'Claims (English)' not in chunk_df.columns:
                print(f"  Warning: 'Claims (English)' column not found in {chunk_filename}")
                print(f"  Available columns: {list(chunk_df.columns)}")
                continue
            
            # Process each row in the chunk file
            valid_claims = 0
            for idx, row in chunk_df.iterrows():
                priority_num = row['Priority Number']
                claims_text = row['Claims (English)']
                
                # Skip if priority number or claims are empty/NaN
                if pd.isna(priority_num) or pd.isna(claims_text):
                    continue
                
                if str(claims_text).strip() == "":
                    continue
                
                # Normalize the priority number for lookup
                normalized_pn = normalize_priority_number(priority_num)
                
                if normalized_pn:
                    claims_dict[normalized_pn] = str(claims_text).strip()
                    valid_claims += 1
            
            print(f"  Extracted {valid_claims} valid claims from {chunk_filename}")
            
        except Exception as e:
            print(f"  Error processing {chunk_filename}: {str(e)}")
            continue
    
    print(f"\nTotal unique priority numbers with claims: {len(claims_dict)}")
    
    # Match claims to main file
    print("\nMatching claims to main patents file...")
    matches_found = 0
    claims_updated = 0
    
    for idx, row in main_df.iterrows():
        normalized_main_pn = row['normalized_priority']
        
        if normalized_main_pn in claims_dict:
            matches_found += 1
            
            # Only update if claims field is empty
            if pd.isna(row['claims']) or str(row['claims']).strip() == "":
                main_df.at[idx, 'claims'] = claims_dict[normalized_main_pn]
                claims_updated += 1
            else:
                print(f"  Row {idx}: Claims already exist, skipping update")
    
    print(f"Priority number matches found: {matches_found}")
    print(f"Claims updated: {claims_updated}")
    
    # Remove the temporary normalized column
    main_df = main_df.drop('normalized_priority', axis=1)
    
    # Save the updated main file
    output_path = main_file_path.replace('.xlsx', '_with_claims.xlsx')
    print(f"\nSaving updated file to: {output_path}")
    
    try:
        main_df.to_excel(output_path, index=False)
        print("File saved successfully!")
        
        # Print summary statistics
        non_empty_claims = main_df['claims'].notna() & (main_df['claims'].astype(str).str.strip() != "")
        print(f"\nSummary:")
        print(f"- Total patents in main file: {len(main_df)}")
        print(f"- Patents with claims after update: {non_empty_claims.sum()}")
        print(f"- Patents still without claims: {len(main_df) - non_empty_claims.sum()}")
        
    except Exception as e:
        print(f"Error saving file: {str(e)}")
        return
    
    return main_df

# Example usage and testing
def test_priority_normalization():
    """
    Test the priority number normalization function
    """
    print("Testing priority number normalization...")
    
    test_cases = [
        "CN200610096873A",
        "CN 200610096873 A", 
        "US 201615156504 A",
        "US201615156504A",
        "EP 1234567 A1",
        "EP1234567A1"
    ]
    
    for case in test_cases:
        normalized = normalize_priority_number(case)
        print(f"'{case}' -> '{normalized}'")

if __name__ == "__main__":
    # Run the test first to show normalization examples
    test_priority_normalization()
    print("\n" + "="*50 + "\n")
    
    # Run the main merge process
    try:
        result_df = merge_patent_claims()
        if result_df is not None:
            print("\nProcess completed successfully!")
        else:
            print("\nProcess failed!")
    except Exception as e:
        print(f"An error occurred during processing: {str(e)}")
        print("Please check file paths and ensure all files exist.")




#%%

#===============================================================================================
# Adding Derwent claims from chunk 5 to Lens Excel file with existing claims
#===============================================================================================

import pandas as pd
import os
import re

def normalize_priority_number(priority_num):
    """
    Normalize priority numbers to a consistent format for comparison.
    Converts both "CN200610096873A" and "CN 200610096873 A" to "CN200610096873A"
    """
    if pd.isna(priority_num):
        return ""
    
    # Convert to string and strip whitespace
    pn = str(priority_num).strip()
    
    # Remove all spaces to create a consistent format
    normalized = re.sub(r'\s+', '', pn)
    
    return normalized.upper()

def merge_patent_claims_chunk5():
    """
    Merge claims from SIB_CN_chunk5.xlsx into the main patents file with existing claims.
    """
    
    # Define file paths
    chunk_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\Get Claims Derwent\SIB_CN_chunk5.xlsx"
    main_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_patents_cleaned_no_claims_newest_with_claims.xlsx"
    
    # Load the main patents file
    print("Loading main patents file with existing claims...")
    try:
        main_df = pd.read_excel(main_file_path)
        print(f"Loaded main file with {len(main_df)} rows")
    except FileNotFoundError:
        print(f"Error: Main file not found at {main_file_path}")
        return
    except Exception as e:
        print(f"Error loading main file: {str(e)}")
        return
    
    # Check if required columns exist in main file
    if 'Priority Numbers' not in main_df.columns:
        print("Error: 'Priority Numbers' column not found in main file")
        print(f"Available columns: {list(main_df.columns)}")
        return
    
    if 'claims' not in main_df.columns:
        print("Warning: 'claims' column not found in main file. Creating it...")
        main_df['claims'] = ""
    
    # Check current claims status
    existing_claims = main_df['claims'].notna() & (main_df['claims'].astype(str).str.strip() != "")
    print(f"Patents with existing claims: {existing_claims.sum()}")
    print(f"Patents without claims: {len(main_df) - existing_claims.sum()}")
    
    # Normalize priority numbers in main file for faster lookup
    print("Normalizing priority numbers in main file...")
    main_df['normalized_priority'] = main_df['Priority Numbers'].apply(normalize_priority_number)
    
    # Load chunk 5 file
    print(f"\nProcessing SIB_CN_chunk5.xlsx...")
    try:
        # Read chunk file, skipping first row, using second row as headers
        chunk_df = pd.read_excel(chunk_file_path, skiprows=1)
        print(f"Loaded {len(chunk_df)} rows from chunk 5 file")
    except FileNotFoundError:
        print(f"Error: Chunk file not found at {chunk_file_path}")
        return
    except Exception as e:
        print(f"Error loading chunk file: {str(e)}")
        return
    
    # Check if required columns exist in chunk file
    if 'Priority Number' not in chunk_df.columns:
        print(f"Warning: 'Priority Number' column not found in chunk file")
        print(f"Available columns: {list(chunk_df.columns)}")
        return
        
    if 'Claims (English)' not in chunk_df.columns:
        print(f"Warning: 'Claims (English)' column not found in chunk file")
        print(f"Available columns: {list(chunk_df.columns)}")
        return
    
    # Create a dictionary to store claims data from chunk 5
    claims_dict = {}
    
    # Process each row in the chunk file
    valid_claims = 0
    for idx, row in chunk_df.iterrows():
        priority_num = row['Priority Number']
        claims_text = row['Claims (English)']
        
        # Skip if priority number or claims are empty/NaN
        if pd.isna(priority_num) or pd.isna(claims_text):
            continue
        
        if str(claims_text).strip() == "":
            continue
        
        # Normalize the priority number for lookup
        normalized_pn = normalize_priority_number(priority_num)
        
        if normalized_pn:
            claims_dict[normalized_pn] = str(claims_text).strip()
            valid_claims += 1
    
    print(f"Extracted {valid_claims} valid claims from chunk 5 file")
    print(f"Unique priority numbers with claims: {len(claims_dict)}")
    
    # Match claims to main file
    print("\nMatching claims to main patents file...")
    matches_found = 0
    claims_updated = 0
    claims_already_exist = 0
    
    for idx, row in main_df.iterrows():
        normalized_main_pn = row['normalized_priority']
        
        if normalized_main_pn in claims_dict:
            matches_found += 1
            
            # Check if claims field is empty
            current_claims = row['claims']
            if pd.isna(current_claims) or str(current_claims).strip() == "":
                main_df.at[idx, 'claims'] = claims_dict[normalized_main_pn]
                claims_updated += 1
                print(f"  Updated row {idx}: Priority {normalized_main_pn}")
            else:
                claims_already_exist += 1
                print(f"  Row {idx}: Claims already exist for priority {normalized_main_pn}, skipping update")
    
    print(f"\nResults:")
    print(f"Priority number matches found: {matches_found}")
    print(f"Claims updated (were empty): {claims_updated}")
    print(f"Claims already existed (skipped): {claims_already_exist}")
    
    # Remove the temporary normalized column
    main_df = main_df.drop('normalized_priority', axis=1)
    
    # Save the updated main file
    output_path = main_file_path.replace('.xlsx', '_updated_chunk5.xlsx')
    print(f"\nSaving updated file to: {output_path}")
    
    try:
        main_df.to_excel(output_path, index=False)
        print("File saved successfully!")
        
        # Print final summary statistics
        final_claims = main_df['claims'].notna() & (main_df['claims'].astype(str).str.strip() != "")
        print(f"\nFinal Summary:")
        print(f"- Total patents in main file: {len(main_df)}")
        print(f"- Patents with claims after update: {final_claims.sum()}")
        print(f"- Patents still without claims: {len(main_df) - final_claims.sum()}")
        print(f"- Claims added from chunk 5: {claims_updated}")
        
    except Exception as e:
        print(f"Error saving file: {str(e)}")
        return
    
    return main_df

# Example usage and testing
def test_priority_normalization():
    """
    Test the priority number normalization function
    """
    print("Testing priority number normalization...")
    
    test_cases = [
        "CN200610096873A",
        "CN 200610096873 A", 
        "US 201615156504 A",
        "US201615156504A",
        "EP 1234567 A1",
        "EP1234567A1"
    ]
    
    for case in test_cases:
        normalized = normalize_priority_number(case)
        print(f"'{case}' -> '{normalized}'")

if __name__ == "__main__":
    # Run the test first to show normalization examples
    test_priority_normalization()
    print("\n" + "="*50 + "\n")
    
    # Run the main merge process for chunk 5
    try:
        result_df = merge_patent_claims_chunk5()
        if result_df is not None:
            print("\nProcess completed successfully!")
        else:
            print("\nProcess failed!")
    except Exception as e:
        print(f"An error occurred during processing: {str(e)}")
        print("Please check file paths and ensure all files exist.")


#%%

#==================================================================================
# Classification of patents on material-, component- and product- vs. process-level
# with additional LIB classification
#==================================================================================


import openai
import pandas as pd
import time
import re
from tqdm import tqdm
import tiktoken  # For dynamic batch creation


# Set your OpenAI API key
openai.api_key = 'insert key here'

# Load the cleaned dataset as LIB_full_data
LIB_full_data = pd.read_excel(r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_patents_final_reclassification.xlsx")

# SAMPLE: Limit to first 20 rows
# LIB_full_data = LIB_full_data.head(20)

# Remove NaN, empty strings, strings with only spaces or "None" in Claims
LIB_full_data = LIB_full_data[
    LIB_full_data['claims'].notna() &  # Remove NaN values
    LIB_full_data['claims'].str.strip().astype(bool)  # Remove empty strings or strings with only spaces
].reset_index(drop=True)

print(f"Dataframe after cleaning: {LIB_full_data.shape[0]} rows remaining after removing empty or NaN claims.")

# Step 1: Clean Claims (remove non-ASCII characters)
def clean_claim(claim):
    """
    Cleans claims by removing non-ASCII characters.
    Args:
        claim (str): The claim text.
    Returns:
        str: Cleaned claim text.
    """
    return re.sub(r'[^\x00-\x7F]+', ' ', claim) if isinstance(claim, str) else ""

# Apply cleaning function to claims
LIB_full_data['claims'] = LIB_full_data['claims'].apply(clean_claim)


# Create a set of unique citing patents if needed for analysis
unique_citing_patents = set()

# Iterate over the rows in the "family_citing_lens_id" column and split the values
if "family_citing_lens_id" in LIB_full_data.columns:
    for citing_patents in LIB_full_data["family_citing_lens_id"]:
        if isinstance(citing_patents, str):  # Ensure it's a string
            citing_patent_list = citing_patents.split(", ")  # Split at ', '
            unique_citing_patents.update(citing_patent_list)

    # Create a set of unique original patents (from Simple Family Members)
    unique_original_patents = set()

    # Iterate over the rows in the "Simple Family Members" column and split the values
    for original_patents in LIB_full_data["Simple Family Members"]:
        if isinstance(original_patents, str):  # Ensure it's a string
            original_patent_list = original_patents.split(", ")  # Split at ', '
            unique_original_patents.update(original_patent_list)

    # Check how many unique citing patents are already in the original patents
    existing_citing_patents = unique_citing_patents.intersection(unique_original_patents)
    non_existing_citing_patents = unique_citing_patents.difference(unique_original_patents)

    # Output the results
    print(f"Number of citing patents already in the original patent family dataset: {len(existing_citing_patents)}")
    print(f"Number of citing patents NOT in the original patent family dataset: {len(non_existing_citing_patents)}")


# Step 1: Initialize tokenizer
tiktoken_tokenizer = tiktoken.get_encoding("cl100k_base")  # GPT-compatible tokenizer

def calculate_token_length(prompt):
    """
    Calculate the number of tokens in a prompt string.
    Args:
        prompt (str): The input prompt.
    Returns:
        int: Number of tokens.
    """
    return len(tiktoken_tokenizer.encode(prompt))


# Step 2: Define Materials, Components, and Product/Process categories with updated categories
materials = {
    # Cathode materials
    "NMC": "LiNiMnCoO2 - Lithium Nickel Manganese Cobalt Oxide",
    "NCA": "LiNiCoAlO2 - Lithium Nickel Cobalt Aluminum Oxide",
    "LMO": "LiMnO2 - Lithium Manganese Oxide",
    "LCO": "LiCoO2 - Lithium Cobalt Oxide",
    "LFP": "LiFePO4 - Lithium Iron Phosphate",
    "other cathode": "Other cathode materials not in the list above",
    
    # Anode materials
    "Graphite": "Graphite",
    "LTO": "Li4Ti15O12 - Lithium Titanate",
    "Silicon/Carbon": "Silicon/Carbon",
    "other anode": "Other anode materials not in the list above",
}

components = ["Positive Electrode", "Negative Electrode", "Electrolyte", 
              "Cell"]

product_process = ["Product", "Process"]


# Step 3: Define the fixed system-level prompt for material/component/product-process classification
fixed_prompt = """
You are an expert in lithium-ion battery technology. Respond **strictly** in the requested format. Any deviation is considered an error. Do not include any introductory or explanatory text—output only the formatted response.

Classify the following patent claims based on the entire patent, considering all claims together into three categories: Material, Component, and Product/Process.

- **Material-Level**: Choose from {materials} or "none".
- **Component-Level**: Choose from {components} or "none".
- **Product-Process-Level**: Choose "Product", "Process", "Product, Process", or "none".

Here is the required format:
1. [Material-Level abbreviation(s)]
2. [Component-Level]
3. [Product vs Process-Level]

Example:
1. NMC
2. Positive Electrode
3. Product
""".format(
    materials=", ".join(materials.keys()), 
    components=", ".join(components)
)


# Step 3b: Define the fixed system-level prompt for LIB classification
lib_classification_prompt = """
You are an expert in battery technology. Respond **strictly** in the requested format. Any deviation is considered an error. Do not include any introductory or explanatory text—output only the formatted response.

Determine if the following patent claims are related to Lithium-Ion Batteries (LIB) with a liquid electrolyte.

Respond with:
- "Yes" if the patent is specifically related to lithium-ion batteries with a liquid electrolyte
- "No" if the patent is not related to lithium-ion batteries with a liquid electrolyte

Example response:
Yes
"""


# Step 4: Create the classification function with optimizations
def classify_patent_with_optimised_prompt(claims):
    """
    Classify patent claims according to material, component, and product/process levels.
    Args:
        claims (str): The patent claim text.
    Returns:
        tuple: (material_level, component_level, product_process_level)
    """
    # Generate the user-specific content
    full_prompt = f"{fixed_prompt}\n\nPatent Claims:\n{claims}\n\nRespond now:"
    
    # Check token length
    token_length = calculate_token_length(full_prompt)
    if token_length > 3900:
        print(f"Warning: Prompt token length ({token_length}) exceeds recommended limits. Truncating claims.")
        claims = " ".join(claims.split()[:1000])  # Truncate to ensure prompt fits

    # Send the API call
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": fixed_prompt},  # Fixed system instructions
                {"role": "user", "content": claims}           # Variable claims
            ],
            max_tokens=100,
            temperature=0
        )
        content = response['choices'][0]['message']['content'].strip()

        # Parse the response
        classification = content.split("\n")
        material_level = classification[0].split("1. ")[-1].strip() if len(classification) > 0 else "none"
        component_level = classification[1].split("2. ")[-1].strip() if len(classification) > 1 else "none"
        product_process_level = classification[2].split("3. ")[-1].strip() if len(classification) > 2 else "none"

    except Exception as e:
        print(f"Error in GPT-4o API call or response parsing: {e}")
        material_level, component_level, product_process_level = "none", "none", "none"

    return material_level, component_level, product_process_level


# Step 4b: Create the LIB classification function
def classify_lib_patent(claims):
    """
    Classify patent claims as related to Lithium-Ion Batteries (LIB) or not.
    Args:
        claims (str): The patent claim text.
    Returns:
        str: "Yes" if related to LIB, "No" otherwise.
    """
    # Generate the user-specific content
    full_prompt = f"{lib_classification_prompt}\n\nPatent Claims:\n{claims}\n\nRespond now:"
    
    # Check token length
    token_length = calculate_token_length(full_prompt)
    if token_length > 3900:
        print(f"Warning: Prompt token length ({token_length}) exceeds recommended limits. Truncating claims.")
        claims = " ".join(claims.split()[:1000])  # Truncate to ensure prompt fits

    # Send the API call
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": lib_classification_prompt},  # Fixed system instructions
                {"role": "user", "content": claims}                       # Variable claims
            ],
            max_tokens=50,
            temperature=0
        )
        content = response['choices'][0]['message']['content'].strip()
        
        # Parse the response - should be just "Yes" or "No"
        lib_classification = content.strip()
        
        # Ensure we're returning exactly "Yes" or "No"
        if lib_classification.lower() == "yes":
            return "Yes"
        else:
            return "No"

    except Exception as e:
        print(f"Error in GPT-4o API call for LIB classification: {e}")
        return "No"  # Default to "No" in case of error


# Step 5: Apply classification to the DataFrame - with both types of classification
def classify_patents_in_dataframe_optimized(df):
    """
    Apply both classification types (Material/Component/Product-Process and LIB relevance)
    to all patents in the dataframe.
    Args:
        df (pandas.DataFrame): The dataframe containing patent claims.
    Returns:
        pandas.DataFrame: The updated dataframe with classification results.
    """
    # Validate DataFrame structure
    if 'claims' not in df.columns:
        raise ValueError("The DataFrame must contain a 'claims' column.")
    
    # Ensure necessary columns exist or are created
    for col in ['Material-Level', 'Component-Level', 'Product-Process-Level', 'LIB']:
        if col not in df.columns:
            df[col] = None

    # Iterate through the DataFrame rows with a progress bar
    for index, row in tqdm(df.iterrows(), total=len(df), desc="Classifying patents"):
        claims = row['claims']

        # Skip invalid or empty claims
        if not isinstance(claims, str) or claims.strip() == "":
            print(f"Skipping row {index}: Empty or invalid claims.")
            df.at[index, 'Material-Level'] = "none"
            df.at[index, 'Component-Level'] = "none"
            df.at[index, 'Product-Process-Level'] = "none"
            df.at[index, 'LIB'] = "No"
            continue

        # Classify the patent claims for material/component/product-process
        material, component, product_process = classify_patent_with_optimised_prompt(claims)
        
        # Classify the patent for LIB relevance
        lib_classification = classify_lib_patent(claims)

        # Store the results in the DataFrame
        df.at[index, 'Material-Level'] = material
        df.at[index, 'Component-Level'] = component
        df.at[index, 'Product-Process-Level'] = product_process
        df.at[index, 'LIB'] = lib_classification

    return df

# Apply the classification
LIB_full_data = classify_patents_in_dataframe_optimized(LIB_full_data)


# Extract year from the "earliest_priority_date" column and save in "priority_year" if needed
if "earliest_priority_date" in LIB_full_data.columns:
    LIB_full_data["priority_year"] = pd.to_datetime(LIB_full_data["earliest_priority_date"]).dt.year
    LIB_full_data["priority_year"] = LIB_full_data["priority_year"].astype(int)
    print(LIB_full_data[["earliest_priority_date", "priority_year"]].head())


# Save results to a new Excel file
output_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_full_data_classified_reclassification.xlsx"
LIB_full_data.to_excel(output_path, index=False)

print(f"Processing complete. Results saved to '{output_path}'.")



#%%

#=================================================================================================
# Transfer of reclassified data to the full dataset
#=================================================================================================

import pandas as pd
import os

# Define path to files
base_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data"

# Create file paths
full_data_path = os.path.join(base_path, "LIB_full_data_classified22042025.xlsx")
reclassification_path = os.path.join(base_path, "LIB_full_data_classified_reclassification.xlsx")
output_path = os.path.join(base_path, "LIB_full_data_classified27052025.xlsx")

try:
    # Load files
    print("Loading full_data file...")
    full_data = pd.read_excel(full_data_path)
    print(f"Full_data loaded: {full_data.shape[0]} rows, {full_data.shape[1]} columns")
    
    print("Loading reclassification file...")
    reclassification = pd.read_excel(reclassification_path)
    print(f"Reclassification loaded: {reclassification.shape[0]} rows, {reclassification.shape[1]} columns")
    
    # Check if "#" column exists in both DataFrames
    if "#" not in full_data.columns:
        raise ValueError("Column '#' not found in full_data")
    if "#" not in reclassification.columns:
        raise ValueError("Column '#' not found in reclassification")
    
    # Check for duplicates in "#" column
    full_data_duplicates = full_data["#"].duplicated().sum()
    recl_duplicates = reclassification["#"].duplicated().sum()
    
    if full_data_duplicates > 0:
        print(f"Warning: {full_data_duplicates} duplicate '#' values found in full_data!")
        # Show duplicate values
        duplicates = full_data[full_data["#"].duplicated(keep=False)]["#"].unique()
        print(f"Duplicate '#' values in full_data: {duplicates[:10]}...")  # Show first 10
    
    if recl_duplicates > 0:
        print(f"Warning: {recl_duplicates} duplicate '#' values found in reclassification!")
        duplicates = reclassification[reclassification["#"].duplicated(keep=False)]["#"].unique()
        print(f"Duplicate '#' values in reclassification: {duplicates[:10]}...")  # Show first 10
    
    # Columns to be updated
    columns_to_update = ["Material-Level", "Component-Level", "Product-Process-Level", "LIB"]
    
    # Check if all columns to update exist in both DataFrames
    missing_cols_full = [col for col in columns_to_update if col not in full_data.columns]
    missing_cols_recl = [col for col in columns_to_update if col not in reclassification.columns]
    
    if missing_cols_full:
        print(f"Error: Following columns missing in full_data: {missing_cols_full}")
        raise ValueError(f"Missing columns in full_data: {missing_cols_full}")
    if missing_cols_recl:
        print(f"Error: Following columns missing in reclassification: {missing_cols_recl}")
        raise ValueError(f"Missing columns in reclassification: {missing_cols_recl}")
    
    print(f"Columns to be updated: {columns_to_update}")
    
    # Create copy of full_data for updates
    updated_data = full_data.copy()
    
    # Statistics for updates
    successful_updates = 0
    not_found_count = 0
    examples = []
    
    print("\nStarting row-by-row update...")
    
    # Iterate through each row in reclassification
    for idx, row in reclassification.iterrows():
        # "#" value of current row
        current_id = row["#"]
        
        # Find corresponding row in full_data
        mask = updated_data["#"] == current_id
        matching_rows = updated_data[mask]
        
        if len(matching_rows) == 0:
            # No corresponding row found
            not_found_count += 1
            continue
        elif len(matching_rows) > 1:
            # Multiple rows found (duplicates)
            print(f"Warning: Multiple rows found for # {current_id}. Using the first one.")
        
        # Index of row to be updated in full_data
        update_idx = matching_rows.index[0]
        
        # Copy values from reclassification and insert into full_data
        old_values = {}
        new_values = {}
        
        for col in columns_to_update:
            old_values[col] = updated_data.loc[update_idx, col]
            new_values[col] = row[col]
            updated_data.loc[update_idx, col] = row[col]
        
        successful_updates += 1
        
        # Collect first 3 updates as examples
        if len(examples) < 3:
            examples.append({
                'id': current_id,
                'old': old_values,
                'new': new_values
            })
        
        # Show progress (every 1000 rows)
        if (idx + 1) % 1000 == 0:
            print(f"  Processed: {idx + 1}/{len(reclassification)} rows...")
    
    print(f"\nUpdate completed!")
    print(f"Successfully updated: {successful_updates} records")
    print(f"Not found: {not_found_count} records")
    
    # Show examples
    if examples:
        print(f"\n--- Examples of first 3 updates ---")
        for i, example in enumerate(examples, 1):
            print(f"\nExample {i} - Record # {example['id']}:")
            for col in columns_to_update:
                print(f"  {col}: '{example['old'][col]}' -> '{example['new'][col]}'")
    
    # Save result
    print(f"\nSaving updated data to: {output_path}")
    updated_data.to_excel(output_path, index=False)
    
    print("Successfully completed!")
    print(f"Final file saved as: LIB_full_data_classified27052025.xlsx")
    print(f"Number of rows: {updated_data.shape[0]}")
    print(f"Number of columns: {updated_data.shape[1]}")
    
except FileNotFoundError as e:
    print(f"Error: File not found - {e}")
    print("Please check the file paths.")
except Exception as e:
    print(f"An error occurred: {e}")
    import traceback
    traceback.print_exc()


#%%


#==================================================================================
# Classification of patents on material-, component- and product- vs. process-level
# with additional SIB classification
#==================================================================================

import openai
import pandas as pd
import time
import re
from tqdm import tqdm
import tiktoken  # For dynamic batch creation

# Set your OpenAI API key
openai.api_key = 'insert key here'

# Load the cleaned dataset as SIB_full_data
SIB_full_data = pd.read_excel(r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_patents_cleaned_no_claims_newest_with_claims_updated_chunk5.xlsx")

# SAMPLE: Limit to first 20 rows
# SIB_full_data = SIB_full_data.head(20)

# Remove NaN, empty strings, strings with only spaces or "None" in Claims
SIB_full_data = SIB_full_data[
    SIB_full_data['claims'].notna() &  # Remove NaN values
    SIB_full_data['claims'].str.strip().astype(bool)  # Remove empty strings or strings with only spaces
].reset_index(drop=True)

print(f"Dataframe after cleaning: {SIB_full_data.shape[0]} rows remaining after removing empty or NaN claims.")

# Step 1: Clean Claims (remove non-ASCII characters)
def clean_claim(claim):
    """
    Cleans claims by removing non-ASCII characters.
    Args:
        claim (str): The claim text.
    Returns:
        str: Cleaned claim text.
    """
    return re.sub(r'[^\x00-\x7F]+', ' ', claim) if isinstance(claim, str) else ""

# Apply cleaning function to claims
SIB_full_data['claims'] = SIB_full_data['claims'].apply(clean_claim)


# Create a set of unique citing patents if needed for analysis
unique_citing_patents = set()

# Iterate over the rows in the "family_citing_lens_id" column and split the values
if "family_citing_lens_id" in SIB_full_data.columns:
    for citing_patents in SIB_full_data["family_citing_lens_id"]:
        if isinstance(citing_patents, str):  # Ensure it's a string
            citing_patent_list = citing_patents.split(", ")  # Split at ', '
            unique_citing_patents.update(citing_patent_list)

    # Create a set of unique original patents (from Simple Family Members)
    unique_original_patents = set()

    # Iterate over the rows in the "Simple Family Members" column and split the values
    for original_patents in SIB_full_data["Simple Family Members"]:
        if isinstance(original_patents, str):  # Ensure it's a string
            original_patent_list = original_patents.split(", ")  # Split at ', '
            unique_original_patents.update(original_patent_list)

    # Check how many unique citing patents are already in the original patents
    existing_citing_patents = unique_citing_patents.intersection(unique_original_patents)
    non_existing_citing_patents = unique_citing_patents.difference(unique_original_patents)

    # Output the results
    print(f"Number of citing patents already in the original patent family dataset: {len(existing_citing_patents)}")
    print(f"Number of citing patents NOT in the original patent family dataset: {len(non_existing_citing_patents)}")


# Step 1: Initialize tokenizer
tiktoken_tokenizer = tiktoken.get_encoding("cl100k_base")  # GPT-compatible tokenizer

def calculate_token_length(prompt):
    """
    Calculate the number of tokens in a prompt string.
    Args:
        prompt (str): The input prompt.
    Returns:
        int: Number of tokens.
    """
    return len(tiktoken_tokenizer.encode(prompt))


# Step 2: Define Materials, Components, and Product/Process categories with updated categories for SIB
# Enhanced material definitions with unambiguous synonyms and more detailed descriptions
materials = {
    # Cathode materials with distinctive synonyms and typical patent terminology
    "Fe-PBA": {
        "full_name": "Iron Prussian Blue Analogue (Fe-PBA)",
        "formula": ["Na₂Fe[Fe(CN)₆]", "Na2Fe[Fe(CN)6]"],
        "synonyms": ["iron hexacyanoferrate", "sodium iron hexacyanoferrate", "FeHCF", 
                    "ferric ferrocyanide", "Na-Fe-Fe-CN", "sodium iron ferrocyanide"],
        "description": "Open framework material with large interstitial sites for Na+ intercalation, high rate capability"
    },
    
    "Mn-PBA": {
        "full_name": "Manganese Prussian Blue Analogue (Mn-PBA)",
        "formula": ["Na₂Mn[Fe(CN)₆]", "Na2Mn[Fe(CN)6]"],
        "synonyms": ["manganese hexacyanoferrate", "sodium manganese hexacyanoferrate", "MnHCF", 
                    "Na-Mn-Fe-CN", "sodium manganese ferrocyanide"],
        "description": "PBA structure with manganese centers, offering good stability and reversibility"
    },
    
    "NVPF": {
        "full_name": "Sodium Vanadium Phosphate Fluoride (NVPF)",
        "formula": ["Na₃V₂(PO₄)₂F₃", "Na3V2(PO4)2F3"],
        "synonyms": ["sodium vanadium fluorophosphate", "NASICON-type vanadium compound", 
                    "sodium vanadium phosphate fluoride", "vanadium fluorophosphate"],
        "description": "NASICON-type structure with high operating voltage and good rate capability"
    },
    
    "NFPP/NFP": {
        "full_name": "Sodium Iron Phosphate/Sodium Iron Pyrophosphate",
        "formula": ["Na₄Fe₃(PO₄)₂(P₂O₇)", "Na₂FeP₂O₇", "NaFePO₄", 
                   "Na4Fe3(PO4)2(P2O7)", "Na2FeP2O7", "NaFePO4"],
        "synonyms": ["sodium iron phosphate", "sodium iron pyrophosphate", "NFPP", "NFP", 
                    "sodium ferric phosphate", "iron phosphate", "iron pyrophosphate", 
                    "mixed-polyanion iron compound", "maricite-NaFePO4", "olivine-NaFePO4"],
        "description": "Phosphate-based cathodes with high thermal stability and moderate energy density"
    },
    
    "NFM": {
        "full_name": "Sodium Nickel Iron Manganese Oxide",
        "formula": ["NaNi₁₋ₓ₋ᵧFeₓMnᵧO₂", "NaNi(1-x-y)FexMnyO2"],
        "synonyms": ["sodium nickel-iron-manganese oxide", "Na-Ni-Fe-Mn-O", "NaNFM", 
                    "sodium nickel ferromanganese oxide", "NaNiFeMnO2", 
                    "nickel-iron-manganese sodium oxide"],
        "description": "Layered oxide with nickel, iron and manganese transition metals"
    },
    
    "CFM": {
        "full_name": "Sodium Copper Iron Manganese Oxide",
        "formula": ["NaCu₁₋ₓ₋ᵧFeₓMnᵧO₂", "NaCu(1-x-y)FexMnyO2"],
        "synonyms": ["sodium copper-iron-manganese oxide", "Na-Cu-Fe-Mn-O", "NaCuFeMnO2", 
                    "copper-iron-manganese sodium oxide", "NaCFM", 
                    "sodium copper ferromanganese oxide"],
        "description": "Layered oxide structure containing copper, iron and manganese transition metals"
    },
    
    "NMO": {
        "full_name": "Sodium Manganese Oxide",
        "formula": ["NaMnO₂", "NaMnO2"],
        "synonyms": ["sodium manganese dioxide", "α-NaMnO2", "beta-NaMnO2", 
                    "α-phase NaMnO2", "β-phase NaMnO2", "manganese-based sodium oxide"],
        "description": "Layered manganese oxide with various phases (α, β)"
    },
    
    # Anode materials with distinctive synonyms
    "Hard Carbon": {
        "full_name": "Hard Carbon",
        "formula": ["C"],
        "synonyms": ["non-graphitizable carbon", "non-graphitic carbon", "disordered carbon", 
                    "turbostratic carbon", "hard carbon anode", "non-graphitizing carbon", 
                    "pyrolyzed carbon", "carbon with d002 > 0.37 nm"],
        "description": "Non-graphitizable carbon material with disordered structure, suitable for Na+ intercalation, typically with interlayer spacing > 0.37 nm"
    },
    
    # Extended "other" categories
    "other cathode": {
        "full_name": "Other cathode materials",
        "formula": [],
        "synonyms": ["novel cathode", "cathode active material", 
                    "positive electrode material", "cathode compound"],
        "description": "Any cathode material not specifically listed above"
    },
    
    "other anode": {
        "full_name": "Other anode materials",
        "formula": [],
        "synonyms": ["novel anode", "anode active material", 
                    "negative electrode material", "anode compound",
                    "expanded graphite", "soft carbon", "metal alloy anode"],
        "description": "Any anode material not specifically listed above, including metal alloys, phosphorus-based materials, and other carbon forms"
    },
}

# Basic component and product/process definitions
components = ["Positive Electrode", "Negative Electrode", "Electrolyte", "Cell"]
product_process = ["Product", "Process"]

# Optimized prompt for material/component/product-process classification
fixed_prompt = """
You are an expert in sodium-ion battery technology. Respond **strictly** in the requested format. Any deviation is considered an error. Do not include any introductory or explanatory text—output only the formatted response.

Classify the following patent claims based on the entire patent, considering all claims together into three categories: Material, Component, and Product/Process.

- **Material-Level**: Identify ALL materials from {materials} or "none".
  IMPORTANT DISAMBIGUATION:
  - NFM must contain nickel (NaNi-based compounds)
  - CFM must contain copper (NaCu-based compounds)
  - NMO contains only manganese (NaMnO2-type)
  - Fe-PBA/Mn-PBA: Look for hexacyanoferrate structures
  - **NVPF**: Look specifically for *vanadium phosphate fluoride* compounds (e.g., Na₃V₂(PO₄)₂F₃). Do **not** classify as NVPF if the text only refers to "sodium vanadium phosphate" or "Na₃V₂(PO₄)₃" **without** fluorine – this is **NVP**, which is not part of the listed materials.  
  - NFPP/NFP: Look for sodium iron phosphate/pyrophosphate
  - Hard Carbon: Non-graphitizable carbon with d-spacing >0.37nm

- **Component-Level**: Choose from {components} or "none".
- **Product-Process-Level**: Choose "Product", "Process", "Product, Process", or "none".

Here is the required format:
1. [All matching Material-Level abbreviation(s), comma-separated]
2. [Component-Level]
3. [Product vs Process-Level]

Example:
1. Fe-PBA, NVPF, NFPP/NFP
2. Positive Electrode
3. Product
""".format(
    materials=", ".join(materials.keys()), 
    components=", ".join(components)
)

    

# Step 3b: Define the fixed system-level prompt for SIB classification
sib_classification_prompt = """
You are an expert in battery technology. Respond **strictly** in the requested format. Any deviation is considered an error. Do not include any introductory or explanatory text—output only the formatted response.

Determine if the following patent claims are related to classical Sodium-Ion Batteries (SIB). Consider materials and battery characteristics specifically used in Sodium-Ion Batteries, such as hard, non-porous carbon materials with specific surface areas, interlayer distance properties, and electrode configurations that are commonly associated with SIBs.
A classical SIB is defined as:
- A battery that operates at or near room temperature (not requiring temperatures above 80°C)
- Employs sodium ions as charge carriers between solid (non-molten) electrodes
- Typically uses a liquid electrolyte, not molten salt electrolytes

Respond with:
- "Yes" if the patent is related to classical sodium-ion batteries 
- "No" if the patent relates to other battery types (including high-temperature sodium batteries) or has no relation to sodium-ion batteries

Example response:
Yes
"""



# Step 4: Create the classification function with optimizations
def classify_patent_with_optimised_prompt(claims):
    """
    Classify patent claims according to material, component, and product/process levels.
    Args:
        claims (str): The patent claim text.
    Returns:
        tuple: (material_level, component_level, product_process_level)
    """
    # Generate the user-specific content
    full_prompt = f"{fixed_prompt}\n\nPatent Claims:\n{claims}\n\nRespond now:"
    
    # Check token length
    token_length = calculate_token_length(full_prompt)
    if token_length > 3900:
        print(f"Warning: Prompt token length ({token_length}) exceeds recommended limits. Truncating claims.")
        claims = " ".join(claims.split()[:1000])  # Truncate to ensure prompt fits

    # Send the API call
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": fixed_prompt},  # Fixed system instructions
                {"role": "user", "content": claims}           # Variable claims
            ],
            max_tokens=100,
            temperature=0
        )
        content = response['choices'][0]['message']['content'].strip()

        # Parse the response
        classification = content.split("\n")
        material_level = classification[0].split("1. ")[-1].strip() if len(classification) > 0 else "none"
        component_level = classification[1].split("2. ")[-1].strip() if len(classification) > 1 else "none"
        product_process_level = classification[2].split("3. ")[-1].strip() if len(classification) > 2 else "none"

    except Exception as e:
        print(f"Error in GPT-4o API call or response parsing: {e}")
        material_level, component_level, product_process_level = "none", "none", "none"

    return material_level, component_level, product_process_level


# Step 4b: Create the SIB classification function
def classify_sib_patent(claims):
    """
    Classify patent claims as related to Sodium-Ion Batteries (SIB) or not.
    Args:
        claims (str): The patent claim text.
    Returns:
        str: "Yes" if related to SIB, "No" otherwise.
    """
    # Generate the user-specific content
    full_prompt = f"{sib_classification_prompt}\n\nPatent Claims:\n{claims}\n\nRespond now:"
    
    # Check token length
    token_length = calculate_token_length(full_prompt)
    if token_length > 3900:
        print(f"Warning: Prompt token length ({token_length}) exceeds recommended limits. Truncating claims.")
        claims = " ".join(claims.split()[:1000])  # Truncate to ensure prompt fits

    # Send the API call
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": sib_classification_prompt},  # Fixed system instructions
                {"role": "user", "content": claims}                        # Variable claims
            ],
            max_tokens=50,
            temperature=0
        )
        content = response['choices'][0]['message']['content'].strip()
        
        # Parse the response - should be just "Yes" or "No"
        sib_classification = content.strip()
        
        # Ensure we're returning exactly "Yes" or "No"
        if sib_classification.lower() == "yes":
            return "Yes"
        else:
            return "No"

    except Exception as e:
        print(f"Error in GPT-4o API call for SIB classification: {e}")
        return "No"  # Default to "No" in case of error


# Step 5: Apply classification to the DataFrame - with both types of classification
def classify_patents_in_dataframe_optimized(df):
    """
    Apply both classification types (Material/Component/Product-Process and SIB relevance)
    to all patents in the dataframe.
    Args:
        df (pandas.DataFrame): The dataframe containing patent claims.
    Returns:
        pandas.DataFrame: The updated dataframe with classification results.
    """
    # Validate DataFrame structure
    if 'claims' not in df.columns:
        raise ValueError("The DataFrame must contain a 'claims' column.")
    
    # Ensure necessary columns exist or are created
    for col in ['Material-Level', 'Component-Level', 'Product-Process-Level', 'SIB']:
        if col not in df.columns:
            df[col] = None

    # Iterate through the DataFrame rows with a progress bar
    for index, row in tqdm(df.iterrows(), total=len(df), desc="Classifying patents"):
        claims = row['claims']

        # Skip invalid or empty claims
        if not isinstance(claims, str) or claims.strip() == "":
            print(f"Skipping row {index}: Empty or invalid claims.")
            df.at[index, 'Material-Level'] = "none"
            df.at[index, 'Component-Level'] = "none"
            df.at[index, 'Product-Process-Level'] = "none"
            df.at[index, 'SIB'] = "No"
            continue

        # Classify the patent claims for material/component/product-process
        material, component, product_process = classify_patent_with_optimised_prompt(claims)
        
        # Classify the patent for SIB relevance
        sib_classification = classify_sib_patent(claims)

        # Store the results in the DataFrame
        df.at[index, 'Material-Level'] = material
        df.at[index, 'Component-Level'] = component
        df.at[index, 'Product-Process-Level'] = product_process
        df.at[index, 'SIB'] = sib_classification

    return df

# Apply the classification
SIB_full_data = classify_patents_in_dataframe_optimized(SIB_full_data)


# Extract year from the "earliest_priority_date" column and save in "priority_year" if needed
if "earliest_priority_date" in SIB_full_data.columns:
    SIB_full_data["priority_year"] = pd.to_datetime(SIB_full_data["earliest_priority_date"]).dt.year
    SIB_full_data["priority_year"] = SIB_full_data["priority_year"].astype(int)
    print(SIB_full_data[["earliest_priority_date", "priority_year"]].head())

#%%

#===================================================================================
# Adding Citing Lens IDs at family level
#===================================================================================
import pandas as pd

def process_library_data(SIB_full_data, path_to_family_members_file):
    """
    Process LIB patent data and extract citing patents from family members.
    
    Args:
        SIB_full_data: DataFrame with the main LIB patent data
        path_to_family_members_file: Path to the Excel file with family members data
    
    Returns:
        DataFrame with processed data including new 'family_citing_lens_id' column
    """
    # Read the family members file
    SIB_full_data_all_family_members = pd.read_excel(path_to_family_members_file)
    
    # Create a dictionary for faster lookup of citing patents by lens_id
    print("Creating lookup dictionary for citing patents...")
    citing_patents_dict = {}
    for index, row in SIB_full_data_all_family_members.iterrows():
        citing_patents_dict[row['lens_id']] = row['citing_lens_ids']
    
    # Initialize new column for family citing lens IDs
    SIB_full_data['family_citing_lens_id'] = ''
    
    # Process each row in the main dataset
    print("Processing simple family members for each patent...")
    for index, row in SIB_full_data.iterrows():
        if pd.notna(row['Simple Family Members']):  # Check if value is not NaN
            # Split the family members by ";;" and strip whitespace
            family_members = [member.strip() for member in str(row['Simple Family Members']).split(';;')]
            
            # Collect all citing patents for these family members
            all_citing_patents = []
            for member in family_members:
                if member in citing_patents_dict and pd.notna(citing_patents_dict[member]):
                    # Add citing patents to the collection if they exist
                    all_citing_patents.append(str(citing_patents_dict[member]))
            
            # Join all citing patents with commas and assign to the new column
            SIB_full_data.at[index, 'family_citing_lens_id'] = ', '.join(all_citing_patents)
        
        # Print progress for every 1000 records
        if index % 1000 == 0:
            print(f"Processed {index} records...")
    
    print("Processing complete!")
    return SIB_full_data

# Example usage
if __name__ == "__main__":
    # Assuming SIB_full_data is already loaded as a DataFrame
    # SIB_full_data = your_existing_dataframe
    
    family_members_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\full_SIB_data_all_family_members.xlsx"
    
    # Process the data
    result_df = process_library_data(SIB_full_data, family_members_file_path)
    
    # The result_df now contains the processed data with the new column
    print("Data processing completed. The DataFrame now includes 'family_citing_lens_id' column.")

# Save results to a new Excel file
output_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_full_data_classified_3.xlsx"
result_df.to_excel(output_path, index=False)

print(f"Processing complete. Results saved to '{output_path}'.")


#%%

#===================================================================================
# Adding Family-Level cited patent counts (total knowledge received per family)
#===================================================================================

import pandas as pd
import numpy as np
import time
from tqdm import tqdm

# Aktiviere tqdm für pandas
tqdm.pandas()

def load_datasets():
    """
    Lädt alle erforderlichen Datensätze
    """
    print("=== LADEN DER DATENSÄTZE ===")
    
    # Dateipfade definieren
    file_paths = {
        'full_SIB_data': r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_full_data_classified_final_onlySIB_26052025.xlsx",
        'SIB_all_family_members': r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\full_SIB_data_all_family_members_CN_JP_US_EP.xlsx",
        'full_LIB_data': r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_full_data_classified_onlyLIB_27052025.xlsx",
        'LIB_all_family_members': r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\full_LIB_data_all_family_members.xlsx"
    }
    
    datasets = {}
    
    # Lade jeden Datensatz
    for name, path in file_paths.items():
        print(f"\nLade {name}...")
        start_time = time.time()
        try:
            # Versuche mit openpyxl engine
            df = pd.read_excel(path, engine='openpyxl')
            datasets[name] = df
            load_time = time.time() - start_time
            print(f"✅ {name} erfolgreich geladen: {df.shape} in {load_time:.2f}s")
            print(f"   Spalten: {', '.join(df.columns[:5])}...")
        except Exception as e:
            print(f"❌ Fehler beim Laden von {name}: {str(e)}")
            return None
    
    return datasets

def extract_lens_ids_from_family(family_str):
    """
    Extrahiert alle lens_ids aus einem 'Simple Family Members' String
    """
    if pd.isna(family_str):
        return []
    
    try:
        # Teile bei ";;" und entferne Leerzeichen
        lens_ids = [lens_id.strip() for lens_id in str(family_str).split(";;")]
        # Filtere leere Strings heraus
        lens_ids = [lens_id for lens_id in lens_ids if lens_id]
        return lens_ids
    except:
        return []

def get_all_unique_lens_ids(df, column_name="Simple Family Members"):
    """
    Extrahiert alle einzigartigen lens_ids aus einer DataFrame-Spalte
    """
    print(f"Extrahiere alle lens_ids aus Spalte '{column_name}'...")
    
    all_lens_ids = set()
    
    for family_str in tqdm(df[column_name], desc="Extrahiere lens_ids"):
        lens_ids = extract_lens_ids_from_family(family_str)
        all_lens_ids.update(lens_ids)
    
    print(f"✅ {len(all_lens_ids)} einzigartige lens_ids gefunden")
    return all_lens_ids

def check_lens_ids_coverage(main_df, family_df, tech_name):
    """
    Überprüft, ob alle lens_ids aus main_df in family_df enthalten sind
    """
    print(f"\n=== ÜBERPRÜFUNG DER LENS_ID ABDECKUNG FÜR {tech_name.upper()} ===")
    
    # Extrahiere alle lens_ids aus main_df
    main_lens_ids = get_all_unique_lens_ids(main_df, "Simple Family Members")
    
    # Erstelle Set der lens_ids aus family_df
    family_lens_ids = set(family_df['lens_id'].astype(str))
    print(f"lens_ids in {tech_name}_all_family_members: {len(family_lens_ids)}")
    
    # Finde fehlende lens_ids
    missing_lens_ids = main_lens_ids - family_lens_ids
    
    if len(missing_lens_ids) == 0:
        print(f"✅ Alle {len(main_lens_ids)} lens_ids aus {tech_name}_full_data sind in {tech_name}_all_family_members enthalten!")
        return True, []
    else:
        print(f"❌ {len(missing_lens_ids)} lens_ids fehlen in {tech_name}_all_family_members:")
        # Zeige die ersten 10 fehlenden IDs
        missing_list = list(missing_lens_ids)[:10]
        for lens_id in missing_list:
            print(f"   - {lens_id}")
        if len(missing_lens_ids) > 10:
            print(f"   ... und {len(missing_lens_ids) - 10} weitere")
        return False, list(missing_lens_ids)

def calculate_family_cited_patents_count(row, family_members_df):
    """
    Berechnet die Summe der cited_patents_count für alle Familienmitglieder einer Zeile
    """
    family_str = row["Simple Family Members"]
    lens_ids = extract_lens_ids_from_family(family_str)
    
    if not lens_ids:
        return 0
    
    # Filtere family_members_df für die lens_ids dieser Familie
    matching_rows = family_members_df[family_members_df['lens_id'].isin(lens_ids)]
    
    # Summiere cited_patents_count (konvertiere zu numerisch und behandle NaN)
    if 'cited_patents_count' in matching_rows.columns:
        cited_counts = pd.to_numeric(matching_rows['cited_patents_count'], errors='coerce')
        return cited_counts.sum(skipna=True)
    else:
        print("Warnung: Spalte 'cited_patents_count' nicht gefunden!")
        return 0

def add_family_level_citations(main_df, family_df, tech_name):
    """
    Fügt family_level_cited_patent_counts zu main_df hinzu
    """
    print(f"\n=== BERECHNUNG DER FAMILY LEVEL CITATIONS FÜR {tech_name.upper()} ===")
    
    # Überprüfe, ob cited_patents_count Spalte existiert
    if 'cited_patents_count' not in family_df.columns:
        print(f"❌ Spalte 'cited_patents_count' nicht in {tech_name}_all_family_members gefunden!")
        print(f"Verfügbare Spalten: {', '.join(family_df.columns)}")
        return main_df
    
    print(f"Berechne family level cited patent counts für {len(main_df)} Zeilen...")
    
    # Konvertiere lens_id zu string für bessere Übereinstimmung
    family_df_copy = family_df.copy()
    family_df_copy['lens_id'] = family_df_copy['lens_id'].astype(str)
    
    # Berechne family level citations mit Progress Bar
    start_time = time.time()
    tqdm.pandas(desc=f"Berechne {tech_name} family citations")
    
    main_df['family_level_cited_patent_counts'] = main_df.progress_apply(
        lambda row: calculate_family_cited_patents_count(row, family_df_copy),
        axis=1
    )
    
    calc_time = time.time() - start_time
    
    # Statistiken ausgeben
    total_citations = main_df['family_level_cited_patent_counts'].sum()
    families_with_citations = (main_df['family_level_cited_patent_counts'] > 0).sum()
    
    print(f"✅ Berechnung abgeschlossen in {calc_time:.2f}s")
    print(f"   Gesamte family level citations: {total_citations}")
    print(f"   Familien mit citations: {families_with_citations}/{len(main_df)} ({families_with_citations/len(main_df)*100:.1f}%)")
    print(f"   Durchschnittliche citations pro Familie: {total_citations/len(main_df):.1f}")
    
    return main_df

def save_results(sib_df, lib_df):
    """
    Speichert die Ergebnisse
    """
    print("\n=== SPEICHERN DER ERGEBNISSE ===")
    
    # Definiere Ausgabepfade
    sib_output = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_full_data_with_family_citations.xlsx"
    lib_output = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_full_data_with_family_citations.xlsx"
    
    try:
        print("Speichere SIB Daten...")
        sib_df.to_excel(sib_output, index=False, engine='openpyxl')
        print(f"✅ SIB Daten gespeichert: {sib_output}")
        
        print("Speichere LIB Daten...")
        lib_df.to_excel(lib_output, index=False, engine='openpyxl')
        print(f"✅ LIB Daten gespeichert: {lib_output}")
        
    except Exception as e:
        print(f"❌ Fehler beim Speichern: {str(e)}")

def main():
    """
    Hauptfunktion, die den gesamten Prozess koordiniert
    """
    print("FAMILY LEVEL CITED PATENTS COUNT ANALYSIS")
    print("=" * 50)
    
    total_start_time = time.time()
    
    # 1. Lade alle Datensätze
    datasets = load_datasets()
    if datasets is None:
        print("❌ Fehler beim Laden der Datensätze. Prozess abgebrochen.")
        return
    
    # 2. Überprüfe lens_id Abdeckung für SIB
    sib_coverage_ok, sib_missing = check_lens_ids_coverage(
        datasets['full_SIB_data'], 
        datasets['SIB_all_family_members'], 
        'SIB'
    )
    
    # 3. Überprüfe lens_id Abdeckung für LIB
    lib_coverage_ok, lib_missing = check_lens_ids_coverage(
        datasets['full_LIB_data'], 
        datasets['LIB_all_family_members'], 
        'LIB'
    )
    
    # 4. Berechne family level citations (unabhängig von der Abdeckung)
    if not sib_coverage_ok or not lib_coverage_ok:
        print("\n⚠️  Warnung: Nicht alle lens_ids sind abgedeckt, aber die Berechnung wird fortgesetzt...")
        if not sib_coverage_ok:
            print(f"   SIB: {len(sib_missing)} fehlende lens_ids")
        if not lib_coverage_ok:
            print(f"   LIB: {len(lib_missing)} fehlende lens_ids")
        print("   Fehlende lens_ids werden bei der Berechnung ignoriert.")
    else:
        print("\n✅ Alle lens_ids sind abgedeckt.")
    
    print("\nBeginne mit Citation-Berechnung...")
    
    # Berechne für SIB
    sib_with_citations = add_family_level_citations(
        datasets['full_SIB_data'].copy(), 
        datasets['SIB_all_family_members'], 
        'SIB'
    )
    
    # Berechne für LIB
    lib_with_citations = add_family_level_citations(
        datasets['full_LIB_data'].copy(), 
        datasets['LIB_all_family_members'], 
        'LIB'
    )
    
    # Speichere Ergebnisse
    save_results(sib_with_citations, lib_with_citations)
    
    total_time = time.time() - total_start_time
    print(f"\n=== ANALYSE ABGESCHLOSSEN ===")
    print(f"Gesamtzeit: {total_time:.2f}s ({total_time/60:.1f} Minuten)")

if __name__ == "__main__":
    main()

#%%

#================================================================================
#================================================================================
# Analysze both datasets - LIB & SIB
#================================================================================
#================================================================================


#==============================================================
# Building a citation network based on LIB and SIB patent datasets
#==============================================================

import pandas as pd
import networkx as nx
import plotly.graph_objects as go
import numpy as np
from datetime import datetime

# File paths for both datasets
lib_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_full_data_classified_onlyLIB_27052025.xlsx"
sib_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_full_data_classified_final_onlySIB_26052025.xlsx"
sheet_name = "Sheet1"

# Load both Excel files
LIB_patents = pd.read_excel(lib_file_path, sheet_name=sheet_name)
SIB_patents = pd.read_excel(sib_file_path, sheet_name=sheet_name)

# Print basic info about both datasets
print(f"LIB dataset shape: {LIB_patents.shape}")
print(f"SIB dataset shape: {SIB_patents.shape}")

# Add a 'battery_type' column to identify the source in the combined dataset
LIB_patents['battery_type'] = 'LIB'
SIB_patents['battery_type'] = 'SIB'

#==============================================================
# Define material sets for both battery types
#==============================================================

# Define LIB material groups
lib_cathode_materials = {"LCO", "LMO", "LFP", "NMC", "NCA"}
lib_anode_materials = {"Silicon/Carbon", "Graphite", "LTO"}

# Define SIB material groups (based on the provided dictionary)
sib_cathode_materials = {"Fe-PBA", "Mn-PBA", "NVPF", "NFPP/NFP", "NFM", "CFM", "NMO"}
sib_anode_materials = {"Hard Carbon"}

#==============================================================
# Function to extract materials from material strings
#==============================================================

def extract_materials(material_str, material_set):
    """Extract materials that match the given material set from a comma-separated string."""
    materials = set(material_str.split(", ")) if isinstance(material_str, str) else set()
    extracted = list(materials.intersection(material_set))
    return extracted if extracted else ["none"]

#==============================================================
# Process LIB dataset
#==============================================================

# Extract priority year from earliest_priority_date for LIB
LIB_patents["earliest_priority_date"] = pd.to_datetime(LIB_patents["Earliest Priority Date"])
LIB_patents["priority_year"] = LIB_patents["earliest_priority_date"].dt.year
LIB_patents = LIB_patents.sort_values(by=["priority_year"], ascending=True)

# Create new columns for Cathode and Anode Materials for LIB
LIB_patents["Cathode Material"] = LIB_patents["Material-Level"].apply(lambda x: extract_materials(x, lib_cathode_materials))
LIB_patents["Anode Material"] = LIB_patents["Material-Level"].apply(lambda x: extract_materials(x, lib_anode_materials))

# Update Component-Level based on Material-Level for LIB
for index, row in LIB_patents.iterrows():
    if row["Material-Level"] != "none" and row["Component-Level"] == "none":
        materials = set(row["Material-Level"].split(", ")) if isinstance(row["Material-Level"], str) else set()

        cathode_match = materials & lib_cathode_materials
        anode_match = materials & lib_anode_materials

        new_component_level = []
        if cathode_match:
            new_component_level.append("Positive Electrode")
        if anode_match:
            new_component_level.append("Negative Electrode")

        LIB_patents.at[index, "Component-Level"] = ", ".join(new_component_level) if new_component_level else "none"

# Update Component-Level for more specific components in LIB
for index, row in LIB_patents.iterrows():
    if row["Material-Level"] != "none" and row["Component-Level"] != "Cell":
        materials = set(row["Material-Level"].split(", ")) if isinstance(row["Material-Level"], str) else set()

        cathode_match = materials & lib_cathode_materials
        anode_match = materials & lib_anode_materials

        existing_components = set(row["Component-Level"].split(", ")) if isinstance(row["Component-Level"], str) and row["Component-Level"] != "none" else set()

        if cathode_match:
            existing_components.add("Positive Electrode")
        if anode_match:
            existing_components.add("Negative Electrode")

        LIB_patents.at[index, "Component-Level"] = ", ".join(existing_components) if existing_components else "none"

# Create family_citing_lens_id_count for LIB dataset by counting citations
LIB_patents["family_citing_lens_id_count"] = LIB_patents["family_citing_lens_id"].apply(
    lambda x: len(x.split(", ")) if isinstance(x, str) and x else 0
)

# Sort component values alphabetically for LIB
for index, row in LIB_patents.iterrows():
    if row["Component-Level"] != "none":
        sorted_components = ", ".join(sorted(row["Component-Level"].split(", ")))
        LIB_patents.at[index, "Component-Level"] = sorted_components

#==============================================================
# Process SIB dataset
#==============================================================

# Extract priority year from earliest_priority_date for SIB
SIB_patents["earliest_priority_date"] = pd.to_datetime(SIB_patents["Earliest Priority Date"])
SIB_patents["priority_year"] = SIB_patents["earliest_priority_date"].dt.year
SIB_patents = SIB_patents.sort_values(by=["priority_year"], ascending=True)

# Create new columns for Cathode and Anode Materials for SIB
SIB_patents["Cathode Material"] = SIB_patents["Material-Level"].apply(lambda x: extract_materials(x, sib_cathode_materials))
SIB_patents["Anode Material"] = SIB_patents["Material-Level"].apply(lambda x: extract_materials(x, sib_anode_materials))

# Update Component-Level based on Material-Level for SIB
for index, row in SIB_patents.iterrows():
    if row["Material-Level"] != "none" and row["Component-Level"] == "none":
        materials = set(row["Material-Level"].split(", ")) if isinstance(row["Material-Level"], str) else set()

        cathode_match = materials & sib_cathode_materials
        anode_match = materials & sib_anode_materials

        new_component_level = []
        if cathode_match:
            new_component_level.append("Positive Electrode")
        if anode_match:
            new_component_level.append("Negative Electrode")

        SIB_patents.at[index, "Component-Level"] = ", ".join(new_component_level) if new_component_level else "none"

# Update Component-Level for more specific components in SIB
for index, row in SIB_patents.iterrows():
    if row["Material-Level"] != "none" and row["Component-Level"] != "Cell":
        materials = set(row["Material-Level"].split(", ")) if isinstance(row["Material-Level"], str) else set()

        cathode_match = materials & sib_cathode_materials
        anode_match = materials & sib_anode_materials

        existing_components = set(row["Component-Level"].split(", ")) if isinstance(row["Component-Level"], str) and row["Component-Level"] != "none" else set()

        if cathode_match:
            existing_components.add("Positive Electrode")
        if anode_match:
            existing_components.add("Negative Electrode")

        SIB_patents.at[index, "Component-Level"] = ", ".join(existing_components) if existing_components else "none"

# Sort component values alphabetically for SIB
for index, row in SIB_patents.iterrows():
    if row["Component-Level"] != "none":
        sorted_components = ", ".join(sorted(row["Component-Level"].split(", ")))
        SIB_patents.at[index, "Component-Level"] = sorted_components
        
# Create family_citing_lens_id_count for SIB dataset by counting citations
SIB_patents["family_citing_lens_id_count"] = SIB_patents["family_citing_lens_id"].apply(
    lambda x: len(x.split(", ")) if isinstance(x, str) and x else 0
)

# Print unique component levels to verify preprocessing
print("LIB Component Levels:", LIB_patents["Component-Level"].unique())
print("SIB Component Levels:", SIB_patents["Component-Level"].unique())


#==============================================================
# Filter patents with citations and apply further processing
#==============================================================

# Filter LIB patents with at least one citation
lib_df = LIB_patents[LIB_patents["family_citing_lens_id_count"] > 0]

# Filter SIB patents with at least one citation
sib_df = SIB_patents[SIB_patents["family_citing_lens_id_count"] > 0]

# Replace complex component combinations with 'Cell' for LIB
cell_combinations = [
    "Cell, Electrolyte, Negative Electrode, Positive Electrode",
    "Cell, Electrolyte, Negative Electrode",
    "Cell, Electrolyte, Positive Electrode",
    "Cell, Negative Electrode, Positive Electrode",
    "Electrolyte, Negative Electrode, Positive Electrode",
    "Electrolyte, Negative Electrode",
    "Electrolyte, Positive Electrode"
]

lib_df["Component-Level"] = lib_df["Component-Level"].replace(
    {combination: "Cell" for combination in cell_combinations}
)

# Apply the same replacement to SIB dataset
sib_df["Component-Level"] = sib_df["Component-Level"].replace(
    {combination: "Cell" for combination in cell_combinations}
)

# Convert priority dates to datetime and sort
lib_df["earliest_priority_date"] = pd.to_datetime(lib_df["earliest_priority_date"])
lib_df = lib_df.sort_values(by=["earliest_priority_date"], ascending=True)

sib_df["earliest_priority_date"] = pd.to_datetime(sib_df["earliest_priority_date"])
sib_df = sib_df.sort_values(by=["earliest_priority_date"], ascending=True)

# Extract lists of citing patents
lib_df["List of Citing Patents"] = lib_df["family_citing_lens_id"].fillna("")
lib_df["List of Citing Patents"] = lib_df["family_citing_lens_id"].apply(
    lambda x: [p.strip() for p in x.split(", ") if p.strip()]
)

sib_df["List of Citing Patents"] = sib_df["family_citing_lens_id"].fillna("")
sib_df["List of Citing Patents"] = sib_df["family_citing_lens_id"].apply(
    lambda x: [p.strip() for p in x.split(", ") if p.strip()]
)

# Remove patents that have neither material nor component information
lib_df = lib_df[~((lib_df['Material-Level'] == 'none') & (lib_df['Component-Level'] == 'none'))]
sib_df = sib_df[~((sib_df['Material-Level'] == 'none') & (sib_df['Component-Level'] == 'none'))]

#==============================================================
# Combine datasets for the network graph
#==============================================================

# Add origin identifiers to node numbers to prevent conflicts
lib_df["original_index"] = lib_df.index
sib_df["original_index"] = sib_df.index

# Combine datasets
combined_df = pd.concat([lib_df, sib_df], ignore_index=True)

# Create a continuous node numbering for the combined dataset
combined_df["node_num"] = range(len(combined_df))

# Create lookup dictionaries for the original indices
lib_original_to_node = {row["original_index"]: row["node_num"] for _, row in combined_df[combined_df["battery_type"] == "LIB"].iterrows()}
sib_original_to_node = {row["original_index"]: row["node_num"] for _, row in combined_df[combined_df["battery_type"] == "SIB"].iterrows()}

#==============================================================
# Create mapping from Lens IDs to node numbers
#==============================================================

# Build dictionary to map Lens IDs to node numbers
lens_id_to_node = {}

# Process each patent family to map all IDs to their node number
for index, row in combined_df.iterrows():
    if isinstance(row["Simple Family Members"], str):
        lens_ids = row["Simple Family Members"].split(";;")
        for lens_id in lens_ids:
            lens_id_to_node[lens_id] = row["node_num"]

# Function to find node numbers for citing patents
def find_citing_nodes(citing_list):
    """Convert a list of citing patent IDs to their corresponding node numbers."""
    citing_nodes = set()
    for citing_patent in citing_list:
        if citing_patent in lens_id_to_node:
            citing_nodes.add(lens_id_to_node[citing_patent])
    return list(citing_nodes)

# Add citing node numbers to the dataframe
combined_df["Citing_node_numbers"] = combined_df["List of Citing Patents"].apply(find_citing_nodes)

#==============================================================
# Filter to include only patents involved in citation relationships
#==============================================================

# Get all nodes that are cited by other nodes in our dataset
all_citing_nodes = set(combined_df["Citing_node_numbers"].explode().dropna())

# Filter to keep only patents that cite or are cited
filtered_rows = []
for _, row in combined_df.iterrows():
    node = row["node_num"]
    citing_nodes = row["Citing_node_numbers"]
    
    # Include node if it's cited by another node or cites other nodes
    if node in all_citing_nodes or citing_nodes:
        filtered_rows.append(row)

# Create the filtered dataframe and reset indices
combined_df = pd.DataFrame(filtered_rows).reset_index(drop=True)

#==============================================================
# Define color and symbol mappings for visualization
#==============================================================

# Function to normalize node sizes based on citation count
def normalize_size(value, min_size=5, max_size=30):
    """Scale the node size based on citation count."""
    min_val, max_val = combined_df["family_citing_lens_id_count"].min(), combined_df["family_citing_lens_id_count"].max()
    return min_size + (max_size - min_size) * (value - min_val) / (max_val - min_val)

# Ensure node numbers are strings for consistency
combined_df["node_num"] = combined_df["node_num"].astype(str)

# Convert node numbers back to integers for processing
combined_df['node_num'] = combined_df['node_num'].astype(int)

# Ensure all elements in Citing_node_numbers are integers
combined_df['Citing_node_numbers'] = combined_df['Citing_node_numbers'].apply(lambda x: [int(i) for i in x])

# Define color mappings for cathode materials
# LIB cathode materials
lib_color_map = {
    frozenset(["LFP"]): "blue", 
    frozenset(["NMC"]): "green", 
    frozenset(["LCO"]): "red", 
    frozenset(["NCA"]): "purple", 
    frozenset(["LMO"]): "cyan", 
    frozenset(["none"]): "black"
}

# SIB cathode materials
sib_color_map = {
    frozenset(["Fe-PBA"]): "darkblue", 
    frozenset(["Mn-PBA"]): "darkgreen", 
    frozenset(["NVPF"]): "darkred", 
    frozenset(["NFPP/NFP"]): "darkmagenta", 
    frozenset(["NFM"]): "darkcyan", 
    frozenset(["CFM"]): "darkorange",
    frozenset(["NMO"]): "darkgrey",
    frozenset(["other cathode"]): "darkviolet",
    frozenset(["none"]): "black"
}

# Define symbol mappings for anode materials
# LIB anode materials
lib_symbol_map = {
    frozenset(["Graphite"]): "circle", 
    frozenset(["Silicon/Carbon"]): "square", 
    frozenset(["LTO"]): "diamond", 
    frozenset(["none"]): "cross"
}

# SIB anode materials
sib_symbol_map = {
    frozenset(["Hard Carbon"]): "triangle-up",
    frozenset(["other anode"]): "triangle-down",
    frozenset(["none"]): "x"
}

# Define the component hierarchy for visualization
component_hierarchy = [
    "Electrolyte",
    "Negative Electrode", 
    "Negative Electrode, Positive Electrode", 
    "Positive Electrode", 
    "Cell"
]

#==============================================================
# Create the Network Graph
#==============================================================

# Create a directed graph
G = nx.DiGraph()

# Add nodes and their attributes
for _, row in combined_df.iterrows():
    node_num = row["node_num"]
    batt_type = row["battery_type"]
    
    # Select color and symbol based on battery type
    if batt_type == "LIB":
        cathode_set = frozenset(row["Cathode Material"])
        anode_set = frozenset(row["Anode Material"])
        color = lib_color_map.get(cathode_set, "grey")
        symbol = lib_symbol_map.get(anode_set, "circle")
    else:  # SIB
        cathode_set = frozenset(row["Cathode Material"])
        anode_set = frozenset(row["Anode Material"])
        color = sib_color_map.get(cathode_set, "grey")
        symbol = sib_symbol_map.get(anode_set, "circle")
    
    # Calculate node size based on citation count
    size = normalize_size(row["family_citing_lens_id_count"])
    
    # Add node with all attributes
    G.add_node(
        node_num,
        year=row["earliest_priority_date"],
        color=color,
        symbol=symbol,
        size=size,
        component=row["Component-Level"],
        batt_type=batt_type
    )
    
    # Add edges for citations
    for citing_patent in row["Citing_node_numbers"]:
        if citing_patent in combined_df["node_num"].values:
            G.add_edge(node_num, citing_patent)  # Direction: from cited to citing patent

# Helper function to get node materials
def get_materials_for_node(node):
    """Get the materials (color and symbol) for a node."""
    cathode_material = G.nodes[node].get("color", "none")
    anode_material = G.nodes[node].get("symbol", "none")
    return sorted([cathode_material, anode_material])

#==============================================================
# Calculate Node Positions
#==============================================================

# Position parameters
node_spacing = 50     # Spacing between nodes in same group
component_margin = 200  # Margin between component groups

# Initialize position dictionary and offsets
pos = {}
y_offset = 0

# Track unique material-component combinations
material_component_combinations = {}

# Process nodes by component hierarchy
for component in component_hierarchy:
    # Get nodes for this component
    nodes_in_component = [n for n in G.nodes() if G.nodes[n].get('component') == component]
    
    # Skip if no nodes for this component
    if not nodes_in_component:
        continue
    
    # Sort nodes by battery type first, then by materials
    nodes_in_component.sort(key=lambda n: (
        G.nodes[n].get('batt_type', ''),
        G.nodes[n].get('color', 'none'), 
        G.nodes[n].get('symbol', 'none')
    ))
    
    # Group nodes by battery type and material combination
    current_combo = None
    current_batt_type = None
    combo_nodes = []
    
    for node in nodes_in_component:
        node_combo = (
            G.nodes[node].get('color', 'none'),
            G.nodes[node].get('symbol', 'none')
        )
        node_batt = G.nodes[node].get('batt_type', '')
        
        # Check if we've moved to a new combination or battery type
        if node_combo != current_combo or node_batt != current_batt_type:
            # Process previous group if it exists
            if combo_nodes:
                # Sort nodes in group by date
                combo_nodes.sort(key=lambda n: G.nodes[n]['year'])
                
                # Position each node in the group
                for i, n in enumerate(combo_nodes):
                    x = G.nodes[n]['year'].timestamp()
                    # Add small horizontal offset for better visibility if same date
                    if i > 0 and G.nodes[combo_nodes[i-1]]['year'].timestamp() == x:
                        x += 86400 * 30  # Add 30 days in seconds
                    pos[n] = (x, y_offset)
                
                # Store this combination's y-position for labels
                if component not in material_component_combinations:
                    material_component_combinations[component] = {}
                
                combo_key = (current_batt_type, current_combo[0], current_combo[1])
                material_component_combinations[component][combo_key] = y_offset
                
                # Increase y-offset for next group
                y_offset += node_spacing
            
            # Start new group
            current_combo = node_combo
            current_batt_type = node_batt
            combo_nodes = [node]
        else:
            # Add to current group
            combo_nodes.append(node)
    
    # Process the last group
    if combo_nodes:
        combo_nodes.sort(key=lambda n: G.nodes[n]['year'])
        for i, n in enumerate(combo_nodes):
            x = G.nodes[n]['year'].timestamp()
            # Add small offset for same dates
            if i > 0 and G.nodes[combo_nodes[i-1]]['year'].timestamp() == x:
                x += 86400 * 30  # 30 days in seconds
            pos[n] = (x, y_offset)
        
        # Store this combination's y-position
        if component not in material_component_combinations:
            material_component_combinations[component] = {}
        
        combo_key = (current_batt_type, current_combo[0], current_combo[1])
        material_component_combinations[component][combo_key] = y_offset
        
        y_offset += node_spacing
    
    # Add margin between components
    y_offset += component_margin

# Handle nodes with no component or not in component hierarchy
nodes_without_pos = [n for n in G.nodes() if n not in pos]
if nodes_without_pos:
    print(f"Assigning positions to {len(nodes_without_pos)} nodes not covered by component hierarchy")
    
    # Group them by battery type
    lib_unpositioned = [n for n in nodes_without_pos if G.nodes[n].get('batt_type') == 'LIB']
    sib_unpositioned = [n for n in nodes_without_pos if G.nodes[n].get('batt_type') == 'SIB']
    
    # Add extra spacing
    y_offset += component_margin
    
    # Position LIB nodes
    if lib_unpositioned:
        for n in lib_unpositioned:
            x = G.nodes[n]['year'].timestamp()
            pos[n] = (x, y_offset)
        y_offset += node_spacing
    
    # Position SIB nodes
    if sib_unpositioned:
        for n in sib_unpositioned:
            x = G.nodes[n]['year'].timestamp()
            pos[n] = (x, y_offset)

#==============================================================
# Prepare Plotly Visualization
#==============================================================

# Create edge traces with error handling
edge_traces = []
for edge in G.edges():
    # Skip edges where either node doesn't have a position
    if edge[0] not in pos or edge[1] not in pos:
        print(f"Skipping edge {edge} because one of the nodes has no position")
        continue
        
    x0, y0 = pos[edge[0]]
    x1, y1 = pos[edge[1]]
    
    # Determine edge color based on battery types
    source_type = G.nodes[edge[0]].get('batt_type')
    target_type = G.nodes[edge[1]].get('batt_type')
    
    if source_type == target_type:
        # Same battery type citation
        edge_color = 'grey'
    else:
        # Cross-battery citation
        edge_color = 'orange'  # Make cross-citations stand out
    
    # Create edge trace
    edge_trace = go.Scatter(
        x=[x0, x1], 
        y=[y0, y1], 
        mode='lines',
        line=dict(width=1, color=edge_color),
        hoverinfo='none',
        showlegend=False
    )
    edge_traces.append(edge_trace)

# Prepare node traces - separate by battery type for better visualization
lib_nodes = {n for n in G.nodes() if G.nodes[n].get('batt_type') == 'LIB' and n in pos}
sib_nodes = {n for n in G.nodes() if G.nodes[n].get('batt_type') == 'SIB' and n in pos}

print(f"LIB nodes with positions: {len(lib_nodes)}")
print(f"SIB nodes with positions: {len(sib_nodes)}")
print(f"Nodes missing positions: {len(G.nodes()) - len(lib_nodes) - len(sib_nodes)}")

# Function to create node trace for a set of nodes
def create_node_trace(nodes, name):
    node_x, node_y, node_color, node_symbol, node_size, node_text = [], [], [], [], [], []
    
    for node in nodes:
        x, y = pos[node]
        node_x.append(x)
        node_y.append(y)
        node_color.append(G.nodes[node]["color"])
        node_symbol.append(G.nodes[node]["symbol"])
        node_size.append(G.nodes[node]["size"])
        
        # Create hover text with battery type
        batt_type = G.nodes[node]["batt_type"]
        hover_text = f"Patent: {node}<br>Type: {batt_type}<br>Year: {G.nodes[node]['year'].year}<br>Component: {G.nodes[node]['component']}"
        node_text.append(hover_text)
    
    return go.Scatter(
        x=node_x, y=node_y, 
        mode='markers',
        marker=dict(
            size=node_size, 
            color=node_color, 
            symbol=node_symbol, 
            line=dict(width=1, color='black')
        ),
        text=node_text, 
        hoverinfo='text',
        name=name
    )

# Create node traces
lib_node_trace = create_node_trace(lib_nodes, "LIB Patents")
sib_node_trace = create_node_trace(sib_nodes, "SIB Patents")

#==============================================================
# Create Legend and Axis Labels
#==============================================================

# Create legend entries for colors and symbols
legend_entries = []

# LIB material legends
for mat, color in lib_color_map.items():
    if mat != frozenset(["none"]):
        mat_name = ", ".join(mat)
        legend_entries.append(go.Scatter(
            x=[None], y=[None], 
            mode='markers', 
            marker=dict(size=18, color=color),
            name=f'LIB Cathode: {mat_name}',
            hoverinfo='none'
        ))

# SIB material legends
for mat, color in sib_color_map.items():
    if mat != frozenset(["none"]):
        mat_name = ", ".join(mat)
        legend_entries.append(go.Scatter(
            x=[None], y=[None], 
            mode='markers', 
            marker=dict(size=18, color=color),
            name=f'SIB Cathode: {mat_name}',
            hoverinfo='none'
        ))

# LIB anode symbols
for mat, symbol in lib_symbol_map.items():
    if mat != frozenset(["none"]):
        mat_name = ", ".join(mat)
        legend_entries.append(go.Scatter(
            x=[None], y=[None], 
            mode='markers', 
            marker=dict(size=18, symbol=symbol, color='black'),
            name=f'LIB Anode: {mat_name}',
            hoverinfo='none'
        ))

# SIB anode symbols
for mat, symbol in sib_symbol_map.items():
    if mat != frozenset(["none"]):
        mat_name = ", ".join(mat)
        legend_entries.append(go.Scatter(
            x=[None], y=[None], 
            mode='markers', 
            marker=dict(size=18, symbol=symbol, color='black'),
            name=f'SIB Anode: {mat_name}',
            hoverinfo='none'
        ))

# Citation type legends
legend_entries.append(go.Scatter(
    x=[None], y=[None], 
    mode='lines', 
    line=dict(color='grey', width=2),
    name='Same Battery Type Citation',
    hoverinfo='none'
))

legend_entries.append(go.Scatter(
    x=[None], y=[None], 
    mode='lines', 
    line=dict(color='orange', width=2),
    name='Cross Battery Type Citation',
    hoverinfo='none'
))

# Prepare x-axis ticks (years)
min_year = min(G.nodes[n]["year"] for n in G.nodes)
max_year = max(G.nodes[n]["year"] for n in G.nodes)

x_ticks = [pd.Timestamp(year, 1, 1).timestamp() for year in range(min_year.year, max_year.year + 1, 5)]
x_tick_labels = list(range(min_year.year, max_year.year + 1, 5))

# Prepare y-axis ticks
y_ticks = []
y_tick_labels = []

# Component level ticks
for component in component_hierarchy:
    if component in material_component_combinations:
        # Add component label at the middle position of its combinations
        combo_positions = list(material_component_combinations[component].values())
        if combo_positions:
            avg_pos = sum(combo_positions) / len(combo_positions)
            y_ticks.append(avg_pos)
            y_tick_labels.append(component)

# Material-component combination ticks
for component, combos in material_component_combinations.items():
    for (batt_type, color, symbol), y_pos in combos.items():
        # Create descriptive label
        cathode_label = ""
        anode_label = ""
        
        # Determine which maps to use based on battery type
        if batt_type == "LIB":
            color_map = lib_color_map
            symbol_map = lib_symbol_map
        else:
            color_map = sib_color_map
            symbol_map = sib_symbol_map
        
        # Find material names from colors/symbols
        for mat_set, c in color_map.items():
            if c == color:
                cathode_label = ", ".join(mat_set)
                break
        
        for mat_set, s in symbol_map.items():
            if s == symbol:
                anode_label = ", ".join(mat_set)
                break
        
        # Create label
        if cathode_label and anode_label:
            if cathode_label != "none" and anode_label != "none":
                material_label = f"{batt_type}: C: {cathode_label}, A: {anode_label}"
                y_ticks.append(y_pos)
                y_tick_labels.append(material_label)

#==============================================================
# Create and Save the Final Visualization
#==============================================================

# Create the figure
fig = go.Figure(data=edge_traces + [lib_node_trace, sib_node_trace] + legend_entries)

# Calculate optimal plot height
used_y_range = max([pos[node][1] for node in G.nodes()]) - min([pos[node][1] for node in G.nodes()])
plot_height = int(used_y_range + component_margin * 2)

# Calculate y-axis limits
min_y = min([pos[node][1] for node in G.nodes()]) - component_margin/2
max_y = max([pos[node][1] for node in G.nodes()]) + component_margin/2

# Update the figure layout
fig.update_layout(
    title=dict(
        text="Lithium-Ion and Sodium-Ion Battery Patent Citation Network",
        font=dict(size=22)  # Larger title
    ),
    showlegend=True,
    xaxis=dict(
        title=dict(
            text="Priority Date", 
            font=dict(size=20)  # Larger axis titles
        ),
        tickmode='array',
        tickvals=x_ticks,
        ticktext=[str(year) for year in x_tick_labels],
        tickfont=dict(size=18)  # Larger tick labels
    ),
    yaxis=dict(
        tickmode='array',
        tickvals=y_ticks,
        ticktext=y_tick_labels,
        title=dict(
            text="Component Hierarchy and Materials",
            font=dict(size=20)  # Larger axis titles
        ),
        tickfont=dict(size=14),  # Smaller font for more text
        range=[min_y, max_y]  # Limit y-axis to used range
    ),
    hovermode='closest',
    hoverlabel=dict(
        font=dict(size=18)  # Larger hover labels
    ),
    plot_bgcolor='rgba(0,0,0,0)',  # Transparent plot background
    paper_bgcolor='rgba(0,0,0,0)',  # Transparent page background
    
    legend=dict(
        yanchor="top",
        y=0.99,
        xanchor="left",
        x=1.01,
        bgcolor="rgba(255, 255, 255, 0.8)",
        font=dict(size=16)  # Larger legend font
    ),
    width=2000,
    height=plot_height,
    margin=dict(l=150, r=250, t=80, b=30)  # Increased right margin for legend
)

# Save the visualization
output_file = "LIB_SIB_Combined_Patent_Network.html"
fig.write_html(output_file)
print(f"Visualization saved to {output_file}")

# Print some network statistics
print("\nNetwork Statistics:")
print(f"Total patents in network: {G.number_of_nodes()}")
print(f"LIB patents: {len(lib_nodes)}")
print(f"SIB patents: {len(sib_nodes)}")
print(f"Total citations: {G.number_of_edges()}")

# Count cross-battery citations
cross_citations = sum(1 for u, v in G.edges() 
                     if G.nodes[u].get('batt_type') != G.nodes[v].get('batt_type'))
print(f"Cross-battery citations: {cross_citations}")

# Calculate citation directionality
lib_to_sib = sum(1 for u, v in G.edges() 
                if G.nodes[u].get('batt_type') == 'LIB' and G.nodes[v].get('batt_type') == 'SIB')
sib_to_lib = sum(1 for u, v in G.edges() 
                if G.nodes[u].get('batt_type') == 'SIB' and G.nodes[v].get('batt_type') == 'LIB')

print(f"LIB patents citing SIB patents: {lib_to_sib}")
print(f"SIB patents citing LIB patents: {sib_to_lib}")

print("\nAnalysis complete!")


#%%


#==========================================================================================================================
# Building a citation network based on LIB and SIB datasets, focusing only on positive electrode (cathode) patents
#==========================================================================================================================

import pandas as pd
import networkx as nx
import plotly.graph_objects as go
import numpy as np
from datetime import datetime

# File paths for both datasets
lib_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_full_data_classified_onlyLIB_27052025.xlsx"
sib_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_full_data_classified_final_onlySIB_26052025.xlsx"
sheet_name = "Sheet1"

# Load both Excel files
LIB_patents = pd.read_excel(lib_file_path, sheet_name=sheet_name)
SIB_patents = pd.read_excel(sib_file_path, sheet_name=sheet_name)

# Print basic info about both datasets
print(f"LIB dataset shape: {LIB_patents.shape}")
print(f"SIB dataset shape: {SIB_patents.shape}")

# Add a 'battery_type' column to identify the source in the combined dataset
LIB_patents['battery_type'] = 'LIB'
SIB_patents['battery_type'] = 'SIB'

#==============================================================
# Define material sets for both battery types
#==============================================================

# Define LIB cathode materials
lib_cathode_materials = {"LCO", "LMO", "LFP", "NMC", "NCA"}

# Define SIB cathode materials
sib_cathode_materials = {"Fe-PBA", "Mn-PBA", "NVPF", "NFPP/NFP", "NFM", "CFM", "NMO"}

#==============================================================
# Function to extract materials from material strings
#==============================================================

def extract_materials(material_str, material_set):
    """Extract materials that match the given material set from a comma-separated string."""
    materials = set(material_str.split(", ")) if isinstance(material_str, str) else set()
    extracted = list(materials.intersection(material_set))
    return extracted if extracted else ["none"]

#==============================================================
# Process LIB dataset
#==============================================================

# Extract priority year from earliest_priority_date for LIB
LIB_patents["earliest_priority_date"] = pd.to_datetime(LIB_patents["Earliest Priority Date"])
LIB_patents["priority_year"] = LIB_patents["earliest_priority_date"].dt.year
LIB_patents = LIB_patents.sort_values(by=["priority_year"], ascending=True)

# Create new column for Cathode Materials for LIB
LIB_patents["Cathode Material"] = LIB_patents["Material-Level"].apply(lambda x: extract_materials(x, lib_cathode_materials))

# Update Component-Level based on Material-Level for LIB
for index, row in LIB_patents.iterrows():
    if row["Material-Level"] != "none" and row["Component-Level"] == "none":
        materials = set(row["Material-Level"].split(", ")) if isinstance(row["Material-Level"], str) else set()
        cathode_match = materials & lib_cathode_materials
        
        if cathode_match:
            LIB_patents.at[index, "Component-Level"] = "Positive Electrode"

# Update Component-Level for more specific components in LIB
for index, row in LIB_patents.iterrows():
    if row["Material-Level"] != "none" and row["Component-Level"] != "Cell":
        materials = set(row["Material-Level"].split(", ")) if isinstance(row["Material-Level"], str) else set()
        cathode_match = materials & lib_cathode_materials
        
        existing_components = set(row["Component-Level"].split(", ")) if isinstance(row["Component-Level"], str) and row["Component-Level"] != "none" else set()
        
        if cathode_match:
            existing_components.add("Positive Electrode")
            
        LIB_patents.at[index, "Component-Level"] = ", ".join(existing_components) if existing_components else "none"

# Create family_citing_lens_id_count for LIB dataset by counting citations
LIB_patents["family_citing_lens_id_count"] = LIB_patents["family_citing_lens_id"].apply(
    lambda x: len(x.split(", ")) if isinstance(x, str) and x else 0
)

# Sort component values alphabetically for LIB
for index, row in LIB_patents.iterrows():
    if row["Component-Level"] != "none":
        sorted_components = ", ".join(sorted(row["Component-Level"].split(", ")))
        LIB_patents.at[index, "Component-Level"] = sorted_components

#==============================================================
# Process SIB dataset
#==============================================================

# Extract priority year from earliest_priority_date for SIB
SIB_patents["earliest_priority_date"] = pd.to_datetime(SIB_patents["Earliest Priority Date"])
SIB_patents["priority_year"] = SIB_patents["earliest_priority_date"].dt.year
SIB_patents = SIB_patents.sort_values(by=["priority_year"], ascending=True)

# Create new column for Cathode Materials for SIB
SIB_patents["Cathode Material"] = SIB_patents["Material-Level"].apply(lambda x: extract_materials(x, sib_cathode_materials))

# Update Component-Level based on Material-Level for SIB
for index, row in SIB_patents.iterrows():
    if row["Material-Level"] != "none" and row["Component-Level"] == "none":
        materials = set(row["Material-Level"].split(", ")) if isinstance(row["Material-Level"], str) else set()
        cathode_match = materials & sib_cathode_materials
        
        if cathode_match:
            SIB_patents.at[index, "Component-Level"] = "Positive Electrode"

# Update Component-Level for more specific components in SIB
for index, row in SIB_patents.iterrows():
    if row["Material-Level"] != "none" and row["Component-Level"] != "Cell":
        materials = set(row["Material-Level"].split(", ")) if isinstance(row["Material-Level"], str) else set()
        cathode_match = materials & sib_cathode_materials
        
        existing_components = set(row["Component-Level"].split(", ")) if isinstance(row["Component-Level"], str) and row["Component-Level"] != "none" else set()
        
        if cathode_match:
            existing_components.add("Positive Electrode")
            
        SIB_patents.at[index, "Component-Level"] = ", ".join(existing_components) if existing_components else "none"

# Create family_citing_lens_id_count for SIB dataset by counting citations
SIB_patents["family_citing_lens_id_count"] = SIB_patents["family_citing_lens_id"].apply(
    lambda x: len(x.split(", ")) if isinstance(x, str) and x else 0
)

# Sort component values alphabetically for SIB
for index, row in SIB_patents.iterrows():
    if row["Component-Level"] != "none":
        sorted_components = ", ".join(sorted(row["Component-Level"].split(", ")))
        SIB_patents.at[index, "Component-Level"] = sorted_components

#==============================================================
# Filter patents to retain only positive electrode (cathode) patents
#==============================================================

# Function to check if any cathode material is present in Material-Level
def contains_cathode_material(material_str, cathode_materials):
    """Check if the material string contains any cathode material from the given set."""
    if not isinstance(material_str, str) or material_str == "none":
        return False
    
    materials = set(material_str.split(", "))
    return bool(materials.intersection(cathode_materials))

# Filter LIB dataset for patents containing LIB cathode materials
lib_positive_electrode = LIB_patents[
    LIB_patents["Material-Level"].apply(
        lambda x: contains_cathode_material(x, lib_cathode_materials)
    )
]

# Filter LIB patents to only include those with at least one citation and cathode materials
lib_cathode = lib_positive_electrode[
    (lib_positive_electrode["family_citing_lens_id_count"] > 0) & 
    (lib_positive_electrode["Cathode Material"].apply(lambda x: x != ["none"]))
]

# Filter SIB dataset for patents containing SIB cathode materials
sib_positive_electrode = SIB_patents[
    SIB_patents["Material-Level"].apply(
        lambda x: contains_cathode_material(x, sib_cathode_materials)
    )
]

# Filter SIB patents to only include those with at least one citation and cathode materials
sib_cathode = sib_positive_electrode[
    (sib_positive_electrode["family_citing_lens_id_count"] > 0) & 
    (sib_positive_electrode["Cathode Material"].apply(lambda x: x != ["none"]))
]

# Print info about filtered datasets
print(f"LIB patents with cathode materials: {len(lib_positive_electrode)}")
print(f"LIB cathode patents with citations: {len(lib_cathode)}")
print(f"SIB patents with cathode materials: {len(sib_positive_electrode)}")
print(f"SIB cathode patents with citations: {len(sib_cathode)}")

#==============================================================
# Function to sort materials in material-level strings
#==============================================================

def sort_material_levels(material_level):
    """Sort materials in a comma-separated string."""
    if not isinstance(material_level, str):
        return material_level
    materials = material_level.split(", ")
    sorted_materials = sorted(materials)
    return ", ".join(sorted_materials)

# Sort materials within Material-Level
lib_cathode["Material-Level"] = lib_cathode["Material-Level"].apply(sort_material_levels)
sib_cathode["Material-Level"] = sib_cathode["Material-Level"].apply(sort_material_levels)

#==============================================================
# Create aggregated data for Material-Year nodes
#==============================================================

# Create lists for aggregated data
aggregated_data = []

# Process LIB cathode patents with consistent node numbering
for idx, (_, row) in enumerate(lib_cathode.iterrows()):
    material_list = row["Cathode Material"]
    for material in material_list:
        if material != "none":
            aggregated_data.append({
                'material': material,
                'battery_type': 'LIB',
                'priority_date': row['earliest_priority_date'],
                'year': row['priority_year'],
                'node_num': idx,  # Consistent numbering starting from 0
                'family_id': row['Simple Family ID'] if 'Simple Family ID' in row else None,
                'patent_id': row['Lens ID'] if 'Lens ID' in row else None,
                'family_citing_lens_id': row['family_citing_lens_id']
            })

# Process SIB cathode patents with consistent node numbering
for idx, (_, row) in enumerate(sib_cathode.iterrows()):
    material_list = row["Cathode Material"]
    for material in material_list:
        if material != "none":
            aggregated_data.append({
                'material': material,
                'battery_type': 'SIB',
                'priority_date': row['earliest_priority_date'],
                'year': row['priority_year'],
                'node_num': idx + len(lib_cathode),  # Continue numbering after LIB patents
                'family_id': row['Simple Family ID'] if 'Simple Family ID' in row else None,
                'patent_id': row['Lens ID'] if 'Lens ID' in row else None,
                'family_citing_lens_id': row['family_citing_lens_id']
            })

# Create a DataFrame from the aggregated data
agg_df = pd.DataFrame(aggregated_data)

# Group by year, battery type, and material to count patents
node_data = agg_df.groupby(['year', 'battery_type', 'material']).size().reset_index(name='patent_count')

#==============================================================
# Create node and edge mappings for the network
#==============================================================

# Create a directed graph
G = nx.DiGraph()

# Create a mapping of (year, battery_type, material) to node ID
node_mapping = {}
for i, row in node_data.iterrows():
    node_mapping[(row['year'], row['battery_type'], row['material'])] = i

# Store the reverse mapping for later access
id_to_node = {i: (row['year'], row['battery_type'], row['material']) for i, row in node_data.iterrows()}

# Create a mapping of patents to their year-material nodes
patent_to_nodes = {}
for _, row in agg_df.iterrows():
    node_key = (row['year'], row['battery_type'], row['material'])
    node_id = node_mapping.get(node_key)
    
    if node_id is not None:
        if row['node_num'] not in patent_to_nodes:
            patent_to_nodes[row['node_num']] = []
        patent_to_nodes[row['node_num']].append(node_id)

#==============================================================
# Create patent citation networks
#==============================================================

# Create consistent combined dataset maintaining node numbering
lib_cathode_copy = lib_cathode.copy().reset_index(drop=True)
sib_cathode_copy = sib_cathode.copy().reset_index(drop=True)
sib_cathode_copy.index = sib_cathode_copy.index + len(lib_cathode)

combined_patents = pd.concat([lib_cathode_copy, sib_cathode_copy])

# Create a dictionary to map lens IDs to node numbers
lens_id_to_node = {}
for i, row in combined_patents.iterrows():
    # Extract lens IDs from Simple Family Members
    if isinstance(row['Simple Family Members'], str):
        for lens_id in row['Simple Family Members'].split(';;'):
            lens_id = lens_id.strip()
            if lens_id:  # Ensure non-empty string
                lens_id_to_node[lens_id] = i

# Extract citation links with corrected direction
edge_data = []
for i, row in combined_patents.iterrows():
    if isinstance(row['family_citing_lens_id'], str) and row['family_citing_lens_id']:
        for citing_id in row['family_citing_lens_id'].split(', '):
            citing_id = citing_id.strip()
            if citing_id and citing_id in lens_id_to_node:
                cited_node = i  # Current patent is being cited
                citing_node = lens_id_to_node[citing_id]  # Patent that cites
                
                # Get material-year nodes for both - CORRECTED DIRECTION
                if cited_node in patent_to_nodes and citing_node in patent_to_nodes:
                    for source_id in patent_to_nodes[citing_node]:  # From citing patent
                        for target_id in patent_to_nodes[cited_node]:  # To cited patent
                            if source_id != target_id:  # Avoid self-loops
                                edge_data.append((source_id, target_id))

# Count edge frequencies
edge_weights = {}
for edge in edge_data:
    if edge in edge_weights:
        edge_weights[edge] += 1
    else:
        edge_weights[edge] = 1

# Debug output to verify corrections
print(f"Total LIB cathode patents: {len(lib_cathode)}")
print(f"Total SIB cathode patents: {len(sib_cathode)}")
print(f"Combined patents shape: {combined_patents.shape}")
print(f"Patents mapped to nodes: {len(patent_to_nodes)}")
print(f"Lens IDs mapped: {len(lens_id_to_node)}")
print(f"Total edges found: {len(edge_data)}")
print(f"Unique edges after counting: {len(edge_weights)}")


#==============================================================
# Create the network visualization
#==============================================================


# Define color mappings for cathode materials by battery type
# LIB cathode colors - reds for layered oxides, blues for polyanionic, cyan for spinel
lib_material_colors = {
    # Layered oxides (reds) - better distinguishable
    'LCO': '#FF0000',  # Bright red
    'NMC': '#B22222',  # Fire brick red
    'NCA': '#FF6B6B',  # Light red/coral
    
    # Polyanionic materials (blues)
    'LFP': '#0000FF',  # Blue
    
    # Spinel (cyan)
    'LMO': '#00FFFF',  # Cyan
}

# SIB cathode colors - reds for layered oxides, blues for polyanionic, greens for PBA
sib_material_colors = {
    # Layered oxides (reds) - better distinguishable
    'CFM': '#FF4500',    # Orange red
    'NFM': '#DC143C',    # Crimson
    'NMO': '#FF69B4',    # Hot pink (distinct from reds)
    
    # Polyanionic materials (blues) - better distinguishable
    'NVPF': '#0066FF',      # Royal blue
    'NFPP/NFP': '#000080',  # Navy blue
    
    # PBA (greens)
    'Fe-PBA': '#00CC00',    # Green
    'Mn-PBA': '#006600',    # Dark green
}

# Add nodes to the graph
for i, row in node_data.iterrows():
    year = row['year']
    battery_type = row['battery_type']
    material = row['material']
    patent_count = row['patent_count']
    
    # Select color based on battery type and material
    if battery_type == 'LIB':
        color = lib_material_colors.get(material, 'grey')
    else:  # SIB
        color = sib_material_colors.get(material, 'grey')
    
    # Add node with attributes
    G.add_node(i,
               year=year,
               battery_type=battery_type,
               material=material,
               size=patent_count,
               color=color)

# Debug: Check for missing nodes
print(f"Graph has {G.number_of_nodes()} nodes: {set(G.nodes())}")
all_edge_nodes = set([node for edge in edge_weights.keys() for node in edge])
print(f"Edge_weights contains node IDs: {all_edge_nodes}")
missing_nodes = all_edge_nodes - set(G.nodes())
if missing_nodes:
    print(f"Missing nodes in edge_weights: {missing_nodes}")

# Filter edge_weights for only existing nodes
valid_edge_weights = {}
for (source, target), weight in edge_weights.items():
    if source in G.nodes() and target in G.nodes():
        valid_edge_weights[(source, target)] = weight

print(f"Original edges: {len(edge_weights)}, Valid edges: {len(valid_edge_weights)}")

# Replace edge_weights with valid_edge_weights
edge_weights = valid_edge_weights

# Add edges with weights - now all are valid
for (source, target), weight in edge_weights.items():
    G.add_edge(source, target, weight=weight)

# Count citations between same materials
same_material_citations = 0
for (source, target) in edge_weights.keys():
    source_material = G.nodes[source]['material']
    target_material = G.nodes[target]['material']
    if source_material == target_material:
        same_material_citations += edge_weights[(source, target)]
print(f"Citations within the same material: {same_material_citations}")

# Count SIB-to-SIB citations by material
sib_material_citations = {mat: 0 for mat in sib_cathode_materials}
for (source, target) in edge_weights.keys():
    if (G.nodes[source]['battery_type'] == 'SIB' and 
        G.nodes[target]['battery_type'] == 'SIB'):
        source_material = G.nodes[source]['material']
        target_material = G.nodes[target]['material']
        if source_material == target_material:
            sib_material_citations[source_material] += edge_weights[(source, target)]

print("SIB-to-SIB citations within the same material:")
for mat, count in sib_material_citations.items():
    print(f"  {mat}: {count}")

# Count LIB-to-LIB citations by material
lib_material_citations = {mat: 0 for mat in lib_cathode_materials}
for (source, target) in edge_weights.keys():
    if (G.nodes[source]['battery_type'] == 'LIB' and 
        G.nodes[target]['battery_type'] == 'LIB'):
        source_material = G.nodes[source]['material']
        target_material = G.nodes[target]['material']
        if source_material == target_material:
            lib_material_citations[source_material] += edge_weights[(source, target)]

print("LIB-to-LIB citations within the same material:")
for mat, count in lib_material_citations.items():
    print(f"  {mat}: {count}")

#==============================================================
# Calculate node positions
#==============================================================

# Define material orders for both battery types
lib_material_order = ['LFP', 'LMO', 'LCO', 'NMC', 'NCA']
sib_material_order = ['Fe-PBA', 'Mn-PBA', 'NVPF', 'NFPP/NFP', 'NFM', 'CFM', 'NMO']

# Calculate y-positions for each material
# LIB materials at the top, SIB materials at the bottom
y_spacing = 50  # Increased spacing between materials (was 40)
material_y_positions = {}

# Position LIB materials
for idx, mat in enumerate(lib_material_order):
    material_y_positions[('LIB', mat)] = (len(lib_material_order) - idx) * y_spacing

# Position SIB materials
for idx, mat in enumerate(sib_material_order):
    material_y_positions[('SIB', mat)] = -(idx + 1) * y_spacing

# Calculate positions for each node
pos = {}
for node_id, (year, battery_type, material) in id_to_node.items():
    # x-position based on year
    x = year
    # y-position based on material and battery type
    y = material_y_positions.get((battery_type, material), 0)
    pos[node_id] = (x, y)

#==============================================================
# Create the Plotly visualization
#==============================================================

# Create edge traces
edge_traces = []
min_width = 0.5   # Minimum edge width
max_width = 5     # Maximum edge width (reduced from 7 to prevent visual overwhelming)

# Find maximum edge weight for scaling
max_weight = max([G.edges[edge]['weight'] for edge in G.edges()]) if G.edges() else 1

# Function to scale edge width based on weight - using square root for better visual distribution
def scale_edge_width(weight):
    # Using sqrt for more balanced visual representation of weights
    return min_width + (max_width - min_width) * ((weight / max_weight) ** 0.5)

# Define structural classes for materials
structure_classes = {
    # LIB materials
    'LCO': 'layered', 'NMC': 'layered', 'NCA': 'layered',
    'LFP': 'polyanionic', 
    'LMO': 'spinel',
    # SIB materials
    'CFM': 'layered', 'NFM': 'layered', 'NMO': 'layered',
    'NVPF': 'polyanionic', 'NFPP/NFP': 'polyanionic',
    'Fe-PBA': 'pba', 'Mn-PBA': 'pba'
}

# Define edge colors with transparency
edge_colors = {
    'intra_material': 'rgba(0, 0, 0, 0.8)',                    # Schwarz
    'same_structure_intra_tech': 'rgba(180, 180, 180, 0.5)',   # Etwas dunkleres helles Grau
    'diff_structure_intra_tech': 'rgba(120, 120, 120, 0.5)',   # Mittleres Grau
    'same_structure_inter_tech': 'rgba(255, 140, 0, 0.6)',     # Orange
    'diff_structure_inter_tech': 'rgba(220, 20, 60, 0.6)'      # Crimson
}

# Function to determine edge category
def get_edge_category(source_material, target_material, source_type, target_type):
    """Determine the category of an edge based on materials and battery types."""
    # Same material
    if source_material == target_material:
        return 'intra_material'
    
    # Get structural classes
    source_structure = structure_classes.get(source_material, 'unknown')
    target_structure = structure_classes.get(target_material, 'unknown')
    
    # Same technology (LIB or SIB)
    if source_type == target_type:
        if source_structure == target_structure:
            return 'same_structure_intra_tech'
        else:
            return 'diff_structure_intra_tech'
    # Different technology (LIB <-> SIB)
    else:
        if source_structure == target_structure:
            return 'same_structure_inter_tech'
        else:
            return 'diff_structure_inter_tech'

# Create edge traces with new categorization
for edge in G.edges():
    source, target = edge
    x0, y0 = pos[source]
    x1, y1 = pos[target]
    weight = G.edges[source, target]['weight']
    width = scale_edge_width(weight)
    
    # Get materials and battery types
    source_material = G.nodes[source]['material']
    target_material = G.nodes[target]['material']
    source_type = G.nodes[source]['battery_type']
    target_type = G.nodes[target]['battery_type']
    
    # Determine edge category and color
    category = get_edge_category(source_material, target_material, source_type, target_type)
    edge_color = edge_colors[category]
    
    # Create descriptive text for hover
    category_descriptions = {
        'intra_material': 'Within same material',
        'same_structure_intra_tech': 'Same structure, same technology',
        'diff_structure_intra_tech': 'Different structure, same technology',
        'same_structure_inter_tech': 'Same structure, different technology',
        'diff_structure_inter_tech': 'Different structure, different technology'
    }
    
    # Create edge trace
    edge_trace = go.Scatter(
        x=[x0, x1, None], 
        y=[y0, y1, None],
        mode='lines',
        line=dict(
            width=width, 
            color=edge_color
        ),
        hoverinfo='text',
        text=f"Citations: {weight}<br>From: {source_type} {source_material} ({G.nodes[source]['year']})<br>To: {target_type} {target_material} ({G.nodes[target]['year']})<br>Category: {category_descriptions[category]}",
        showlegend=False
    )
    edge_traces.append(edge_trace)

# Create node traces
node_x, node_y = [], []
node_size, node_color, node_text = [], [], []
lib_nodes, sib_nodes = [], []

# Node size scaling parameters
min_node_size = 6    # Minimum node size (reduced from 8)
max_node_size = 30   # Maximum node size
size_scale_factor = 1.5  # Scale factor (reduced from 2)

# Find maximum patent count for scaling
max_patent_count = max([G.nodes[n]['size'] for n in G.nodes()]) if G.nodes() else 1

# Function to scale node size based on patent count
def scale_node_size(patent_count):
    # Using sqrt for better visual scaling
    return min_node_size + (max_node_size - min_node_size) * ((patent_count / max_patent_count) ** 0.5)

# Separate nodes by battery type
for node in G.nodes():
    x, y = pos[node]
    patent_count = G.nodes[node]['size']
    scaled_size = scale_node_size(patent_count)
    
    # Create node text for hover
    hover_text = (
        f"Material: {G.nodes[node]['material']}<br>"
        f"Battery Type: {G.nodes[node]['battery_type']}<br>"
        f"Year: {G.nodes[node]['year']}<br>"
        f"Patents: {G.nodes[node]['size']}"
    )
    
    # Add to appropriate list based on battery type
    if G.nodes[node]['battery_type'] == 'LIB':
        lib_nodes.append({
            'x': x,
            'y': y,
            'size': scaled_size,
            'color': G.nodes[node]['color'],
            'text': hover_text
        })
    else:  # SIB
        sib_nodes.append({
            'x': x,
            'y': y,
            'size': scaled_size,
            'color': G.nodes[node]['color'],
            'text': hover_text
        })

# Create LIB node trace
lib_node_trace = go.Scatter(
    x=[node['x'] for node in lib_nodes],
    y=[node['y'] for node in lib_nodes],
    mode='markers',
    marker=dict(
        size=[node['size'] for node in lib_nodes],
        color=[node['color'] for node in lib_nodes],
        line=dict(width=1, color='black')
    ),
    text=[node['text'] for node in lib_nodes],
    hoverinfo='text',
    showlegend=False  # Entfernt aus Legende
)

# Create SIB node trace
sib_node_trace = go.Scatter(
    x=[node['x'] for node in sib_nodes],
    y=[node['y'] for node in sib_nodes],
    mode='markers',
    marker=dict(
        size=[node['size'] for node in sib_nodes],
        color=[node['color'] for node in sib_nodes],
        line=dict(width=1, color='black')
    ),
    text=[node['text'] for node in sib_nodes],
    hoverinfo='text',
    showlegend=False  # Entfernt aus Legende
)

# Create SIB node trace
sib_node_trace = go.Scatter(
    x=[node['x'] for node in sib_nodes],
    y=[node['y'] for node in sib_nodes],
    mode='markers',
    marker=dict(
        size=[node['size'] for node in sib_nodes],
        color=[node['color'] for node in sib_nodes],
        line=dict(width=1, color='black')
    ),
    text=[node['text'] for node in sib_nodes],
    hoverinfo='text',
    name='SIB Cathode Materials'
)

# Create legend entries
legend_entries = []

# LIB material legends
for material, color in lib_material_colors.items():
    legend_entries.append(go.Scatter(
        x=[None], y=[None],
        mode='markers',
        marker=dict(size=10, color=color),
        name=f'LIB: {material}',
        hoverinfo='none'
    ))

# SIB material legends
for material, color in sib_material_colors.items():
    legend_entries.append(go.Scatter(
        x=[None], y=[None],
        mode='markers',
        marker=dict(size=10, color=color),
        name=f'SIB: {material}',
        hoverinfo='none'
    ))

# Citation type legends
legend_entries.append(go.Scatter(
    x=[None], y=[None],
    mode='lines',
    line=dict(color='rgba(0, 0, 0, 0.8)', width=3),
    name='Within same material',
    hoverinfo='none'
))

legend_entries.append(go.Scatter(
    x=[None], y=[None],
    mode='lines',
    line=dict(color='rgba(180, 180, 180, 0.5)', width=3),
    name='Same structure, same technology',
    hoverinfo='none'
))

legend_entries.append(go.Scatter(
    x=[None], y=[None],
    mode='lines',
    line=dict(color='rgba(120, 120, 120, 0.5)', width=3),
    name='Different structure, same technology',
    hoverinfo='none'
))

legend_entries.append(go.Scatter(
    x=[None], y=[None],
    mode='lines',
    line=dict(color='rgba(255, 140, 0, 0.6)', width=3),
    name='Same structure, different technology',
    hoverinfo='none'
))

legend_entries.append(go.Scatter(
    x=[None], y=[None],
    mode='lines',
    line=dict(color='rgba(220, 20, 60, 0.6)', width=3),
    name='Different structure, different technology',
    hoverinfo='none'
))

# Create the figure
fig = go.Figure(data=edge_traces + [lib_node_trace, sib_node_trace] + legend_entries)

# Calculate x-axis ticks (years)
min_year = min([G.nodes[n]['year'] for n in G.nodes()])
max_year = max([G.nodes[n]['year'] for n in G.nodes()])
x_ticks = list(range(min_year, max_year + 1, 5))

# Create y-axis ticks and labels
y_ticks = []
y_tick_labels = []

# Add divider line between LIB and SIB materials
divider_y = 0
fig.add_shape(
    type="line",
    x0=min_year-1,
    y0=divider_y,
    x1=max_year+1,
    y1=divider_y,
    line=dict(
        color="black",
        width=1,
        dash="dash",
    )
)

# Add text label for the divider
fig.add_annotation(
    x=min_year-2,
    y=divider_y,
    xref="x",
    yref="y",
    text="LIB | SIB",
    showarrow=False,
    font=dict(
        size=18,  # Divider-Text-Schriftgröße erhöht
        color="black"
    )
)

# Y-ticks for LIB materials
for mat in lib_material_order:
    y_pos = material_y_positions.get(('LIB', mat), 0)
    y_ticks.append(y_pos)
    y_tick_labels.append(f"LIB: {mat}")

# Y-ticks for SIB materials
for mat in sib_material_order:
    y_pos = material_y_positions.get(('SIB', mat), 0)
    y_ticks.append(y_pos)
    y_tick_labels.append(f"SIB: {mat}")

# Update layout
fig.update_layout(
    title=dict(
        text="Lithium-Ion and Sodium-Ion Battery Positive Electrode Materials Patent Network",
        font=dict(size=24) # Title font size
    ),
    showlegend=True,
    xaxis=dict(
        title=dict(text="Priority Year", font=dict(size=20)),  # Achsentitel-Schriftgröße
        tickmode='array',
        tickvals=x_ticks,
        ticktext=[str(year) for year in x_ticks],
        tickfont=dict(size=18),  # Achsen-Tick-Schriftgröße
        showgrid=True,
        gridwidth=1,
        gridcolor='lightgrey'
    ),
    yaxis=dict(
        tickmode='array',
        tickvals=y_ticks,
        ticktext=y_tick_labels,
        title=dict(text="Positive Electrode Material", font=dict(size=20)),  # Achsentitel-Schriftgröße
        tickfont=dict(size=18),  # Achsen-Tick-Schriftgröße
        zeroline=False
    ),
    hovermode='closest',
    hoverlabel=dict(font=dict(size=16)),  # Hover-Text-Schriftgröße erhöht
    plot_bgcolor='rgba(0,0,0,0)',
    paper_bgcolor='rgba(0,0,0,0)',
    legend=dict(
        yanchor="top", y=0.99, xanchor="left", x=1.01,
        bgcolor="rgba(255, 255, 255, 0.8)", font=dict(size=18)  # Legenden-Schriftgröße
    ),
    width=1800,
    height=1000,
    margin=dict(l=150, r=250, t=80, b=50)
)

# Save the visualization
output_file = "LIB_SIB_Positive Electrode_Material_Patent_Network.html"
fig.write_html(output_file)
print(f"Visualization saved to {output_file}")

# Print network statistics
print("\nNetwork Statistics:")
print(f"Total material-year nodes: {G.number_of_nodes()}")
print(f"LIB material-year nodes: {len([n for n in G.nodes() if G.nodes[n]['battery_type'] == 'LIB'])}")
print(f"SIB material-year nodes: {len([n for n in G.nodes() if G.nodes[n]['battery_type'] == 'SIB'])}")
print(f"Total citations: {G.number_of_edges()}")

# Count cross-battery citations
cross_citations = sum(1 for u, v in G.edges() 
                     if G.nodes[u]['battery_type'] != G.nodes[v]['battery_type'])
print(f"Cross-battery citations: {cross_citations}")

# Calculate citation directionality
lib_to_sib = sum(1 for u, v in G.edges() 
                if G.nodes[u]['battery_type'] == 'LIB' and G.nodes[v]['battery_type'] == 'SIB')
sib_to_lib = sum(1 for u, v in G.edges() 
                if G.nodes[u]['battery_type'] == 'SIB' and G.nodes[v]['battery_type'] == 'LIB')

print(f"LIB materials citing SIB materials: {lib_to_sib}")
print(f"SIB materials citing LIB materials: {sib_to_lib}")

print("\nAnalysis complete!")


# Additional Statistics - Add this at the end of your existing code

print("\n" + "="*80)
print("DETAILED PATENT NETWORK STATISTICS")
print("="*80)

#==============================================================
# 1. Number of patents per material
#==============================================================

print("\n1. NUMBER OF PATENTS PER MATERIAL:")
print("-" * 40)

# LIB materials
print("LIB Materials:")
lib_material_patents = {}
for material in lib_cathode_materials:
    count = len(lib_cathode[lib_cathode['Cathode Material'].apply(lambda x: material in x)])
    lib_material_patents[material] = count
    print(f"  {material}: {count} patents")

print(f"  Total LIB patents: {sum(lib_material_patents.values())}")

print("\nSIB Materials:")
sib_material_patents = {}
for material in sib_cathode_materials:
    count = len(sib_cathode[sib_cathode['Cathode Material'].apply(lambda x: material in x)])
    sib_material_patents[material] = count
    print(f"  {material}: {count} patents")

print(f"  Total SIB patents: {sum(sib_material_patents.values())}")

#==============================================================
# 2. Intra-technology citations (within LIB or within SIB)
#==============================================================

print("\n2. INTRA-TECHNOLOGY CITATIONS:")
print("-" * 40)

# LIB-to-LIB citations by material combination
print("LIB-to-LIB Citations:")
lib_intra_citations = {}
lib_total_intra = 0

for (source, target), weight in edge_weights.items():
    source_type = G.nodes[source]['battery_type']
    target_type = G.nodes[target]['battery_type']
    
    if source_type == 'LIB' and target_type == 'LIB':
        source_material = G.nodes[source]['material']
        target_material = G.nodes[target]['material']
        
        citation_pair = f"{source_material} → {target_material}"
        if citation_pair not in lib_intra_citations:
            lib_intra_citations[citation_pair] = 0
        lib_intra_citations[citation_pair] += weight
        lib_total_intra += weight

# Sort and display LIB intra-citations
for pair, count in sorted(lib_intra_citations.items(), key=lambda x: x[1], reverse=True):
    print(f"  {pair}: {count} citations")

print(f"  Total LIB-to-LIB citations: {lib_total_intra}")

# SIB-to-SIB citations by material combination
print("\nSIB-to-SIB Citations:")
sib_intra_citations = {}
sib_total_intra = 0

for (source, target), weight in edge_weights.items():
    source_type = G.nodes[source]['battery_type']
    target_type = G.nodes[target]['battery_type']
    
    if source_type == 'SIB' and target_type == 'SIB':
        source_material = G.nodes[source]['material']
        target_material = G.nodes[target]['material']
        
        citation_pair = f"{source_material} → {target_material}"
        if citation_pair not in sib_intra_citations:
            sib_intra_citations[citation_pair] = 0
        sib_intra_citations[citation_pair] += weight
        sib_total_intra += weight

# Sort and display SIB intra-citations
for pair, count in sorted(sib_intra_citations.items(), key=lambda x: x[1], reverse=True):
    print(f"  {pair}: {count} citations")

print(f"  Total SIB-to-SIB citations: {sib_total_intra}")

#==============================================================
# 3. Inter-technology/Cross-technology citations
#==============================================================

print("\n3. INTER-TECHNOLOGY (CROSS) CITATIONS:")
print("-" * 40)

# LIB-to-SIB citations
print("LIB-to-SIB Citations:")
lib_to_sib_citations = {}
lib_to_sib_total = 0

for (source, target), weight in edge_weights.items():
    source_type = G.nodes[source]['battery_type']
    target_type = G.nodes[target]['battery_type']
    
    if source_type == 'LIB' and target_type == 'SIB':
        source_material = G.nodes[source]['material']
        target_material = G.nodes[target]['material']
        
        citation_pair = f"{source_material} → {target_material}"
        if citation_pair not in lib_to_sib_citations:
            lib_to_sib_citations[citation_pair] = 0
        lib_to_sib_citations[citation_pair] += weight
        lib_to_sib_total += weight

# Sort and display LIB-to-SIB citations
for pair, count in sorted(lib_to_sib_citations.items(), key=lambda x: x[1], reverse=True):
    print(f"  {pair}: {count} citations")

print(f"  Total LIB-to-SIB citations: {lib_to_sib_total}")

# SIB-to-LIB citations
print("\nSIB-to-LIB Citations:")
sib_to_lib_citations = {}
sib_to_lib_total = 0

for (source, target), weight in edge_weights.items():
    source_type = G.nodes[source]['battery_type']
    target_type = G.nodes[target]['battery_type']
    
    if source_type == 'SIB' and target_type == 'LIB':
        source_material = G.nodes[source]['material']
        target_material = G.nodes[target]['material']
        
        citation_pair = f"{source_material} → {target_material}"
        if citation_pair not in sib_to_lib_citations:
            sib_to_lib_citations[citation_pair] = 0
        sib_to_lib_citations[citation_pair] += weight
        sib_to_lib_total += weight

# Sort and display SIB-to-LIB citations
for pair, count in sorted(sib_to_lib_citations.items(), key=lambda x: x[1], reverse=True):
    print(f"  {pair}: {count} citations")

print(f"  Total SIB-to-LIB citations: {sib_to_lib_total}")

#==============================================================
# 4. Summary statistics
#==============================================================

print("\n4. SUMMARY STATISTICS:")
print("-" * 40)

total_citations = sum(edge_weights.values())
total_cross_citations = lib_to_sib_total + sib_to_lib_total

print(f"Total patents analyzed: {len(combined_patents)}")
print(f"  - LIB patents: {len(lib_cathode)}")
print(f"  - SIB patents: {len(sib_cathode)}")

print(f"\nTotal citations: {total_citations}")
print(f"  - Intra-technology citations: {lib_total_intra + sib_total_intra} ({((lib_total_intra + sib_total_intra)/total_citations*100):.1f}%)")
print(f"    • LIB-to-LIB: {lib_total_intra}")
print(f"    • SIB-to-SIB: {sib_total_intra}")
print(f"  - Inter-technology citations: {total_cross_citations} ({(total_cross_citations/total_citations*100):.1f}%)")
print(f"    • LIB-to-SIB: {lib_to_sib_total}")
print(f"    • SIB-to-LIB: {sib_to_lib_total}")

# Citation directionality in cross-technology flow
if total_cross_citations > 0:
    print(f"\nCross-technology citation flow:")
    print(f"  - LIB → SIB: {(lib_to_sib_total/total_cross_citations*100):.1f}%")
    print(f"  - SIB → LIB: {(sib_to_lib_total/total_cross_citations*100):.1f}%")

print("\n" + "="*80)

#%%

#==========================================================================================================================
# Building a citation network based on LIB and SIB datasets, focusing only on positive electrode (cathode) patents
# Modified to create three separate visualizations: nodes only, intra-tech edges, and inter-tech edges
#==========================================================================================================================

import pandas as pd
import networkx as nx
import plotly.graph_objects as go
import numpy as np
from datetime import datetime

# File paths for both datasets
lib_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_full_data_classified_onlyLIB_27052025.xlsx"
sib_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_full_data_classified_final_onlySIB_26052025.xlsx"
sheet_name = "Sheet1"

# Load both Excel files
LIB_patents = pd.read_excel(lib_file_path, sheet_name=sheet_name)
SIB_patents = pd.read_excel(sib_file_path, sheet_name=sheet_name)

# Print basic info about both datasets
print(f"LIB dataset shape: {LIB_patents.shape}")
print(f"SIB dataset shape: {SIB_patents.shape}")

# Add a 'battery_type' column to identify the source in the combined dataset
LIB_patents['battery_type'] = 'LIB'
SIB_patents['battery_type'] = 'SIB'

#==============================================================
# Define material sets for both battery types
#==============================================================

# Define LIB cathode materials
lib_cathode_materials = {"LCO", "LMO", "LFP", "NMC", "NCA"}

# Define SIB cathode materials
sib_cathode_materials = {"Fe-PBA", "Mn-PBA", "NVPF", "NFPP/NFP", "NFM", "CFM", "NMO"}

#==============================================================
# Function to extract materials from material strings
#==============================================================

def extract_materials(material_str, material_set):
    """Extract materials that match the given material set from a comma-separated string."""
    materials = set(material_str.split(", ")) if isinstance(material_str, str) else set()
    extracted = list(materials.intersection(material_set))
    return extracted if extracted else ["none"]

#==============================================================
# Process LIB dataset
#==============================================================

# Extract priority year from earliest_priority_date for LIB
LIB_patents["earliest_priority_date"] = pd.to_datetime(LIB_patents["Earliest Priority Date"])
LIB_patents["priority_year"] = LIB_patents["earliest_priority_date"].dt.year
LIB_patents = LIB_patents.sort_values(by=["priority_year"], ascending=True)

# Create new column for Cathode Materials for LIB
LIB_patents["Cathode Material"] = LIB_patents["Material-Level"].apply(lambda x: extract_materials(x, lib_cathode_materials))

# Update Component-Level based on Material-Level for LIB
for index, row in LIB_patents.iterrows():
    if row["Material-Level"] != "none" and row["Component-Level"] == "none":
        materials = set(row["Material-Level"].split(", ")) if isinstance(row["Material-Level"], str) else set()
        cathode_match = materials & lib_cathode_materials
        
        if cathode_match:
            LIB_patents.at[index, "Component-Level"] = "Positive Electrode"

# Update Component-Level for more specific components in LIB
for index, row in LIB_patents.iterrows():
    if row["Material-Level"] != "none" and row["Component-Level"] != "Cell":
        materials = set(row["Material-Level"].split(", ")) if isinstance(row["Material-Level"], str) else set()
        cathode_match = materials & lib_cathode_materials
        
        existing_components = set(row["Component-Level"].split(", ")) if isinstance(row["Component-Level"], str) and row["Component-Level"] != "none" else set()
        
        if cathode_match:
            existing_components.add("Positive Electrode")
            
        LIB_patents.at[index, "Component-Level"] = ", ".join(existing_components) if existing_components else "none"

# Create family_citing_lens_id_count for LIB dataset by counting citations
LIB_patents["family_citing_lens_id_count"] = LIB_patents["family_citing_lens_id"].apply(
    lambda x: len(x.split(", ")) if isinstance(x, str) and x else 0
)

# Sort component values alphabetically for LIB
for index, row in LIB_patents.iterrows():
    if row["Component-Level"] != "none":
        sorted_components = ", ".join(sorted(row["Component-Level"].split(", ")))
        LIB_patents.at[index, "Component-Level"] = sorted_components

#==============================================================
# Process SIB dataset
#==============================================================

# Extract priority year from earliest_priority_date for SIB
SIB_patents["earliest_priority_date"] = pd.to_datetime(SIB_patents["Earliest Priority Date"])
SIB_patents["priority_year"] = SIB_patents["earliest_priority_date"].dt.year
SIB_patents = SIB_patents.sort_values(by=["priority_year"], ascending=True)

# Create new column for Cathode Materials for SIB
SIB_patents["Cathode Material"] = SIB_patents["Material-Level"].apply(lambda x: extract_materials(x, sib_cathode_materials))

# Update Component-Level based on Material-Level for SIB
for index, row in SIB_patents.iterrows():
    if row["Material-Level"] != "none" and row["Component-Level"] == "none":
        materials = set(row["Material-Level"].split(", ")) if isinstance(row["Material-Level"], str) else set()
        cathode_match = materials & sib_cathode_materials
        
        if cathode_match:
            SIB_patents.at[index, "Component-Level"] = "Positive Electrode"

# Update Component-Level for more specific components in SIB
for index, row in SIB_patents.iterrows():
    if row["Material-Level"] != "none" and row["Component-Level"] != "Cell":
        materials = set(row["Material-Level"].split(", ")) if isinstance(row["Material-Level"], str) else set()
        cathode_match = materials & sib_cathode_materials
        
        existing_components = set(row["Component-Level"].split(", ")) if isinstance(row["Component-Level"], str) and row["Component-Level"] != "none" else set()
        
        if cathode_match:
            existing_components.add("Positive Electrode")
            
        SIB_patents.at[index, "Component-Level"] = ", ".join(existing_components) if existing_components else "none"

# Create family_citing_lens_id_count for SIB dataset by counting citations
SIB_patents["family_citing_lens_id_count"] = SIB_patents["family_citing_lens_id"].apply(
    lambda x: len(x.split(", ")) if isinstance(x, str) and x else 0
)

# Sort component values alphabetically for SIB
for index, row in SIB_patents.iterrows():
    if row["Component-Level"] != "none":
        sorted_components = ", ".join(sorted(row["Component-Level"].split(", ")))
        SIB_patents.at[index, "Component-Level"] = sorted_components

#==============================================================
# Filter patents to retain only positive electrode (cathode) patents
#==============================================================

# Function to check if any cathode material is present in Material-Level
def contains_cathode_material(material_str, cathode_materials):
    """Check if the material string contains any cathode material from the given set."""
    if not isinstance(material_str, str) or material_str == "none":
        return False
    
    materials = set(material_str.split(", "))
    return bool(materials.intersection(cathode_materials))

# Filter LIB dataset for patents containing LIB cathode materials
lib_positive_electrode = LIB_patents[
    LIB_patents["Material-Level"].apply(
        lambda x: contains_cathode_material(x, lib_cathode_materials)
    )
]

# Filter LIB patents to only include those with at least one citation and cathode materials
lib_cathode = lib_positive_electrode[
    (lib_positive_electrode["family_citing_lens_id_count"] > 0) & 
    (lib_positive_electrode["Cathode Material"].apply(lambda x: x != ["none"]))
]

# Filter SIB dataset for patents containing SIB cathode materials
sib_positive_electrode = SIB_patents[
    SIB_patents["Material-Level"].apply(
        lambda x: contains_cathode_material(x, sib_cathode_materials)
    )
]

# Filter SIB patents to only include those with at least one citation and cathode materials
sib_cathode = sib_positive_electrode[
    (sib_positive_electrode["family_citing_lens_id_count"] > 0) & 
    (sib_positive_electrode["Cathode Material"].apply(lambda x: x != ["none"]))
]

# Print info about filtered datasets
print(f"LIB patents with cathode materials: {len(lib_positive_electrode)}")
print(f"LIB cathode patents with citations: {len(lib_cathode)}")
print(f"SIB patents with cathode materials: {len(sib_positive_electrode)}")
print(f"SIB cathode patents with citations: {len(sib_cathode)}")

#==============================================================
# Function to sort materials in material-level strings
#==============================================================

def sort_material_levels(material_level):
    """Sort materials in a comma-separated string."""
    if not isinstance(material_level, str):
        return material_level
    materials = material_level.split(", ")
    sorted_materials = sorted(materials)
    return ", ".join(sorted_materials)

# Sort materials within Material-Level
lib_cathode["Material-Level"] = lib_cathode["Material-Level"].apply(sort_material_levels)
sib_cathode["Material-Level"] = sib_cathode["Material-Level"].apply(sort_material_levels)

#==============================================================
# Create aggregated data for Material-Year nodes
#==============================================================

# Create lists for aggregated data
aggregated_data = []

# Process LIB cathode patents with consistent node numbering
for idx, (_, row) in enumerate(lib_cathode.iterrows()):
    material_list = row["Cathode Material"]
    for material in material_list:
        if material != "none":
            aggregated_data.append({
                'material': material,
                'battery_type': 'LIB',
                'priority_date': row['earliest_priority_date'],
                'year': row['priority_year'],
                'node_num': idx,  # Consistent numbering starting from 0
                'family_id': row['Simple Family ID'] if 'Simple Family ID' in row else None,
                'patent_id': row['Lens ID'] if 'Lens ID' in row else None,
                'family_citing_lens_id': row['family_citing_lens_id']
            })

# Process SIB cathode patents with consistent node numbering
for idx, (_, row) in enumerate(sib_cathode.iterrows()):
    material_list = row["Cathode Material"]
    for material in material_list:
        if material != "none":
            aggregated_data.append({
                'material': material,
                'battery_type': 'SIB',
                'priority_date': row['earliest_priority_date'],
                'year': row['priority_year'],
                'node_num': idx + len(lib_cathode),  # Continue numbering after LIB patents
                'family_id': row['Simple Family ID'] if 'Simple Family ID' in row else None,
                'patent_id': row['Lens ID'] if 'Lens ID' in row else None,
                'family_citing_lens_id': row['family_citing_lens_id']
            })

# Create a DataFrame from the aggregated data
agg_df = pd.DataFrame(aggregated_data)

# Group by year, battery type, and material to count patents
node_data = agg_df.groupby(['year', 'battery_type', 'material']).size().reset_index(name='patent_count')

#==============================================================
# Create node and edge mappings for the network
#==============================================================

# Create a directed graph
G = nx.DiGraph()

# Create a mapping of (year, battery_type, material) to node ID
node_mapping = {}
for i, row in node_data.iterrows():
    node_mapping[(row['year'], row['battery_type'], row['material'])] = i

# Store the reverse mapping for later access
id_to_node = {i: (row['year'], row['battery_type'], row['material']) for i, row in node_data.iterrows()}

# Create a mapping of patents to their year-material nodes
patent_to_nodes = {}
for _, row in agg_df.iterrows():
    node_key = (row['year'], row['battery_type'], row['material'])
    node_id = node_mapping.get(node_key)
    
    if node_id is not None:
        if row['node_num'] not in patent_to_nodes:
            patent_to_nodes[row['node_num']] = []
        patent_to_nodes[row['node_num']].append(node_id)

#==============================================================
# Create patent citation networks
#==============================================================

# Create consistent combined dataset maintaining node numbering
lib_cathode_copy = lib_cathode.copy().reset_index(drop=True)
sib_cathode_copy = sib_cathode.copy().reset_index(drop=True)
sib_cathode_copy.index = sib_cathode_copy.index + len(lib_cathode)

combined_patents = pd.concat([lib_cathode_copy, sib_cathode_copy])

# Create a dictionary to map lens IDs to node numbers
lens_id_to_node = {}
for i, row in combined_patents.iterrows():
    # Extract lens IDs from Simple Family Members
    if isinstance(row['Simple Family Members'], str):
        for lens_id in row['Simple Family Members'].split(';;'):
            lens_id = lens_id.strip()
            if lens_id:  # Ensure non-empty string
                lens_id_to_node[lens_id] = i

# Extract citation links with corrected direction
edge_data = []
for i, row in combined_patents.iterrows():
    if isinstance(row['family_citing_lens_id'], str) and row['family_citing_lens_id']:
        for citing_id in row['family_citing_lens_id'].split(', '):
            citing_id = citing_id.strip()
            if citing_id and citing_id in lens_id_to_node:
                cited_node = i  # Current patent is being cited
                citing_node = lens_id_to_node[citing_id]  # Patent that cites
                
                # Get material-year nodes for both - CORRECTED DIRECTION
                if cited_node in patent_to_nodes and citing_node in patent_to_nodes:
                    for source_id in patent_to_nodes[citing_node]:  # From citing patent
                        for target_id in patent_to_nodes[cited_node]:  # To cited patent
                            if source_id != target_id:  # Avoid self-loops
                                edge_data.append((source_id, target_id))

# Count edge frequencies
edge_weights = {}
for edge in edge_data:
    if edge in edge_weights:
        edge_weights[edge] += 1
    else:
        edge_weights[edge] = 1

# Debug output to verify corrections
print(f"Total LIB cathode patents: {len(lib_cathode)}")
print(f"Total SIB cathode patents: {len(sib_cathode)}")
print(f"Combined patents shape: {combined_patents.shape}")
print(f"Patents mapped to nodes: {len(patent_to_nodes)}")
print(f"Lens IDs mapped: {len(lens_id_to_node)}")
print(f"Total edges found: {len(edge_data)}")
print(f"Unique edges after counting: {len(edge_weights)}")

#==============================================================
# Add nodes to the graph (common for all visualizations)
#==============================================================

# Define color mappings for cathode materials by battery type
# LIB cathode colors - reds for layered oxides, blues for polyanionic, cyan for spinel
lib_material_colors = {
    # Layered oxides (reds) - better distinguishable
    'LCO': '#FF0000',  # Bright red
    'NMC': '#B22222',  # Fire brick red
    'NCA': '#FF6B6B',  # Light red/coral
    
    # Polyanionic materials (blues)
    'LFP': '#0000FF',  # Blue
    
    # Spinel (cyan)
    'LMO': '#00FFFF',  # Cyan
}

# SIB cathode colors - reds for layered oxides, blues for polyanionic, greens for PBA
sib_material_colors = {
    # Layered oxides (reds) - better distinguishable
    'CFM': '#FF4500',    # Orange red
    'NFM': '#DC143C',    # Crimson
    'NMO': '#FF69B4',    # Hot pink (distinct from reds)
    
    # Polyanionic materials (blues) - better distinguishable
    'NVPF': '#0066FF',      # Royal blue
    'NFPP/NFP': '#000080',  # Navy blue
    
    # PBA (greens)
    'Fe-PBA': '#00CC00',    # Green
    'Mn-PBA': '#006600',    # Dark green
}

# Add nodes to the graph
for i, row in node_data.iterrows():
    year = row['year']
    battery_type = row['battery_type']
    material = row['material']
    patent_count = row['patent_count']
    
    # Select color based on battery type and material
    if battery_type == 'LIB':
        color = lib_material_colors.get(material, 'grey')
    else:  # SIB
        color = sib_material_colors.get(material, 'grey')
    
    # Add node with attributes
    G.add_node(i,
               year=year,
               battery_type=battery_type,
               material=material,
               size=patent_count,
               color=color)

#==============================================================
# Calculate node positions (common for all visualizations)
#==============================================================

# Define material orders for both battery types
lib_material_order = ['LFP', 'LMO', 'LCO', 'NMC', 'NCA']
sib_material_order = ['Fe-PBA', 'Mn-PBA', 'NVPF', 'NFPP/NFP', 'NFM', 'CFM', 'NMO']

# Calculate y-positions for each material
# LIB materials at the top, SIB materials at the bottom
y_spacing = 50  # Increased spacing between materials (was 40)
material_y_positions = {}

# Position LIB materials
for idx, mat in enumerate(lib_material_order):
    material_y_positions[('LIB', mat)] = (len(lib_material_order) - idx) * y_spacing

# Position SIB materials
for idx, mat in enumerate(sib_material_order):
    material_y_positions[('SIB', mat)] = -(idx + 1) * y_spacing

# Calculate positions for each node
pos = {}
for node_id, (year, battery_type, material) in id_to_node.items():
    # x-position based on year
    x = year
    # y-position based on material and battery type
    y = material_y_positions.get((battery_type, material), 0)
    pos[node_id] = (x, y)

#==============================================================
# Filter edges by technology type
#==============================================================

# Debug: Check for missing nodes
print(f"Graph has {G.number_of_nodes()} nodes: {set(G.nodes())}")
all_edge_nodes = set([node for edge in edge_weights.keys() for node in edge])
print(f"Edge_weights contains node IDs: {all_edge_nodes}")
missing_nodes = all_edge_nodes - set(G.nodes())
if missing_nodes:
    print(f"Missing nodes in edge_weights: {missing_nodes}")

# Filter edge_weights for only existing nodes
valid_edge_weights = {}
for (source, target), weight in edge_weights.items():
    if source in G.nodes() and target in G.nodes():
        valid_edge_weights[(source, target)] = weight

print(f"Original edges: {len(edge_weights)}, Valid edges: {len(valid_edge_weights)}")

# Replace edge_weights with valid_edge_weights
edge_weights = valid_edge_weights

# Separate edges by technology type
intra_tech_edges = {}  # Same technology (LIB-LIB or SIB-SIB)
inter_tech_edges = {}  # Different technology (LIB-SIB or SIB-LIB)

for (source, target), weight in edge_weights.items():
    source_type = G.nodes[source]['battery_type']
    target_type = G.nodes[target]['battery_type']
    
    if source_type == target_type:
        intra_tech_edges[(source, target)] = weight
    else:
        inter_tech_edges[(source, target)] = weight

print(f"Intra-technology edges: {len(intra_tech_edges)}")
print(f"Inter-technology edges: {len(inter_tech_edges)}")

#==============================================================
# Common functions for visualization
#==============================================================

# Define structural classes for materials
structure_classes = {
    # LIB materials
    'LCO': 'layered', 'NMC': 'layered', 'NCA': 'layered',
    'LFP': 'polyanionic', 
    'LMO': 'spinel',
    # SIB materials
    'CFM': 'layered', 'NFM': 'layered', 'NMO': 'layered',
    'NVPF': 'polyanionic', 'NFPP/NFP': 'polyanionic',
    'Fe-PBA': 'pba', 'Mn-PBA': 'pba'
}

# Define edge colors with transparency
edge_colors = {
    'intra_material': 'rgba(0, 0, 0, 0.8)',                    # Schwarz
    'same_structure_intra_tech': 'rgba(180, 180, 180, 0.5)',   # Etwas dunkleres helles Grau
    'diff_structure_intra_tech': 'rgba(120, 120, 120, 0.5)',   # Mittleres Grau
    'same_structure_inter_tech': 'rgba(255, 140, 0, 0.6)',     # Orange
    'diff_structure_inter_tech': 'rgba(220, 20, 60, 0.6)'      # Crimson
}

def get_edge_category(source_material, target_material, source_type, target_type):
    """Determine the category of an edge based on materials and battery types."""
    # Same material
    if source_material == target_material:
        return 'intra_material'
    
    # Get structural classes
    source_structure = structure_classes.get(source_material, 'unknown')
    target_structure = structure_classes.get(target_material, 'unknown')
    
    # Same technology (LIB or SIB)
    if source_type == target_type:
        if source_structure == target_structure:
            return 'same_structure_intra_tech'
        else:
            return 'diff_structure_intra_tech'
    # Different technology (LIB <-> SIB)
    else:
        if source_structure == target_structure:
            return 'same_structure_inter_tech'
        else:
            return 'diff_structure_inter_tech'

def create_edge_traces(selected_edges):
    """Create edge traces for the given set of edges."""
    edge_traces = []
    min_width = 0.5   # Minimum edge width
    max_width = 5     # Maximum edge width
    
    # Find maximum edge weight for scaling
    max_weight = max(selected_edges.values()) if selected_edges else 1
    
    # Function to scale edge width based on weight
    def scale_edge_width(weight):
        return min_width + (max_width - min_width) * ((weight / max_weight) ** 0.5)
    
    for (source, target), weight in selected_edges.items():
        x0, y0 = pos[source]
        x1, y1 = pos[target]
        width = scale_edge_width(weight)
        
        # Get materials and battery types
        source_material = G.nodes[source]['material']
        target_material = G.nodes[target]['material']
        source_type = G.nodes[source]['battery_type']
        target_type = G.nodes[target]['battery_type']
        
        # Determine edge category and color
        category = get_edge_category(source_material, target_material, source_type, target_type)
        edge_color = edge_colors[category]
        
        # Create descriptive text for hover
        category_descriptions = {
            'intra_material': 'Within same material',
            'same_structure_intra_tech': 'Same structure, same technology',
            'diff_structure_intra_tech': 'Different structure, same technology',
            'same_structure_inter_tech': 'Same structure, different technology',
            'diff_structure_inter_tech': 'Different structure, different technology'
        }
        
        # Create edge trace
        edge_trace = go.Scatter(
            x=[x0, x1, None], 
            y=[y0, y1, None],
            mode='lines',
            line=dict(
                width=width, 
                color=edge_color
            ),
            hoverinfo='text',
            text=f"Citations: {weight}<br>From: {source_type} {source_material} ({G.nodes[source]['year']})<br>To: {target_type} {target_material} ({G.nodes[target]['year']})<br>Category: {category_descriptions[category]}",
            showlegend=False
        )
        edge_traces.append(edge_trace)
    
    return edge_traces

def create_node_traces():
    """Create node traces separated by battery type."""
    lib_nodes, sib_nodes = [], []
    
    # Node size scaling parameters
    min_node_size = 6    # Minimum node size
    max_node_size = 30   # Maximum node size
    
    # Find maximum patent count for scaling
    max_patent_count = max([G.nodes[n]['size'] for n in G.nodes()]) if G.nodes() else 1
    
    # Function to scale node size based on patent count
    def scale_node_size(patent_count):
        return min_node_size + (max_node_size - min_node_size) * ((patent_count / max_patent_count) ** 0.5)
    
    # Separate nodes by battery type
    for node in G.nodes():
        x, y = pos[node]
        patent_count = G.nodes[node]['size']
        scaled_size = scale_node_size(patent_count)
        
        # Create node text for hover
        hover_text = (
            f"Material: {G.nodes[node]['material']}<br>"
            f"Battery Type: {G.nodes[node]['battery_type']}<br>"
            f"Year: {G.nodes[node]['year']}<br>"
            f"Patents: {G.nodes[node]['size']}"
        )
        
        # Add to appropriate list based on battery type
        if G.nodes[node]['battery_type'] == 'LIB':
            lib_nodes.append({
                'x': x,
                'y': y,
                'size': scaled_size,
                'color': G.nodes[node]['color'],
                'text': hover_text
            })
        else:  # SIB
            sib_nodes.append({
                'x': x,
                'y': y,
                'size': scaled_size,
                'color': G.nodes[node]['color'],
                'text': hover_text
            })
    
    # Create LIB node trace
    lib_node_trace = go.Scatter(
        x=[node['x'] for node in lib_nodes],
        y=[node['y'] for node in lib_nodes],
        mode='markers',
        marker=dict(
            size=[node['size'] for node in lib_nodes],
            color=[node['color'] for node in lib_nodes],
            line=dict(width=1, color='black')
        ),
        text=[node['text'] for node in lib_nodes],
        hoverinfo='text',
        showlegend=False
    )
    
    # Create SIB node trace
    sib_node_trace = go.Scatter(
        x=[node['x'] for node in sib_nodes],
        y=[node['y'] for node in sib_nodes],
        mode='markers',
        marker=dict(
            size=[node['size'] for node in sib_nodes],
            color=[node['color'] for node in sib_nodes],
            line=dict(width=1, color='black')
        ),
        text=[node['text'] for node in sib_nodes],
        hoverinfo='text',
        showlegend=False
    )
    
    return lib_node_trace, sib_node_trace

def create_legend_entries(include_edge_legends=True):
    """Create legend entries for materials and optionally edge types."""
    legend_entries = []
    
    # LIB material legends
    for material, color in lib_material_colors.items():
        legend_entries.append(go.Scatter(
            x=[None], y=[None],
            mode='markers',
            marker=dict(size=10, color=color),
            name=f'LIB: {material}',
            hoverinfo='none'
        ))
    
    # SIB material legends
    for material, color in sib_material_colors.items():
        legend_entries.append(go.Scatter(
            x=[None], y=[None],
            mode='markers',
            marker=dict(size=10, color=color),
            name=f'SIB: {material}',
            hoverinfo='none'
        ))
    
    # Citation type legends (only if edges are included)
    if include_edge_legends:
        legend_entries.append(go.Scatter(
            x=[None], y=[None],
            mode='lines',
            line=dict(color='rgba(0, 0, 0, 0.8)', width=3),
            name='Within same material',
            hoverinfo='none'
        ))

        legend_entries.append(go.Scatter(
            x=[None], y=[None],
            mode='lines',
            line=dict(color='rgba(180, 180, 180, 0.5)', width=3),
            name='Same structure, same technology',
            hoverinfo='none'
        ))

        legend_entries.append(go.Scatter(
            x=[None], y=[None],
            mode='lines',
            line=dict(color='rgba(120, 120, 120, 0.5)', width=3),
            name='Different structure, same technology',
            hoverinfo='none'
        ))

        legend_entries.append(go.Scatter(
            x=[None], y=[None],
            mode='lines',
            line=dict(color='rgba(255, 140, 0, 0.6)', width=3),
            name='Same structure, different technology',
            hoverinfo='none'
        ))

        legend_entries.append(go.Scatter(
            x=[None], y=[None],
            mode='lines',
            line=dict(color='rgba(220, 20, 60, 0.6)', width=3),
            name='Different structure, different technology',
            hoverinfo='none'
        ))
    
    return legend_entries

def create_figure_layout(title, min_year, max_year, y_ticks, y_tick_labels):
    """Create the common layout configuration for all figures."""
    # Calculate x-axis ticks (years)
    x_ticks = list(range(min_year, max_year + 1, 5))
    
    layout = dict(
        title=dict(
            text=title,
            font=dict(size=24)
        ),
        showlegend=True,
        xaxis=dict(
            title=dict(text="Priority Year", font=dict(size=20)),
            tickmode='array',
            tickvals=x_ticks,
            ticktext=[str(year) for year in x_ticks],
            tickfont=dict(size=18),
            showgrid=True,
            gridwidth=1,
            gridcolor='lightgrey'
        ),
        yaxis=dict(
            tickmode='array',
            tickvals=y_ticks,
            ticktext=y_tick_labels,
            title=dict(text="Cathode Material", font=dict(size=20)),
            tickfont=dict(size=18),
            zeroline=False
        ),
        hovermode='closest',
        hoverlabel=dict(font=dict(size=16)),
        plot_bgcolor='rgba(0,0,0,0)',
        paper_bgcolor='rgba(0,0,0,0)',
        legend=dict(
            yanchor="top", y=0.99, xanchor="left", x=1.01,
            bgcolor="rgba(255, 255, 255, 0.8)", font=dict(size=18)
        ),
        width=1800,
        height=1000,
        margin=dict(l=150, r=250, t=80, b=50)
    )
    
    return layout

def add_divider_line_and_annotation(fig, min_year, max_year):
    """Add divider line and annotation between LIB and SIB materials."""
    divider_y = 0
    fig.add_shape(
        type="line",
        x0=min_year-1,
        y0=divider_y,
        x1=max_year+1,
        y1=divider_y,
        line=dict(
            color="black",
            width=1,
            dash="dash",
        )
    )

    # Add text label for the divider
    fig.add_annotation(
        x=min_year-2,
        y=divider_y,
        xref="x",
        yref="y",
        text="LIB | SIB",
        showarrow=False,
        font=dict(
            size=18,
            color="black"
        )
    )

#==============================================================
# Create common elements for all visualizations
#==============================================================

# Calculate year range
min_year = min([G.nodes[n]['year'] for n in G.nodes()])
max_year = max([G.nodes[n]['year'] for n in G.nodes()])

# Create y-axis ticks and labels
y_ticks = []
y_tick_labels = []

# Y-ticks for LIB materials
for mat in lib_material_order:
    y_pos = material_y_positions.get(('LIB', mat), 0)
    y_ticks.append(y_pos)
    y_tick_labels.append(f"LIB: {mat}")

# Y-ticks for SIB materials
for mat in sib_material_order:
    y_pos = material_y_positions.get(('SIB', mat), 0)
    y_ticks.append(y_pos)
    y_tick_labels.append(f"SIB: {mat}")

# Create node traces (common for all visualizations)
lib_node_trace, sib_node_trace = create_node_traces()

#==============================================================
# VISUALIZATION 1: Nodes Only
#==============================================================

print("\nCreating Visualization 1: Nodes Only...")

# Create legend entries without edge legends
legend_entries_nodes_only = create_legend_entries(include_edge_legends=False)

# Create the figure with only nodes
fig_nodes_only = go.Figure(data=[lib_node_trace, sib_node_trace] + legend_entries_nodes_only)

# Update layout
layout_nodes_only = create_figure_layout(
    "LIB and SIB Battery Positive Electrode Materials Patent Network - Nodes Only",
    min_year, max_year, y_ticks, y_tick_labels
)
fig_nodes_only.update_layout(layout_nodes_only)

# Add divider line and annotation
add_divider_line_and_annotation(fig_nodes_only, min_year, max_year)

# Save the visualization
output_file_nodes_only = "LIB_SIB_Cathode_Material_Patent_Network_NODES_ONLY.html"
fig_nodes_only.write_html(output_file_nodes_only)
print(f"Nodes-only visualization saved to {output_file_nodes_only}")

#==============================================================
# VISUALIZATION 2: Nodes + Intra-Technology Edges
#==============================================================

print("\nCreating Visualization 2: Nodes + Intra-Technology Edges...")

# Create edge traces for intra-technology edges
edge_traces_intra = create_edge_traces(intra_tech_edges)

# Create legend entries with edge legends
legend_entries_with_edges = create_legend_entries(include_edge_legends=True)

# Create the figure with nodes and intra-tech edges
fig_intra_tech = go.Figure(data=edge_traces_intra + [lib_node_trace, sib_node_trace] + legend_entries_with_edges)

# Update layout
layout_intra_tech = create_figure_layout(
    "LIB and SIB Battery Positive Electrode Materials Patent Network - Intra-Technology Citations",
    min_year, max_year, y_ticks, y_tick_labels
)
fig_intra_tech.update_layout(layout_intra_tech)

# Add divider line and annotation
add_divider_line_and_annotation(fig_intra_tech, min_year, max_year)

# Save the visualization
output_file_intra_tech = "LIB_SIB_Cathode_Material_Patent_Network_INTRA_TECH.html"
fig_intra_tech.write_html(output_file_intra_tech)
print(f"Intra-technology visualization saved to {output_file_intra_tech}")

#==============================================================
# VISUALIZATION 3: Nodes + Inter-Technology Edges
#==============================================================

print("\nCreating Visualization 3: Nodes + Inter-Technology Edges...")

# Create edge traces for inter-technology edges
edge_traces_inter = create_edge_traces(inter_tech_edges)

# Create the figure with nodes and inter-tech edges
fig_inter_tech = go.Figure(data=edge_traces_inter + [lib_node_trace, sib_node_trace] + legend_entries_with_edges)

# Update layout
layout_inter_tech = create_figure_layout(
    "LIB and SIB Battery Positive Electrode Materials Patent Network - Inter-Technology Citations",
    min_year, max_year, y_ticks, y_tick_labels
)
fig_inter_tech.update_layout(layout_inter_tech)

# Add divider line and annotation
add_divider_line_and_annotation(fig_inter_tech, min_year, max_year)

# Save the visualization
output_file_inter_tech = "LIB_SIB_Cathode_Material_Patent_Network_INTER_TECH.html"
fig_inter_tech.write_html(output_file_inter_tech)
print(f"Inter-technology visualization saved to {output_file_inter_tech}")

#==============================================================
# Print statistics for all visualizations
#==============================================================

print("\n" + "="*60)
print("NETWORK STATISTICS SUMMARY")
print("="*60)

print(f"Total material-year nodes: {G.number_of_nodes()}")
print(f"LIB material-year nodes: {len([n for n in G.nodes() if G.nodes[n]['battery_type'] == 'LIB'])}")
print(f"SIB material-year nodes: {len([n for n in G.nodes() if G.nodes[n]['battery_type'] == 'SIB'])}")

print(f"\nTotal citations (all edges): {len(edge_weights)}")
print(f"Intra-technology citations: {len(intra_tech_edges)}")
print(f"Inter-technology citations: {len(inter_tech_edges)}")

# Count citations within same materials for intra-tech
intra_same_material_citations = 0
for (source, target) in intra_tech_edges.keys():
    source_material = G.nodes[source]['material']
    target_material = G.nodes[target]['material']
    if source_material == target_material:
        intra_same_material_citations += intra_tech_edges[(source, target)]

print(f"Intra-tech citations within same material: {intra_same_material_citations}")

# Count LIB-to-SIB vs SIB-to-LIB citations
lib_to_sib = sum(1 for u, v in inter_tech_edges.keys() 
                if G.nodes[u]['battery_type'] == 'LIB' and G.nodes[v]['battery_type'] == 'SIB')
sib_to_lib = sum(1 for u, v in inter_tech_edges.keys() 
                if G.nodes[u]['battery_type'] == 'SIB' and G.nodes[v]['battery_type'] == 'LIB')

print(f"\nInter-technology citation direction:")
print(f"LIB materials citing SIB materials: {lib_to_sib}")
print(f"SIB materials citing LIB materials: {sib_to_lib}")

# Count SIB-to-SIB citations by material
sib_material_citations = {mat: 0 for mat in sib_cathode_materials}
for (source, target) in intra_tech_edges.keys():
    if (G.nodes[source]['battery_type'] == 'SIB' and 
        G.nodes[target]['battery_type'] == 'SIB'):
        source_material = G.nodes[source]['material']
        target_material = G.nodes[target]['material']
        if source_material == target_material:
            sib_material_citations[source_material] += intra_tech_edges[(source, target)]

print(f"\nSIB-to-SIB citations within same material:")
for mat, count in sib_material_citations.items():
    if count > 0:
        print(f"  {mat}: {count}")

# Count LIB-to-LIB citations by material
lib_material_citations = {mat: 0 for mat in lib_cathode_materials}
for (source, target) in intra_tech_edges.keys():
    if (G.nodes[source]['battery_type'] == 'LIB' and 
        G.nodes[target]['battery_type'] == 'LIB'):
        source_material = G.nodes[source]['material']
        target_material = G.nodes[target]['material']
        if source_material == target_material:
            lib_material_citations[source_material] += intra_tech_edges[(source, target)]

print(f"\nLIB-to-LIB citations within same material:")
for mat, count in lib_material_citations.items():
    if count > 0:
        print(f"  {mat}: {count}")

print("\n" + "="*60)
print("THREE VISUALIZATIONS CREATED SUCCESSFULLY!")
print("="*60)
print(f"1. {output_file_nodes_only}")
print(f"2. {output_file_intra_tech}")
print(f"3. {output_file_inter_tech}")
print("="*60)


#%%


#==========================================================================================================================
# Building a citation network based on LIB and SIB datasets, focusing only on negative electrode (anode) patents
#==========================================================================================================================

import pandas as pd
import networkx as nx
import plotly.graph_objects as go
import numpy as np
from datetime import datetime

# File paths for both datasets
lib_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_full_data_classified_onlyLIB_27052025.xlsx"
sib_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_full_data_classified_final_onlySIB_26052025.xlsx"
sheet_name = "Sheet1"

# Load both Excel files
LIB_patents = pd.read_excel(lib_file_path, sheet_name=sheet_name)
SIB_patents = pd.read_excel(sib_file_path, sheet_name=sheet_name)

# Print basic info about both datasets
print(f"LIB dataset shape: {LIB_patents.shape}")
print(f"SIB dataset shape: {SIB_patents.shape}")

# Add a 'battery_type' column to identify the source in the combined dataset
LIB_patents['battery_type'] = 'LIB'
SIB_patents['battery_type'] = 'SIB'

#==============================================================
# Define material sets for both battery types
#==============================================================

# Define LIB anode materials
lib_anode_materials = {"Silicon/Carbon", "Graphite", "LTO"}

# Define SIB anode materials
sib_anode_materials = {"Hard Carbon"}

#==============================================================
# Function to extract materials from material strings
#==============================================================

def extract_materials(material_str, material_set):
    """Extract materials that match the given material set from a comma-separated string."""
    materials = set(material_str.split(", ")) if isinstance(material_str, str) else set()
    extracted = list(materials.intersection(material_set))
    return extracted if extracted else ["none"]

#==============================================================
# Process LIB dataset
#==============================================================

# Extract priority year from earliest_priority_date for LIB
LIB_patents["earliest_priority_date"] = pd.to_datetime(LIB_patents["Earliest Priority Date"])
LIB_patents["priority_year"] = LIB_patents["earliest_priority_date"].dt.year
LIB_patents = LIB_patents.sort_values(by=["priority_year"], ascending=True)

# Create new column for Anode Materials for LIB
LIB_patents["Anode Material"] = LIB_patents["Material-Level"].apply(lambda x: extract_materials(x, lib_anode_materials))

# Update Component-Level based on Material-Level for LIB
for index, row in LIB_patents.iterrows():
    if row["Material-Level"] != "none" and row["Component-Level"] == "none":
        materials = set(row["Material-Level"].split(", ")) if isinstance(row["Material-Level"], str) else set()
        anode_match = materials & lib_anode_materials
        
        if anode_match:
            LIB_patents.at[index, "Component-Level"] = "Negative Electrode"

# Update Component-Level for more specific components in LIB
for index, row in LIB_patents.iterrows():
    if row["Material-Level"] != "none" and row["Component-Level"] != "Cell":
        materials = set(row["Material-Level"].split(", ")) if isinstance(row["Material-Level"], str) else set()
        anode_match = materials & lib_anode_materials
        
        existing_components = set(row["Component-Level"].split(", ")) if isinstance(row["Component-Level"], str) and row["Component-Level"] != "none" else set()
        
        if anode_match:
            existing_components.add("Negative Electrode")
            
        LIB_patents.at[index, "Component-Level"] = ", ".join(existing_components) if existing_components else "none"

# Create family_citing_lens_id_count for LIB dataset by counting citations
LIB_patents["family_citing_lens_id_count"] = LIB_patents["family_citing_lens_id"].apply(
    lambda x: len(x.split(", ")) if isinstance(x, str) and x else 0
)

# Sort component values alphabetically for LIB
for index, row in LIB_patents.iterrows():
    if row["Component-Level"] != "none":
        sorted_components = ", ".join(sorted(row["Component-Level"].split(", ")))
        LIB_patents.at[index, "Component-Level"] = sorted_components

#==============================================================
# Process SIB dataset
#==============================================================

# Extract priority year from earliest_priority_date for SIB
SIB_patents["earliest_priority_date"] = pd.to_datetime(SIB_patents["Earliest Priority Date"])
SIB_patents["priority_year"] = SIB_patents["earliest_priority_date"].dt.year
SIB_patents = SIB_patents.sort_values(by=["priority_year"], ascending=True)

# Create new column for Anode Materials for SIB
SIB_patents["Anode Material"] = SIB_patents["Material-Level"].apply(lambda x: extract_materials(x, sib_anode_materials))

# Update Component-Level based on Material-Level for SIB
for index, row in SIB_patents.iterrows():
    if row["Material-Level"] != "none" and row["Component-Level"] == "none":
        materials = set(row["Material-Level"].split(", ")) if isinstance(row["Material-Level"], str) else set()
        anode_match = materials & sib_anode_materials
        
        if anode_match:
            SIB_patents.at[index, "Component-Level"] = "Negative Electrode"

# Update Component-Level for more specific components in SIB
for index, row in SIB_patents.iterrows():
    if row["Material-Level"] != "none" and row["Component-Level"] != "Cell":
        materials = set(row["Material-Level"].split(", ")) if isinstance(row["Material-Level"], str) else set()
        anode_match = materials & sib_anode_materials
        
        existing_components = set(row["Component-Level"].split(", ")) if isinstance(row["Component-Level"], str) and row["Component-Level"] != "none" else set()
        
        if anode_match:
            existing_components.add("Negative Electrode")
            
        SIB_patents.at[index, "Component-Level"] = ", ".join(existing_components) if existing_components else "none"

# Create family_citing_lens_id_count for SIB dataset by counting citations
SIB_patents["family_citing_lens_id_count"] = SIB_patents["family_citing_lens_id"].apply(
    lambda x: len(x.split(", ")) if isinstance(x, str) and x else 0
)

# Sort component values alphabetically for SIB
for index, row in SIB_patents.iterrows():
    if row["Component-Level"] != "none":
        sorted_components = ", ".join(sorted(row["Component-Level"].split(", ")))
        SIB_patents.at[index, "Component-Level"] = sorted_components

#==============================================================
# Filter patents to retain only negative electrode (anode) patents
#==============================================================

# Function to check if any anode material is present in Material-Level
def contains_anode_material(material_str, anode_materials):
    """Check if the material string contains any anode material from the given set."""
    if not isinstance(material_str, str) or material_str == "none":
        return False
    
    materials = set(material_str.split(", "))
    return bool(materials.intersection(anode_materials))

# Filter LIB dataset for patents containing LIB anode materials
lib_negative_electrode = LIB_patents[
    LIB_patents["Material-Level"].apply(
        lambda x: contains_anode_material(x, lib_anode_materials)
    )
]

# Filter LIB patents to only include those with at least one citation and anode materials
lib_anode = lib_negative_electrode[
    (lib_negative_electrode["family_citing_lens_id_count"] > 0) & 
    (lib_negative_electrode["Anode Material"].apply(lambda x: x != ["none"]))
]

# Filter SIB dataset for patents containing SIB anode materials
sib_negative_electrode = SIB_patents[
    SIB_patents["Material-Level"].apply(
        lambda x: contains_anode_material(x, sib_anode_materials)
    )
]

# Filter SIB patents to only include those with at least one citation and anode materials
sib_anode = sib_negative_electrode[
    (sib_negative_electrode["family_citing_lens_id_count"] > 0) & 
    (sib_negative_electrode["Anode Material"].apply(lambda x: x != ["none"]))
]

# Print info about filtered datasets
print(f"LIB patents with anode materials: {len(lib_negative_electrode)}")
print(f"LIB anode patents with citations: {len(lib_anode)}")
print(f"SIB patents with anode materials: {len(sib_negative_electrode)}")
print(f"SIB anode patents with citations: {len(sib_anode)}")

#==============================================================
# Function to sort materials in material-level strings
#==============================================================

def sort_material_levels(material_level):
    """Sort materials in a comma-separated string."""
    if not isinstance(material_level, str):
        return material_level
    materials = material_level.split(", ")
    sorted_materials = sorted(materials)
    return ", ".join(sorted_materials)

# Sort materials within Material-Level
lib_anode["Material-Level"] = lib_anode["Material-Level"].apply(sort_material_levels)
sib_anode["Material-Level"] = sib_anode["Material-Level"].apply(sort_material_levels)

#==============================================================
# Create aggregated data for Material-Year nodes
#==============================================================

# Create a directed graph
G = nx.DiGraph()

# Create lists for aggregated data
aggregated_data = []

# Process LIB anode patents with consistent node numbering
for idx, (_, row) in enumerate(lib_anode.iterrows()):
    material_list = row["Anode Material"]
    for material in material_list:
        if material != "none":
            aggregated_data.append({
                'material': material,
                'battery_type': 'LIB',
                'priority_date': row['earliest_priority_date'],
                'year': row['priority_year'],
                'node_num': idx,  # Consistent numbering starting from 0
                'family_id': row['Simple Family ID'] if 'Simple Family ID' in row else None,
                'patent_id': row['Lens ID'] if 'Lens ID' in row else None,
                'family_citing_lens_id': row['family_citing_lens_id']
            })

# Process SIB anode patents with consistent node numbering
for idx, (_, row) in enumerate(sib_anode.iterrows()):
    material_list = row["Anode Material"]
    for material in material_list:
        if material != "none":
            aggregated_data.append({
                'material': material,
                'battery_type': 'SIB',
                'priority_date': row['earliest_priority_date'],
                'year': row['priority_year'],
                'node_num': idx + len(lib_anode),  # Continue numbering after LIB patents
                'family_id': row['Simple Family ID'] if 'Simple Family ID' in row else None,
                'patent_id': row['Lens ID'] if 'Lens ID' in row else None,
                'family_citing_lens_id': row['family_citing_lens_id']
            })

# Create a DataFrame from the aggregated data
agg_df = pd.DataFrame(aggregated_data)

# Group by year, battery type, and material to count patents
node_data = agg_df.groupby(['year', 'battery_type', 'material']).size().reset_index(name='patent_count')

#==============================================================
# Create node and edge mappings for the network
#==============================================================

# Create a mapping of (year, battery_type, material) to node ID
node_mapping = {}
for i, row in node_data.iterrows():
    node_mapping[(row['year'], row['battery_type'], row['material'])] = i

# Store the reverse mapping for later access
id_to_node = {i: (row['year'], row['battery_type'], row['material']) for i, row in node_data.iterrows()}

# Create a mapping of patents to their year-material nodes
patent_to_nodes = {}
for _, row in agg_df.iterrows():
    node_key = (row['year'], row['battery_type'], row['material'])
    node_id = node_mapping.get(node_key)
    
    if node_id is not None:
        if row['node_num'] not in patent_to_nodes:
            patent_to_nodes[row['node_num']] = []
        patent_to_nodes[row['node_num']].append(node_id)

#==============================================================
# Create patent citation networks
#==============================================================

# Create consistent combined dataset maintaining node numbering
lib_anode_copy = lib_anode.copy().reset_index(drop=True)
sib_anode_copy = sib_anode.copy().reset_index(drop=True)
sib_anode_copy.index = sib_anode_copy.index + len(lib_anode)

combined_patents = pd.concat([lib_anode_copy, sib_anode_copy])

# Create a dictionary to map lens IDs to node numbers
lens_id_to_node = {}
for i, row in combined_patents.iterrows():
    # Extract lens IDs from Simple Family Members
    if isinstance(row['Simple Family Members'], str):
        for lens_id in row['Simple Family Members'].split(';;'):
            lens_id = lens_id.strip()
            if lens_id:  # Ensure non-empty string
                lens_id_to_node[lens_id] = i

# Extract citation links
edge_data = []
for i, row in combined_patents.iterrows():
    if isinstance(row['family_citing_lens_id'], str) and row['family_citing_lens_id']:
        for citing_id in row['family_citing_lens_id'].split(', '):
            citing_id = citing_id.strip()
            if citing_id and citing_id in lens_id_to_node:
                cited_node = i
                citing_node = lens_id_to_node[citing_id]
                
                # Get material-year nodes for both
                if cited_node in patent_to_nodes and citing_node in patent_to_nodes:
                    for source_id in patent_to_nodes[citing_node]:  # From citing patent
                        for target_id in patent_to_nodes[cited_node]:  # To cited patent
                            if source_id != target_id:  # Avoid self-loops
                                edge_data.append((source_id, target_id))

# Count edge frequencies
edge_weights = {}
for edge in edge_data:
    if edge in edge_weights:
        edge_weights[edge] += 1
    else:
        edge_weights[edge] = 1

# Debug output to verify corrections
print(f"Total LIB anode patents: {len(lib_anode)}")
print(f"Total SIB anode patents: {len(sib_anode)}")
print(f"Combined patents shape: {combined_patents.shape}")
print(f"Patents mapped to nodes: {len(patent_to_nodes)}")
print(f"Lens IDs mapped: {len(lens_id_to_node)}")
print(f"Total edges found: {len(edge_data)}")
print(f"Unique edges after counting: {len(edge_weights)}")


#==============================================================
# Create the network visualization
#==============================================================

# Define color mappings for anode materials by battery type
# LIB anode colors - orange for LTO, grays for carbon
lib_material_colors = {
    'LTO': '#FFA500',        # Orange
    'Silicon/Carbon': '#666666',  # Dark gray
    'Graphite': '#333333',        # Darker gray
}

# SIB anode colors - gray for Hard Carbon
sib_material_colors = {
    'Hard Carbon': '#999999',  # Gray
}


# Define structural classes for materials
structure_classes = {
    # LIB materials
    'Graphite': 'carbon',           # Kohlenstoff-basiert
    'Silicon/Carbon': 'carbon',     # Kohlenstoff-basiert (Si-dotiert)
    'LTO': 'oxide',                # Oxid-basiert
    # SIB materials
    'Hard Carbon': 'carbon'        # Kohlenstoff-basiert
}

# Define edge colors with transparency
edge_colors = {
    'intra_material': 'rgba(0, 0, 0, 0.8)',                    # Schwarz
    'same_structure_intra_tech': 'rgba(180, 180, 180, 0.5)',   # Etwas dunkleres helles Grau
    'diff_structure_intra_tech': 'rgba(120, 120, 120, 0.5)',   # Mittleres Grau
    'same_structure_inter_tech': 'rgba(255, 140, 0, 0.6)',     # Orange
    'diff_structure_inter_tech': 'rgba(220, 20, 60, 0.6)'      # Crimson
}

# Function to determine edge category
def get_edge_category(source_material, target_material, source_type, target_type):
    """Determine the category of an edge based on materials and battery types."""
    # Same material
    if source_material == target_material:
        return 'intra_material'
    
    # Get structural classes
    source_structure = structure_classes.get(source_material, 'unknown')
    target_structure = structure_classes.get(target_material, 'unknown')
    
    # Same technology (LIB or SIB)
    if source_type == target_type:
        if source_structure == target_structure:
            return 'same_structure_intra_tech'
        else:
            return 'diff_structure_intra_tech'
    # Different technology (LIB <-> SIB)
    else:
        if source_structure == target_structure:
            return 'same_structure_inter_tech'
        else:
            return 'diff_structure_inter_tech'

# Add nodes to the graph
for i, row in node_data.iterrows():
    year = row['year']
    battery_type = row['battery_type']
    material = row['material']
    patent_count = row['patent_count']
    
    # Select color based on battery type and material
    if battery_type == 'LIB':
        color = lib_material_colors.get(material, 'grey')
    else:  # SIB
        color = sib_material_colors.get(material, 'grey')
    
    # Add node with attributes
    G.add_node(i,
               year=year,
               battery_type=battery_type,
               material=material,
               size=patent_count,
               color=color)

# Debug: Check for missing nodes
print(f"Graph has {G.number_of_nodes()} nodes: {set(G.nodes())}")
all_edge_nodes = set([node for edge in edge_weights.keys() for node in edge])
print(f"Edge_weights contains node IDs: {all_edge_nodes}")
missing_nodes = all_edge_nodes - set(G.nodes())
if missing_nodes:
    print(f"Missing nodes in edge_weights: {missing_nodes}")

# Filter edge_weights for only existing nodes
valid_edge_weights = {}
for (source, target), weight in edge_weights.items():
    if source in G.nodes() and target in G.nodes():
        valid_edge_weights[(source, target)] = weight

print(f"Original edges: {len(edge_weights)}, Valid edges: {len(valid_edge_weights)}")

# Replace edge_weights with valid_edge_weights
edge_weights = valid_edge_weights

# Add edges with weights - now all are valid
for (source, target), weight in edge_weights.items():
    G.add_edge(source, target, weight=weight)

# Count citations between same materials
same_material_citations = 0
for (source, target) in edge_weights.keys():
    source_material = G.nodes[source]['material']
    target_material = G.nodes[target]['material']
    if source_material == target_material:
        same_material_citations += edge_weights[(source, target)]
print(f"Citations within the same material: {same_material_citations}")

# Count SIB-to-SIB citations by material
sib_material_citations = {mat: 0 for mat in sib_anode_materials}
for (source, target) in edge_weights.keys():
    if (G.nodes[source]['battery_type'] == 'SIB' and 
        G.nodes[target]['battery_type'] == 'SIB'):
        source_material = G.nodes[source]['material']
        target_material = G.nodes[target]['material']
        if source_material == target_material:
            sib_material_citations[source_material] += edge_weights[(source, target)]

print("SIB-to-SIB citations within the same material:")
for mat, count in sib_material_citations.items():
    print(f"  {mat}: {count}")

# Count LIB-to-LIB citations by material
lib_material_citations = {mat: 0 for mat in lib_anode_materials}
for (source, target) in edge_weights.keys():
    if (G.nodes[source]['battery_type'] == 'LIB' and 
        G.nodes[target]['battery_type'] == 'LIB'):
        source_material = G.nodes[source]['material']
        target_material = G.nodes[target]['material']
        if source_material == target_material:
            lib_material_citations[source_material] += edge_weights[(source, target)]

print("LIB-to-LIB citations within the same material:")
for mat, count in lib_material_citations.items():
    print(f"  {mat}: {count}")

#==============================================================
# Calculate node positions
#==============================================================

# Define material orders for both battery types
lib_material_order = ['Graphite', 'Silicon/Carbon', 'LTO']
sib_material_order = ['Hard Carbon']

# Calculate y-positions for each material
# LIB materials at the top, SIB materials at the bottom
y_spacing = 50  # Increased spacing between materials
material_y_positions = {}

# Position LIB materials
for idx, mat in enumerate(lib_material_order):
    material_y_positions[('LIB', mat)] = (len(lib_material_order) - idx) * y_spacing

# Position SIB materials
for idx, mat in enumerate(sib_material_order):
    material_y_positions[('SIB', mat)] = -(idx + 1) * y_spacing

# Calculate positions for each node
pos = {}
for node_id, (year, battery_type, material) in id_to_node.items():
    # x-position based on year
    x = year
    # y-position based on material and battery type
    y = material_y_positions.get((battery_type, material), 0)
    pos[node_id] = (x, y)

#==============================================================
# Create the Plotly visualization
#==============================================================

# Create edge traces
edge_traces = []
min_width = 0.5   # Minimum edge width
max_width = 5     # Maximum edge width (reduced from 7 to prevent visual overwhelming)

# Find maximum edge weight for scaling
max_weight = max([G.edges[edge]['weight'] for edge in G.edges()]) if G.edges() else 1

# Function to scale edge width based on weight - using square root for better visual distribution
def scale_edge_width(weight):
    # Using sqrt for more balanced visual representation of weights
    return min_width + (max_width - min_width) * ((weight / max_weight) ** 0.5)

# Create edge traces with new categorization
for edge in G.edges():
    source, target = edge
    x0, y0 = pos[source]
    x1, y1 = pos[target]
    weight = G.edges[source, target]['weight']
    width = scale_edge_width(weight)
    
    # Get materials and battery types
    source_material = G.nodes[source]['material']
    target_material = G.nodes[target]['material']
    source_type = G.nodes[source]['battery_type']
    target_type = G.nodes[target]['battery_type']
    
    # Determine edge category and color
    category = get_edge_category(source_material, target_material, source_type, target_type)
    edge_color = edge_colors[category]
    
    # Create descriptive text for hover
    category_descriptions = {
        'intra_material': 'Within same material',
        'same_structure_intra_tech': 'Same structure, same technology',
        'diff_structure_intra_tech': 'Different structure, same technology',
        'same_structure_inter_tech': 'Same structure, different technology',
        'diff_structure_inter_tech': 'Different structure, different technology'
    }
    
    # Create edge trace
    edge_trace = go.Scatter(
        x=[x0, x1, None], 
        y=[y0, y1, None],
        mode='lines',
        line=dict(
            width=width, 
            color=edge_color
        ),
        hoverinfo='text',
        text=f"Citations: {weight}<br>From: {source_type} {source_material} ({G.nodes[source]['year']})<br>To: {target_type} {target_material} ({G.nodes[target]['year']})<br>Category: {category_descriptions[category]}",
        showlegend=False
    )
    edge_traces.append(edge_trace)

# Create node traces
node_x, node_y = [], []
node_size, node_color, node_text = [], [], []
lib_nodes, sib_nodes = [], []

# Node size scaling parameters
min_node_size = 6    # Minimum node size
max_node_size = 25   # Maximum node size
size_scale_factor = 1.5  # Scale factor

# Find maximum patent count for scaling
max_patent_count = max([G.nodes[n]['size'] for n in G.nodes()]) if G.nodes() else 1

# Function to scale node size based on patent count
def scale_node_size(patent_count):
    # Using sqrt for better visual scaling
    return min_node_size + (max_node_size - min_node_size) * ((patent_count / max_patent_count) ** 0.5)

# Separate nodes by battery type
for node in G.nodes():
    x, y = pos[node]
    patent_count = G.nodes[node]['size']
    scaled_size = scale_node_size(patent_count)
    
    # Create node text for hover
    hover_text = (
        f"Material: {G.nodes[node]['material']}<br>"
        f"Battery Type: {G.nodes[node]['battery_type']}<br>"
        f"Year: {G.nodes[node]['year']}<br>"
        f"Patents: {G.nodes[node]['size']}"
    )
    
    # Add to appropriate list based on battery type
    if G.nodes[node]['battery_type'] == 'LIB':
        lib_nodes.append({
            'x': x,
            'y': y,
            'size': scaled_size,
            'color': G.nodes[node]['color'],
            'text': hover_text
        })
    else:  # SIB
        sib_nodes.append({
            'x': x,
            'y': y,
            'size': scaled_size,
            'color': G.nodes[node]['color'],
            'text': hover_text
        })

# Create LIB node trace
lib_node_trace = go.Scatter(
    x=[node['x'] for node in lib_nodes],
    y=[node['y'] for node in lib_nodes],
    mode='markers',
    marker=dict(
        size=[node['size'] for node in lib_nodes],
        color=[node['color'] for node in lib_nodes],
        line=dict(width=1, color='black')
    ),
    text=[node['text'] for node in lib_nodes],
    hoverinfo='text',
    showlegend=False  # Entfernt aus Legende
)

# Create SIB node trace
sib_node_trace = go.Scatter(
    x=[node['x'] for node in sib_nodes],
    y=[node['y'] for node in sib_nodes],
    mode='markers',
    marker=dict(
        size=[node['size'] for node in sib_nodes],
        color=[node['color'] for node in sib_nodes],
        line=dict(width=1, color='black')
    ),
    text=[node['text'] for node in sib_nodes],
    hoverinfo='text',
    showlegend=False  # Entfernt aus Legende
)

# Create legend entries
legend_entries = []

# LIB material legends
for material, color in lib_material_colors.items():
    legend_entries.append(go.Scatter(
        x=[None], y=[None],
        mode='markers',
        marker=dict(size=10, color=color),
        name=f'LIB: {material}',
        hoverinfo='none'
    ))

# SIB material legends
for material, color in sib_material_colors.items():
    legend_entries.append(go.Scatter(
        x=[None], y=[None],
        mode='markers',
        marker=dict(size=10, color=color),
        name=f'SIB: {material}',
        hoverinfo='none'
    ))

# Citation type legends
legend_entries.append(go.Scatter(
    x=[None], y=[None],
    mode='lines',
    line=dict(color='rgba(0, 0, 0, 0.8)', width=3),
    name='Within same material',
    hoverinfo='none'
))

legend_entries.append(go.Scatter(
    x=[None], y=[None],
    mode='lines',
    line=dict(color='rgba(180, 180, 180, 0.5)', width=3),
    name='Same structure, same technology',
    hoverinfo='none'
))

legend_entries.append(go.Scatter(
    x=[None], y=[None],
    mode='lines',
    line=dict(color='rgba(120, 120, 120, 0.5)', width=3),
    name='Different structure, same technology',
    hoverinfo='none'
))

legend_entries.append(go.Scatter(
    x=[None], y=[None],
    mode='lines',
    line=dict(color='rgba(255, 140, 0, 0.6)', width=3),
    name='Same structure, different technology',
    hoverinfo='none'
))

legend_entries.append(go.Scatter(
    x=[None], y=[None],
    mode='lines',
    line=dict(color='rgba(220, 20, 60, 0.6)', width=3),
    name='Different structure, different technology',
    hoverinfo='none'
))

# Create the figure
fig = go.Figure(data=edge_traces + [lib_node_trace, sib_node_trace] + legend_entries)

# Calculate x-axis ticks (years)
min_year = min([G.nodes[n]['year'] for n in G.nodes()]) if G.nodes() else 2000
max_year = max([G.nodes[n]['year'] for n in G.nodes()]) if G.nodes() else 2025
x_ticks = list(range(min_year, max_year + 1, 5))

# Create y-axis ticks and labels
y_ticks = []
y_tick_labels = []

# Add divider line between LIB and SIB materials
divider_y = 0
fig.add_shape(
    type="line",
    x0=min_year-1,
    y0=divider_y,
    x1=max_year+1,
    y1=divider_y,
    line=dict(
        color="black",
        width=1,
        dash="dash",
    )
)

# Add text label for the divider
fig.add_annotation(
    x=min_year-2,
    y=divider_y,
    xref="x",
    yref="y",
    text="LIB | SIB",
    showarrow=False,
    font=dict(
        size=18,  # Divider-Text-Schriftgröße erhöht
        color="black"
    )
)

# Y-ticks for LIB materials
for mat in lib_material_order:
    y_pos = material_y_positions.get(('LIB', mat), 0)
    y_ticks.append(y_pos)
    y_tick_labels.append(f"LIB: {mat}")

# Y-ticks for SIB materials
for mat in sib_material_order:
    y_pos = material_y_positions.get(('SIB', mat), 0)
    y_ticks.append(y_pos)
    y_tick_labels.append(f"SIB: {mat}")

# Update layout
fig.update_layout(
    title=dict(
        text="Lithium-Ion and Sodium-Ion Battery Negative Electrode Materials Patent Network",
        font=dict(size=24)  # Titel-Schriftgröße
    ),
    showlegend=True,
    xaxis=dict(
        title=dict(text="Priority Year", font=dict(size=20)),  # Achsentitel-Schriftgröße
        tickmode='array',
        tickvals=x_ticks,
        ticktext=[str(year) for year in x_ticks],
        tickfont=dict(size=18),  # Achsen-Tick-Schriftgröße
        showgrid=True,
        gridwidth=1,
        gridcolor='lightgrey'
    ),
    yaxis=dict(
        tickmode='array',
        tickvals=y_ticks,
        ticktext=y_tick_labels,
        title=dict(text="Negative Electrode Material", font=dict(size=20)),  # Achsentitel-Schriftgröße
        tickfont=dict(size=18),  # Achsen-Tick-Schriftgröße
        zeroline=False
    ),
    hovermode='closest',
    hoverlabel=dict(font=dict(size=16)),  # Hover-Text-Schriftgröße erhöht
    plot_bgcolor='rgba(0,0,0,0)',
    paper_bgcolor='rgba(0,0,0,0)',
    legend=dict(
        yanchor="top", y=0.99, xanchor="left", x=1.01,
        bgcolor="rgba(255, 255, 255, 0.8)", font=dict(size=20)  # Legenden-Schriftgröße
    ),
    width=1800,
    height=1000,
    margin=dict(l=150, r=250, t=80, b=50)
)

# Save the visualization
output_file = "LIB_SIB_Negative_Electrode_Material_Patent_Network.html"
fig.write_html(output_file)
print(f"Visualization saved to {output_file}")

# Print network statistics
print("\nNetwork Statistics:")
print(f"Total material-year nodes: {G.number_of_nodes()}")
print(f"LIB material-year nodes: {len([n for n in G.nodes() if G.nodes[n]['battery_type'] == 'LIB'])}")
print(f"SIB material-year nodes: {len([n for n in G.nodes() if G.nodes[n]['battery_type'] == 'SIB'])}")
print(f"Total citations: {G.number_of_edges()}")

# Count cross-battery citations
cross_citations = sum(1 for u, v in G.edges() 
                     if G.nodes[u]['battery_type'] != G.nodes[v]['battery_type'])
print(f"Cross-battery citations: {cross_citations}")

# Calculate citation directionality
lib_to_sib = sum(1 for u, v in G.edges() 
                if G.nodes[u]['battery_type'] == 'LIB' and G.nodes[v]['battery_type'] == 'SIB')
sib_to_lib = sum(1 for u, v in G.edges() 
                if G.nodes[u]['battery_type'] == 'SIB' and G.nodes[v]['battery_type'] == 'LIB')

print(f"LIB materials citing SIB materials: {lib_to_sib}")
print(f"SIB materials citing LIB materials: {sib_to_lib}")

print("\nAnalysis complete!")


#%%


#==========================================================================================================================
# Building a combined citation network showing cross-material flows between cathode and anode patents
#==========================================================================================================================

import pandas as pd
import networkx as nx
import plotly.graph_objects as go
import numpy as np
from datetime import datetime

# File paths for both datasets
lib_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_full_data_classified_onlyLIB_27052025.xlsx"
sib_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_full_data_classified_final_onlySIB_26052025.xlsx"
sheet_name = "Sheet1"

# Load both Excel files
LIB_patents = pd.read_excel(lib_file_path, sheet_name=sheet_name)
SIB_patents = pd.read_excel(sib_file_path, sheet_name=sheet_name)

# Print basic info about both datasets
print(f"LIB dataset shape: {LIB_patents.shape}")
print(f"SIB dataset shape: {SIB_patents.shape}")

# Add a 'battery_type' column to identify the source in the combined dataset
LIB_patents['battery_type'] = 'LIB'
SIB_patents['battery_type'] = 'SIB'

#==============================================================
# Define material sets for both battery types
#==============================================================

# Define LIB cathode materials
lib_cathode_materials = {"LCO", "LMO", "LFP", "NMC", "NCA"}

# Define SIB cathode materials
sib_cathode_materials = {"Fe-PBA", "Mn-PBA", "NVPF", "NFPP/NFP", "NFM", "CFM", "NMO"}

# Define LIB anode materials
lib_anode_materials = {"Silicon/Carbon", "Graphite", "LTO"}

# Define SIB anode materials
sib_anode_materials = {"Hard Carbon"}

# Combine all materials
all_cathode_materials = lib_cathode_materials | sib_cathode_materials
all_anode_materials = lib_anode_materials | sib_anode_materials
all_materials = all_cathode_materials | all_anode_materials

#==============================================================
# Function to extract materials from material strings
#==============================================================

def extract_materials(material_str, material_set):
    """Extract materials that match the given material set from a comma-separated string."""
    materials = set(material_str.split(", ")) if isinstance(material_str, str) else set()
    extracted = list(materials.intersection(material_set))
    return extracted if extracted else ["none"]

def contains_material(material_str, material_set):
    """Check if the material string contains any material from the given set."""
    if not isinstance(material_str, str) or material_str == "none":
        return False
    materials = set(material_str.split(", "))
    return bool(materials.intersection(material_set))

#==============================================================
# Process both datasets for cathode and anode materials
#==============================================================

def process_dataset(patents_df, battery_type, cathode_materials, anode_materials):
    """Process dataset to extract both cathode and anode materials."""
    
    # Extract priority year
    patents_df["earliest_priority_date"] = pd.to_datetime(patents_df["Earliest Priority Date"])
    patents_df["priority_year"] = patents_df["earliest_priority_date"].dt.year
    patents_df = patents_df.sort_values(by=["priority_year"], ascending=True)
    
    # Create material columns
    patents_df["Cathode Material"] = patents_df["Material-Level"].apply(lambda x: extract_materials(x, cathode_materials))
    patents_df["Anode Material"] = patents_df["Material-Level"].apply(lambda x: extract_materials(x, anode_materials))
    
    # Create citation count
    patents_df["family_citing_lens_id_count"] = patents_df["family_citing_lens_id"].apply(
        lambda x: len(x.split(", ")) if isinstance(x, str) and x else 0
    )
    
    # Filter for patents containing cathode materials
    cathode_patents = patents_df[
        patents_df["Material-Level"].apply(lambda x: contains_material(x, cathode_materials)) &
        (patents_df["family_citing_lens_id_count"] > 0) &
        (patents_df["Cathode Material"].apply(lambda x: x != ["none"]))
    ]
    
    # Filter for patents containing anode materials
    anode_patents = patents_df[
        patents_df["Material-Level"].apply(lambda x: contains_material(x, anode_materials)) &
        (patents_df["family_citing_lens_id_count"] > 0) &
        (patents_df["Anode Material"].apply(lambda x: x != ["none"]))
    ]
    
    return cathode_patents, anode_patents

# Process LIB dataset
lib_cathode, lib_anode = process_dataset(LIB_patents, 'LIB', lib_cathode_materials, lib_anode_materials)

# Process SIB dataset
sib_cathode, sib_anode = process_dataset(SIB_patents, 'SIB', sib_cathode_materials, sib_anode_materials)

print(f"LIB cathode patents: {len(lib_cathode)}")
print(f"LIB anode patents: {len(lib_anode)}")
print(f"SIB cathode patents: {len(sib_cathode)}")
print(f"SIB anode patents: {len(sib_anode)}")

# Debug: Check SIB anode data
print(f"\nDEBUG: SIB anode patents analysis:")
print(f"Total SIB anode patents: {len(sib_anode)}")
print(f"SIB anode patents with citations: {len(sib_anode[sib_anode['family_citing_lens_id_count'] > 0])}")

# Check Hard Carbon specifically
hard_carbon_patents = sib_anode[sib_anode['Anode Material'].apply(lambda x: 'Hard Carbon' in x if isinstance(x, list) else False)]
print(f"Hard Carbon patents: {len(hard_carbon_patents)}")
print(f"Hard Carbon patents with citations: {len(hard_carbon_patents[hard_carbon_patents['family_citing_lens_id_count'] > 0])}")

if len(hard_carbon_patents) > 0:
    print("Sample Hard Carbon patent data:")
    print(hard_carbon_patents[['priority_year', 'Material-Level', 'family_citing_lens_id_count', 'family_citing_lens_id']].head())

#==============================================================
# Create aggregated data for Material-Year nodes
#==============================================================

aggregated_data = []
node_counter = 0

# Process LIB cathode patents
for idx, (_, row) in enumerate(lib_cathode.iterrows()):
    material_list = row["Cathode Material"]
    for material in material_list:
        if material != "none":
            aggregated_data.append({
                'material': material,
                'battery_type': 'LIB',
                'material_type': 'cathode',
                'priority_date': row['earliest_priority_date'],
                'year': row['priority_year'],
                'node_num': node_counter,
                'original_index': idx,
                'dataset_type': 'lib_cathode',
                'family_citing_lens_id': row['family_citing_lens_id']
            })
            node_counter += 1

# Process SIB cathode patents
for idx, (_, row) in enumerate(sib_cathode.iterrows()):
    material_list = row["Cathode Material"]
    for material in material_list:
        if material != "none":
            aggregated_data.append({
                'material': material,
                'battery_type': 'SIB',
                'material_type': 'cathode',
                'priority_date': row['earliest_priority_date'],
                'year': row['priority_year'],
                'node_num': node_counter,
                'original_index': idx + len(lib_cathode),
                'dataset_type': 'sib_cathode',
                'family_citing_lens_id': row['family_citing_lens_id']
            })
            node_counter += 1

# Process LIB anode patents
for idx, (_, row) in enumerate(lib_anode.iterrows()):
    material_list = row["Anode Material"]
    for material in material_list:
        if material != "none":
            aggregated_data.append({
                'material': material,
                'battery_type': 'LIB',
                'material_type': 'anode',
                'priority_date': row['earliest_priority_date'],
                'year': row['priority_year'],
                'node_num': node_counter,
                'original_index': idx + len(lib_cathode) + len(sib_cathode),
                'dataset_type': 'lib_anode',
                'family_citing_lens_id': row['family_citing_lens_id']
            })
            node_counter += 1

# Process SIB anode patents
for idx, (_, row) in enumerate(sib_anode.iterrows()):
    material_list = row["Anode Material"]
    for material in material_list:
        if material != "none":
            aggregated_data.append({
                'material': material,
                'battery_type': 'SIB',
                'material_type': 'anode',
                'priority_date': row['earliest_priority_date'],
                'year': row['priority_year'],
                'node_num': node_counter,
                'original_index': idx + len(lib_cathode) + len(sib_cathode) + len(lib_anode),
                'dataset_type': 'sib_anode',
                'family_citing_lens_id': row['family_citing_lens_id']
            })
            node_counter += 1

# Create a DataFrame from the aggregated data
agg_df = pd.DataFrame(aggregated_data)

# Group by year, battery type, material type, and material to count patents
node_data = agg_df.groupby(['year', 'battery_type', 'material_type', 'material']).size().reset_index(name='patent_count')



#==============================================================
# Create node and edge mappings for the network
#==============================================================

# Create a directed graph
G = nx.DiGraph()

# Create a mapping of (year, battery_type, material_type, material) to node ID
node_mapping = {}
for i, row in node_data.iterrows():
    node_mapping[(row['year'], row['battery_type'], row['material_type'], row['material'])] = i

# Store the reverse mapping for later access
id_to_node = {i: (row['year'], row['battery_type'], row['material_type'], row['material']) for i, row in node_data.iterrows()}

# Create a mapping of patents to their year-material nodes
patent_to_nodes = {}
for _, row in agg_df.iterrows():
    node_key = (row['year'], row['battery_type'], row['material_type'], row['material'])
    node_id = node_mapping.get(node_key)
    
    if node_id is not None:
        if row['node_num'] not in patent_to_nodes:
            patent_to_nodes[row['node_num']] = []
        patent_to_nodes[row['node_num']].append(node_id)

#==============================================================
# Create patent citation networks
#==============================================================

# Create consistent combined dataset maintaining node numbering
lib_cathode_copy = lib_cathode.copy().reset_index(drop=True)
sib_cathode_copy = sib_cathode.copy().reset_index(drop=True)
lib_anode_copy = lib_anode.copy().reset_index(drop=True)
sib_anode_copy = sib_anode.copy().reset_index(drop=True)

# Set consistent indices
sib_cathode_copy.index = sib_cathode_copy.index + len(lib_cathode)
lib_anode_copy.index = lib_anode_copy.index + len(lib_cathode) + len(sib_cathode)
sib_anode_copy.index = sib_anode_copy.index + len(lib_cathode) + len(sib_cathode) + len(lib_anode)

combined_patents = pd.concat([lib_cathode_copy, sib_cathode_copy, lib_anode_copy, sib_anode_copy])

# Create a dictionary to map lens IDs to node numbers - CORRECTED VERSION
lens_id_to_node = {}
for i, row in combined_patents.iterrows():
    # First try Simple Family Members
    if isinstance(row.get('Simple Family Members'), str) and row['Simple Family Members'].strip():
        for lens_id in row['Simple Family Members'].split(';;'):
            lens_id = lens_id.strip()
            if lens_id:
                lens_id_to_node[lens_id] = i
    # If no Simple Family Members, use the Lens ID itself
    elif isinstance(row.get('Lens ID'), str) and row['Lens ID'].strip():
        lens_id_to_node[row['Lens ID']] = i

# Extract citation links - only cross-material type citations
edge_data = []
for i, row in combined_patents.iterrows():
    if isinstance(row['family_citing_lens_id'], str) and row['family_citing_lens_id']:
        for citing_id in row['family_citing_lens_id'].split(', '):
            citing_id = citing_id.strip()
            if citing_id and citing_id in lens_id_to_node:
                cited_node = i
                citing_node = lens_id_to_node[citing_id]
                
                # Get material information directly from combined_patents
                cited_row = combined_patents.loc[cited_node]
                citing_row = combined_patents.loc[citing_node]
                
                # Extract materials from both patents
                cited_materials = []
                citing_materials = []
                
                # Get cited patent materials
                if isinstance(cited_row.get('Cathode Material'), list):
                    cited_materials.extend([(m, 'cathode') for m in cited_row['Cathode Material'] if m != 'none'])
                if isinstance(cited_row.get('Anode Material'), list):
                    cited_materials.extend([(m, 'anode') for m in cited_row['Anode Material'] if m != 'none'])
                
                # Get citing patent materials  
                if isinstance(citing_row.get('Cathode Material'), list):
                    citing_materials.extend([(m, 'cathode') for m in citing_row['Cathode Material'] if m != 'none'])
                if isinstance(citing_row.get('Anode Material'), list):
                    citing_materials.extend([(m, 'anode') for m in citing_row['Anode Material'] if m != 'none'])
                
                # Create edges for cross-material-type citations
                for citing_material, citing_mat_type in citing_materials:
                    for cited_material, cited_mat_type in cited_materials:
                        if citing_mat_type != cited_mat_type:  # Only cross-material-type
                            
                            # Find corresponding nodes in the graph
                            citing_node_key = (citing_row['priority_year'], citing_row['battery_type'], citing_mat_type, citing_material)
                            cited_node_key = (cited_row['priority_year'], cited_row['battery_type'], cited_mat_type, cited_material)
                            
                            citing_graph_node = node_mapping.get(citing_node_key)
                            cited_graph_node = node_mapping.get(cited_node_key)
                            
                            if citing_graph_node is not None and cited_graph_node is not None and citing_graph_node != cited_graph_node:
                                edge_data.append((citing_graph_node, cited_graph_node))

# Count edge frequencies
edge_weights = {}
for edge in edge_data:
    if edge in edge_weights:
        edge_weights[edge] += 1
    else:
        edge_weights[edge] = 1

print(f"Total cross-material citations found: {len(edge_data)}")
print(f"Unique cross-material citation edges: {len(edge_weights)}")


# Debug: Check lens_id_to_node mapping for Hard Carbon
print(f"\nDEBUG: Lens ID mapping for Hard Carbon:")

# Find Hard Carbon patents in combined_patents
hard_carbon_patents_combined = []
for i, row in combined_patents.iterrows():
    # Check if this patent contains Hard Carbon
    if isinstance(row.get('Anode Material'), list) and 'Hard Carbon' in row['Anode Material']:
        hard_carbon_patents_combined.append(i)
    elif 'Hard Carbon' in str(row.get('Material-Level', '')):
        hard_carbon_patents_combined.append(i)

print(f"Hard Carbon patents in combined_patents: {len(hard_carbon_patents_combined)}")

# Check how many of these have Simple Family Members
hc_with_family = 0
hc_without_family = 0
hc_with_lens_id = 0

for idx in hard_carbon_patents_combined[:10]:  # Check first 10
    row = combined_patents.loc[idx]
    
    print(f"\nHard Carbon patent {idx}:")
    print(f"  Lens ID: {repr(row.get('Lens ID'))}")
    print(f"  Simple Family Members: {repr(row.get('Simple Family Members'))}")
    print(f"  Material-Level: {repr(row.get('Material-Level'))}")
    
    # Count categories
    if isinstance(row.get('Simple Family Members'), str) and row['Simple Family Members'].strip():
        hc_with_family += 1
    else:
        hc_without_family += 1
    
    if isinstance(row.get('Lens ID'), str) and row['Lens ID'].strip():
        hc_with_lens_id += 1

print(f"\nHard Carbon Summary:")
print(f"  With Simple Family Members: {hc_with_family}")
print(f"  Without Simple Family Members: {hc_without_family}")
print(f"  With Lens ID: {hc_with_lens_id}")

# Check if ANY Hard Carbon patents made it into lens_id_to_node
hard_carbon_in_mapping = 0
for lens_id, node_num in lens_id_to_node.items():
    if node_num in hard_carbon_patents_combined:
        hard_carbon_in_mapping += 1

print(f"  Hard Carbon patents in lens_id_to_node: {hard_carbon_in_mapping}")


#==============================================================
# Create the network visualization
#==============================================================

# Define color mappings for materials
lib_cathode_colors = {
    'LCO': '#FF0000',  'NMC': '#B22222',  'NCA': '#FF6B6B',
    'LFP': '#0000FF',  'LMO': '#00FFFF'
}

sib_cathode_colors = {
    'CFM': '#FF4500',  'NFM': '#DC143C',  'NMO': '#FF69B4',
    'NVPF': '#0066FF',  'NFPP/NFP': '#000080',
    'Fe-PBA': '#00CC00',  'Mn-PBA': '#006600'
}

lib_anode_colors = {
    'LTO': '#FFA500',  'Silicon/Carbon': '#666666',  'Graphite': '#333333'
}

sib_anode_colors = {
    'Hard Carbon': '#999999'
}

# Add nodes to the graph
for i, row in node_data.iterrows():
    year = row['year']
    battery_type = row['battery_type']
    material_type = row['material_type']
    material = row['material']
    patent_count = row['patent_count']
    
    # Select color based on battery type and material
    if battery_type == 'LIB' and material_type == 'cathode':
        color = lib_cathode_colors.get(material, 'grey')
    elif battery_type == 'SIB' and material_type == 'cathode':
        color = sib_cathode_colors.get(material, 'grey')
    elif battery_type == 'LIB' and material_type == 'anode':
        color = lib_anode_colors.get(material, 'grey')
    else:  # SIB anode
        color = sib_anode_colors.get(material, 'grey')
    
    # Add node with attributes
    G.add_node(i,
               year=year,
               battery_type=battery_type,
               material_type=material_type,
               material=material,
               size=patent_count,
               color=color)

# Filter edge_weights for only existing nodes
valid_edge_weights = {}
for (source, target), weight in edge_weights.items():
    if source in G.nodes() and target in G.nodes():
        valid_edge_weights[(source, target)] = weight

edge_weights = valid_edge_weights

# Add edges with weights
for (source, target), weight in edge_weights.items():
    G.add_edge(source, target, weight=weight)



#==============================================================
# Create detailed citation analysis DataFrame
#==============================================================

# Create detailed citation records
citation_records = []

for i, row in combined_patents.iterrows():
    if isinstance(row['family_citing_lens_id'], str) and row['family_citing_lens_id']:
        for citing_id in row['family_citing_lens_id'].split(', '):
            citing_id = citing_id.strip()
            if citing_id and citing_id in lens_id_to_node:
                cited_node = i  # Current patent is being cited
                citing_node = lens_id_to_node[citing_id]  # Patent that cites
                
                # Get material information directly from combined_patents
                cited_row = combined_patents.loc[cited_node]
                citing_row = combined_patents.loc[citing_node]
                
                # Extract materials from both patents
                cited_materials = []
                citing_materials = []
                
                # Get cited patent materials
                if isinstance(cited_row.get('Cathode Material'), list):
                    cited_materials.extend([m for m in cited_row['Cathode Material'] if m != 'none'])
                if isinstance(cited_row.get('Anode Material'), list):
                    cited_materials.extend([m for m in cited_row['Anode Material'] if m != 'none'])
                
                # Get citing patent materials  
                if isinstance(citing_row.get('Cathode Material'), list):
                    citing_materials.extend([m for m in citing_row['Cathode Material'] if m != 'none'])
                if isinstance(citing_row.get('Anode Material'), list):
                    citing_materials.extend([m for m in citing_row['Anode Material'] if m != 'none'])
                
                # Create citation records for all material combinations
                for citing_material in citing_materials:
                    for cited_material in cited_materials:
                        if citing_material != cited_material:  # Avoid self-citations
                            
                            # Determine material types
                            citing_mat_type = 'cathode' if citing_material in all_cathode_materials else 'anode'
                            cited_mat_type = 'cathode' if cited_material in all_cathode_materials else 'anode'
                            
                            # Determine battery types
                            citing_battery = citing_row['battery_type']
                            cited_battery = cited_row['battery_type']
                            
                            citation_records.append({
                                'citing_patent_index': citing_node,
                                'cited_patent_index': cited_node,
                                'citing_year': citing_row['priority_year'],
                                'citing_battery_type': citing_battery,
                                'citing_material_type': citing_mat_type,
                                'citing_material': citing_material,
                                'cited_year': cited_row['priority_year'],
                                'cited_battery_type': cited_battery, 
                                'cited_material_type': cited_mat_type,
                                'cited_material': cited_material,
                                'citation_direction': f"{citing_material} → {cited_material}",
                                'is_cross_material_type': citing_mat_type != cited_mat_type,
                                'is_cross_battery_type': citing_battery != cited_battery,
                                'citing_lens_id': citing_id
                            })

# Create DataFrame
citations_df = pd.DataFrame(citation_records)

if len(citations_df) > 0:
    # Create summary by material pairs
    material_pair_summary = citations_df.groupby([
        'citing_material', 'cited_material', 
        'citing_battery_type', 'cited_battery_type',
        'citing_material_type', 'cited_material_type'
    ]).size().reset_index(name='citation_count')
    
    # Sort by citation count
    material_pair_summary = material_pair_summary.sort_values('citation_count', ascending=False)
    
    # Create cross-material summary (only cathode <-> anode)
    cross_material_summary = citations_df[
        citations_df['is_cross_material_type'] == True
    ].groupby([
        'citing_material', 'cited_material',
        'citing_battery_type', 'cited_battery_type'
    ]).size().reset_index(name='citation_count')
    
    cross_material_summary = cross_material_summary.sort_values('citation_count', ascending=False)
    
    # Export to Excel with multiple sheets
    excel_filename = "Material_Citation_Analysis.xlsx"
    with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:
        # All citations
        citations_df.to_excel(writer, sheet_name='All_Citations', index=False)
        
        # Material pair summary
        material_pair_summary.to_excel(writer, sheet_name='Material_Pair_Summary', index=False)
        
        # Cross-material citations only
        cross_material_summary.to_excel(writer, sheet_name='Cross_Material_Summary', index=False)
        
        # Hard Carbon specific analysis
        hard_carbon_citations = citations_df[
            (citations_df['citing_material'] == 'Hard Carbon') | 
            (citations_df['cited_material'] == 'Hard Carbon')
        ]
        hard_carbon_citations.to_excel(writer, sheet_name='Hard_Carbon_Citations', index=False)
    
    print(f"\nCitation analysis exported to {excel_filename}")
    print(f"Total citation records: {len(citations_df)}")
    print(f"Unique material pairs: {len(material_pair_summary)}")
    print(f"Cross-material citations: {len(cross_material_summary)}")
    print(f"Hard Carbon citations: {len(hard_carbon_citations)}")
    
    # Quick check for Hard Carbon
    hard_carbon_as_citing = citations_df[citations_df['citing_material'] == 'Hard Carbon']
    hard_carbon_as_cited = citations_df[citations_df['cited_material'] == 'Hard Carbon']
    
    print(f"\nHard Carbon Analysis:")
    print(f"Hard Carbon citing others: {len(hard_carbon_as_citing)}")
    print(f"Others citing Hard Carbon: {len(hard_carbon_as_cited)}")
    
    if len(hard_carbon_as_citing) > 0:
        print("Hard Carbon cites these materials:")
        print(hard_carbon_as_citing['cited_material'].value_counts().head())
    
    if len(hard_carbon_as_cited) > 0:
        print("These materials cite Hard Carbon:")
        print(hard_carbon_as_cited['citing_material'].value_counts().head())

else:
    print("No citation records found!")


#==============================================================
# Calculate node positions with the specified layout
#==============================================================

# Define material orders
lib_cathode_order = ['LCO', 'NMC', 'NCA', 'LFP', 'LMO']
sib_cathode_order = ['Fe-PBA', 'Mn-PBA', 'NVPF', 'NFPP/NFP', 'NFM', 'CFM', 'NMO']
lib_anode_order = ['Graphite', 'Silicon/Carbon', 'LTO']
sib_anode_order = ['Hard Carbon']

# Y-positions for materials - corrected spacing
y_spacing = 60
total_cathode_materials = len(lib_cathode_order) + len(sib_cathode_order)
total_anode_materials = len(lib_anode_order) + len(sib_anode_order)

# Calculate positions to avoid overlap
cathode_section_start = 500
cathode_section_end = cathode_section_start - (total_cathode_materials * y_spacing)
gap_between_sections = 100
anode_section_start = cathode_section_end - gap_between_sections
anode_section_end = anode_section_start - (total_anode_materials * y_spacing) - 50  # Extra gap for SIB anodes

material_y_positions = {}

# Cathode materials (top section)
y_pos = cathode_section_start
for material in lib_cathode_order:
    material_y_positions[('LIB', 'cathode', material)] = y_pos
    y_pos -= y_spacing

for material in sib_cathode_order:
    material_y_positions[('SIB', 'cathode', material)] = y_pos
    y_pos -= y_spacing

# Anode materials (bottom section)
y_pos = anode_section_start
for material in lib_anode_order:
    material_y_positions[('LIB', 'anode', material)] = y_pos
    y_pos -= y_spacing

# Gap for SIB anodes
y_pos -= 50
for material in sib_anode_order:
    material_y_positions[('SIB', 'anode', material)] = y_pos
    y_pos -= y_spacing

print(f"Cathode section: {cathode_section_start} to {cathode_section_end}")
print(f"Anode section: {anode_section_start} to {anode_section_end}")

# Calculate positions for each node
pos = {}
for node_id, (year, battery_type, material_type, material) in id_to_node.items():
    x = year
    y = material_y_positions.get((battery_type, material_type, material), 0)
    pos[node_id] = (x, y)

#==============================================================
# Define edge colors for cross-material citations
#==============================================================

def get_cross_material_edge_color(source_id, target_id):
    """Determine edge color based on source and target material types and battery types."""
    source_battery = G.nodes[source_id]['battery_type']
    source_mat_type = G.nodes[source_id]['material_type']
    target_battery = G.nodes[target_id]['battery_type']
    target_mat_type = G.nodes[target_id]['material_type']
    
    # Define the 8 categories
    if source_battery == 'LIB' and source_mat_type == 'cathode' and target_battery == 'LIB' and target_mat_type == 'anode':
        return 'rgba(220, 20, 60, 0.8)'  # Crimson - LIB Cathode -> LIB Anode
    elif source_battery == 'LIB' and source_mat_type == 'anode' and target_battery == 'LIB' and target_mat_type == 'cathode':
        return 'rgba(255, 140, 0, 0.8)'  # Orange - LIB Anode -> LIB Cathode
    elif source_battery == 'SIB' and source_mat_type == 'anode' and target_battery == 'SIB' and target_mat_type == 'cathode':
        return 'rgba(0, 100, 0, 0.8)'  # Dark Green - SIB Anode -> SIB Cathode
    elif source_battery == 'SIB' and source_mat_type == 'cathode' and target_battery == 'SIB' and target_mat_type == 'anode':
        return 'rgba(0, 191, 255, 0.8)'  # Deep Sky Blue - SIB Cathode -> SIB Anode
    elif source_battery == 'LIB' and source_mat_type == 'cathode' and target_battery == 'SIB' and target_mat_type == 'anode':
        return 'rgba(138, 43, 226, 0.8)'  # Blue Violet - LIB Cathode -> SIB Anode
    elif source_battery == 'SIB' and source_mat_type == 'anode' and target_battery == 'LIB' and target_mat_type == 'cathode':
        return 'rgba(255, 20, 147, 0.8)'  # Deep Pink - SIB Anode -> LIB Cathode
    elif source_battery == 'LIB' and source_mat_type == 'anode' and target_battery == 'SIB' and target_mat_type == 'cathode':
        return 'rgba(50, 205, 50, 0.8)'  # Lime Green - LIB Anode -> SIB Cathode
    elif source_battery == 'SIB' and source_mat_type == 'cathode' and target_battery == 'LIB' and target_mat_type == 'anode':
        return 'rgba(255, 215, 0, 0.8)'  # Gold - SIB Cathode -> LIB Anode
    else:
        return 'rgba(128, 128, 128, 0.5)'  # Default gray

def get_edge_category_name(source_id, target_id):
    """Get descriptive name for edge category."""
    source_battery = G.nodes[source_id]['battery_type']
    source_mat_type = G.nodes[source_id]['material_type']
    target_battery = G.nodes[target_id]['battery_type']
    target_mat_type = G.nodes[target_id]['material_type']
    
    categories = {
        ('LIB', 'cathode', 'LIB', 'anode'): 'LIB Cathode → LIB Anode',
        ('LIB', 'anode', 'LIB', 'cathode'): 'LIB Anode → LIB Cathode',
        ('SIB', 'anode', 'SIB', 'cathode'): 'SIB Anode → SIB Cathode',
        ('SIB', 'cathode', 'SIB', 'anode'): 'SIB Cathode → SIB Anode',
        ('LIB', 'cathode', 'SIB', 'anode'): 'LIB Cathode → SIB Anode',
        ('SIB', 'anode', 'LIB', 'cathode'): 'SIB Anode → LIB Cathode',
        ('LIB', 'anode', 'SIB', 'cathode'): 'LIB Anode → SIB Cathode',
        ('SIB', 'cathode', 'LIB', 'anode'): 'SIB Cathode → LIB Anode'
    }
    
    return categories.get((source_battery, source_mat_type, target_battery, target_mat_type), 'Unknown')

#==============================================================
# Create the Plotly visualization
#==============================================================

# Create edge traces
edge_traces = []
min_width = 1.0
max_width = 6.0

# Find maximum edge weight for scaling
max_weight = max([G.edges[edge]['weight'] for edge in G.edges()]) if G.edges() else 1

def scale_edge_width(weight):
    return min_width + (max_width - min_width) * ((weight / max_weight) ** 0.5)

# Create edge traces
for edge in G.edges():
    source, target = edge
    x0, y0 = pos[source]
    x1, y1 = pos[target]
    weight = G.edges[source, target]['weight']
    width = scale_edge_width(weight)
    
    edge_color = get_cross_material_edge_color(source, target)
    category_name = get_edge_category_name(source, target)
    
    edge_trace = go.Scatter(
        x=[x0, x1, None], 
        y=[y0, y1, None],
        mode='lines',
        line=dict(width=width, color=edge_color),
        hoverinfo='text',
        text=f"Citations: {weight}<br>From: {G.nodes[source]['battery_type']} {G.nodes[source]['material']} ({G.nodes[source]['year']})<br>To: {G.nodes[target]['battery_type']} {G.nodes[target]['material']} ({G.nodes[target]['year']})<br>Category: {category_name}",
        showlegend=False
    )
    edge_traces.append(edge_trace)

# Create node traces by material type and battery type
cathode_nodes = []
anode_nodes = []

min_node_size = 8
max_node_size = 35

max_patent_count = max([G.nodes[n]['size'] for n in G.nodes()]) if G.nodes() else 1

def scale_node_size(patent_count):
    return min_node_size + (max_node_size - min_node_size) * ((patent_count / max_patent_count) ** 0.5)

for node in G.nodes():
    x, y = pos[node]
    patent_count = G.nodes[node]['size']
    scaled_size = scale_node_size(patent_count)
    
    hover_text = (
        f"Material: {G.nodes[node]['material']}<br>"
        f"Battery Type: {G.nodes[node]['battery_type']}<br>"
        f"Material Type: {G.nodes[node]['material_type']}<br>"
        f"Year: {G.nodes[node]['year']}<br>"
        f"Patents: {G.nodes[node]['size']}"
    )
    
    node_data = {
        'x': x, 'y': y, 'size': scaled_size,
        'color': G.nodes[node]['color'], 'text': hover_text
    }
    
    if G.nodes[node]['material_type'] == 'cathode':
        cathode_nodes.append(node_data)
    else:
        anode_nodes.append(node_data)

# Create node traces
cathode_trace = go.Scatter(
    x=[node['x'] for node in cathode_nodes],
    y=[node['y'] for node in cathode_nodes],
    mode='markers',
    marker=dict(
        size=[node['size'] for node in cathode_nodes],
        color=[node['color'] for node in cathode_nodes],
        line=dict(width=1, color='black')
    ),
    text=[node['text'] for node in cathode_nodes],
    hoverinfo='text',
    showlegend=False
)

anode_trace = go.Scatter(
    x=[node['x'] for node in anode_nodes],
    y=[node['y'] for node in anode_nodes],
    mode='markers',
    marker=dict(
        size=[node['size'] for node in anode_nodes],
        color=[node['color'] for node in anode_nodes],
        line=dict(width=1, color='black')
    ),
    text=[node['text'] for node in anode_nodes],
    hoverinfo='text',
    showlegend=False
)

# Create legend entries
legend_entries = []

# Material legends
all_colors = {**lib_cathode_colors, **sib_cathode_colors, **lib_anode_colors, **sib_anode_colors}
for material, color in all_colors.items():
    if material in lib_cathode_materials:
        name = f'LIB Cathode: {material}'
    elif material in sib_cathode_materials:
        name = f'SIB Cathode: {material}'
    elif material in lib_anode_materials:
        name = f'LIB Anode: {material}'
    else:
        name = f'SIB Anode: {material}'
    
    legend_entries.append(go.Scatter(
        x=[None], y=[None], mode='markers',
        marker=dict(size=12, color=color),
        name=name, hoverinfo='none'
    ))

# Edge category legends
edge_legend_data = [
    ('LIB Cathode → LIB Anode', 'rgba(220, 20, 60, 0.8)'),
    ('LIB Anode → LIB Cathode', 'rgba(255, 140, 0, 0.8)'),
    ('SIB Anode → SIB Cathode', 'rgba(0, 100, 0, 0.8)'),
    ('SIB Cathode → SIB Anode', 'rgba(0, 191, 255, 0.8)'),
    ('LIB Cathode → SIB Anode', 'rgba(138, 43, 226, 0.8)'),
    ('SIB Anode → LIB Cathode', 'rgba(255, 20, 147, 0.8)'),
    ('LIB Anode → SIB Cathode', 'rgba(50, 205, 50, 0.8)'),
    ('SIB Cathode → LIB Anode', 'rgba(255, 215, 0, 0.8)')
]

for name, color in edge_legend_data:
    legend_entries.append(go.Scatter(
        x=[None], y=[None], mode='lines',
        line=dict(color=color, width=4),
        name=name, hoverinfo='none'
    ))

# Create the figure
fig = go.Figure(data=edge_traces + [cathode_trace, anode_trace] + legend_entries)

# Calculate layout parameters
min_year = min([G.nodes[n]['year'] for n in G.nodes()]) if G.nodes() else 2000
max_year = max([G.nodes[n]['year'] for n in G.nodes()]) if G.nodes() else 2025
x_ticks = list(range(min_year, max_year + 1, 5))

# Create y-axis ticks and labels
y_ticks = []
y_tick_labels = []

# Add cathode section label
all_cathode_materials = lib_cathode_order + sib_cathode_order
for i, material in enumerate(all_cathode_materials):
    y_pos = cathode_section_start - i * y_spacing
    y_ticks.append(y_pos)
    if material in lib_cathode_materials:
        y_tick_labels.append(f"LIB Cathode: {material}")
    else:
        y_tick_labels.append(f"SIB Cathode: {material}")

# Add anode section label
all_anode_materials = lib_anode_order + sib_anode_order
for i, material in enumerate(all_anode_materials):
    y_pos = anode_section_start - i * y_spacing
    if i == len(lib_anode_order):  # Add gap before SIB anodes
        y_pos -= 30
    y_ticks.append(y_pos)
    if material in lib_anode_materials:
        y_tick_labels.append(f"LIB Anode: {material}")
    else:
        y_tick_labels.append(f"SIB Anode: {material}")

# Add section dividers
cathode_anode_divider_y = (min(material_y_positions[key] for key in material_y_positions.keys() if key[1] == 'cathode') + 
                          max(material_y_positions[key] for key in material_y_positions.keys() if key[1] == 'anode')) / 2

lib_sib_cathode_divider_y = material_y_positions[('LIB', 'cathode', 'LMO')] - y_spacing/2
lib_sib_anode_divider_y = material_y_positions[('LIB', 'anode', 'LTO')] - y_spacing/2 - 15

# Add divider lines and annotations
fig.add_shape(
    type="line",
    x0=min_year-1, y0=cathode_anode_divider_y,
    x1=max_year+1, y1=cathode_anode_divider_y,
    line=dict(color="black", width=3, dash="solid")
)

fig.add_shape(
    type="line",
    x0=min_year-1, y0=lib_sib_cathode_divider_y,
    x1=max_year+1, y1=lib_sib_cathode_divider_y,
    line=dict(color="gray", width=2, dash="dash")
)

fig.add_shape(
    type="line",
    x0=min_year-1, y0=lib_sib_anode_divider_y,
    x1=max_year+1, y1=lib_sib_anode_divider_y,
    line=dict(color="gray", width=2, dash="dash")
)

# Add section annotations
fig.add_annotation(
    x=min_year-4, y=lib_sib_cathode_divider_y,
    text="POSITIVE ELECTRODE MATERIALS", showarrow=False,
    font=dict(size=16, color="black", family="Arial Black"),
    textangle=-90  # Rotate 90 degrees
)

fig.add_annotation(
    x=min_year-4, y=lib_sib_anode_divider_y,
    text="NEGATIVE ELECTRODE MATERIALS", showarrow=False,
    font=dict(size=16, color="black", family="Arial Black"),
    textangle=-90  # Rotate 90 degrees
)

fig.add_annotation(
    x=min_year-2, y=lib_sib_cathode_divider_y,
    text="LIB | SIB", showarrow=False,
    font=dict(size=14, color="gray")
)

fig.add_annotation(
    x=min_year-2, y=lib_sib_anode_divider_y,
    text="LIB | SIB", showarrow=False,
    font=dict(size=14, color="gray")
)

# Update layout
fig.update_layout(
    title=dict(
        text="Cross-Material Patent Citation Network: Positive Electrode-Negative Electrode Knowledge Flows",
        font=dict(size=24)
    ),
    showlegend=True,
    xaxis=dict(
        title=dict(text="Priority Year", font=dict(size=20)),
        tickmode='array',
        tickvals=x_ticks,
        ticktext=[str(year) for year in x_ticks],
        tickfont=dict(size=16),
        showgrid=True,
        gridwidth=1,
        gridcolor='lightgrey'
    ),
    yaxis=dict(
        tickmode='array',
        tickvals=y_ticks,
        ticktext=y_tick_labels,
        title=dict(text="Material Type and Specific Material", font=dict(size=20)),
        tickfont=dict(size=14),
        zeroline=False
    ),
    hovermode='closest',
    hoverlabel=dict(font=dict(size=16)),
    plot_bgcolor='rgba(0,0,0,0)',
    paper_bgcolor='rgba(0,0,0,0)',
    legend=dict(
        yanchor="top", y=0.99, xanchor="left", x=1.01,
        bgcolor="rgba(255, 255, 255, 0.9)", 
        font=dict(size=14),
        itemsizing="constant"
    ),
    width=2000,
    height=1200,
    margin=dict(l=200, r=300, t=100, b=80)
)

# Save the visualization
output_file = "Cross_Material_Cathode_Anode_Patent_Network.html"
fig.write_html(output_file)
print(f"Visualization saved to {output_file}")

# Print network statistics
print("\nCross-Material Citation Network Statistics:")
print(f"Total material-year nodes: {G.number_of_nodes()}")
print(f"Cathode material-year nodes: {len([n for n in G.nodes() if G.nodes[n]['material_type'] == 'cathode'])}")
print(f"Anode material-year nodes: {len([n for n in G.nodes() if G.nodes[n]['material_type'] == 'anode'])}")
print(f"Total cross-material citations: {G.number_of_edges()}")

# Count citations by category
citation_categories = {}
for edge in G.edges():
    source, target = edge
    category = get_edge_category_name(source, target)
    if category not in citation_categories:
        citation_categories[category] = 0
    citation_categories[category] += G.edges[edge]['weight']

print("\nCitation counts by category:")
for category, count in sorted(citation_categories.items()):
    print(f"  {category}: {count}")

# Calculate overall directional flows
cathode_to_anode = sum(G.edges[edge]['weight'] for edge in G.edges() 
                      if G.nodes[edge[0]]['material_type'] == 'cathode' and 
                      G.nodes[edge[1]]['material_type'] == 'anode')

anode_to_cathode = sum(G.edges[edge]['weight'] for edge in G.edges() 
                      if G.nodes[edge[0]]['material_type'] == 'anode' and 
                      G.nodes[edge[1]]['material_type'] == 'cathode')

lib_to_sib = sum(G.edges[edge]['weight'] for edge in G.edges() 
                if G.nodes[edge[0]]['battery_type'] == 'LIB' and 
                G.nodes[edge[1]]['battery_type'] == 'SIB')

sib_to_lib = sum(G.edges[edge]['weight'] for edge in G.edges() 
                if G.nodes[edge[0]]['battery_type'] == 'SIB' and 
                G.nodes[edge[1]]['battery_type'] == 'LIB')

print(f"\nDirectional flow analysis:")
print(f"Cathode materials citing Anode materials: {cathode_to_anode}")
print(f"Anode materials citing Cathode materials: {anode_to_cathode}")
print(f"LIB materials citing SIB materials: {lib_to_sib}")
print(f"SIB materials citing LIB materials: {sib_to_lib}")

print("\nAnalysis complete!")


#%%

#==========================================================================================================================
# Building Cathode → Anode citation network showing knowledge flows from cathode to anode materials
#==========================================================================================================================

import pandas as pd
import networkx as nx
import plotly.graph_objects as go
import numpy as np
from datetime import datetime

# File paths for both datasets
lib_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_full_data_classified_onlyLIB_27052025.xlsx"
sib_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_full_data_classified_final_onlySIB_26052025.xlsx"
sheet_name = "Sheet1"

# Load both Excel files
LIB_patents = pd.read_excel(lib_file_path, sheet_name=sheet_name)
SIB_patents = pd.read_excel(sib_file_path, sheet_name=sheet_name)

# Print basic info about both datasets
print(f"LIB dataset shape: {LIB_patents.shape}")
print(f"SIB dataset shape: {SIB_patents.shape}")

# Add a 'battery_type' column to identify the source in the combined dataset
LIB_patents['battery_type'] = 'LIB'
SIB_patents['battery_type'] = 'SIB'

#==============================================================
# Define material sets for both battery types
#==============================================================

# Define LIB cathode materials
lib_cathode_materials = {"LCO", "LMO", "LFP", "NMC", "NCA"}

# Define SIB cathode materials
sib_cathode_materials = {"Fe-PBA", "Mn-PBA", "NVPF", "NFPP/NFP", "NFM", "CFM", "NMO"}

# Define LIB anode materials
lib_anode_materials = {"Silicon/Carbon", "Graphite", "LTO"}

# Define SIB anode materials
sib_anode_materials = {"Hard Carbon"}

# Combine all materials
all_cathode_materials = lib_cathode_materials | sib_cathode_materials
all_anode_materials = lib_anode_materials | sib_anode_materials
all_materials = all_cathode_materials | all_anode_materials

#==============================================================
# Function to extract materials from material strings
#==============================================================

def extract_materials(material_str, material_set):
    """Extract materials that match the given material set from a comma-separated string."""
    materials = set(material_str.split(", ")) if isinstance(material_str, str) else set()
    extracted = list(materials.intersection(material_set))
    return extracted if extracted else ["none"]

def contains_material(material_str, material_set):
    """Check if the material string contains any material from the given set."""
    if not isinstance(material_str, str) or material_str == "none":
        return False
    materials = set(material_str.split(", "))
    return bool(materials.intersection(material_set))

#==============================================================
# Process both datasets for cathode and anode materials
#==============================================================

def process_dataset(patents_df, battery_type, cathode_materials, anode_materials):
    """Process dataset to extract both cathode and anode materials."""
    
    # Extract priority year
    patents_df["earliest_priority_date"] = pd.to_datetime(patents_df["Earliest Priority Date"])
    patents_df["priority_year"] = patents_df["earliest_priority_date"].dt.year
    patents_df = patents_df.sort_values(by=["priority_year"], ascending=True)
    
    # Create material columns
    patents_df["Cathode Material"] = patents_df["Material-Level"].apply(lambda x: extract_materials(x, cathode_materials))
    patents_df["Anode Material"] = patents_df["Material-Level"].apply(lambda x: extract_materials(x, anode_materials))
    
    # Create citation count
    patents_df["family_citing_lens_id_count"] = patents_df["family_citing_lens_id"].apply(
        lambda x: len(x.split(", ")) if isinstance(x, str) and x else 0
    )
    
    # Filter for patents containing cathode materials
    cathode_patents = patents_df[
        patents_df["Material-Level"].apply(lambda x: contains_material(x, cathode_materials)) &
        (patents_df["family_citing_lens_id_count"] > 0) &
        (patents_df["Cathode Material"].apply(lambda x: x != ["none"]))
    ]
    
    # Filter for patents containing anode materials
    anode_patents = patents_df[
        patents_df["Material-Level"].apply(lambda x: contains_material(x, anode_materials)) &
        (patents_df["family_citing_lens_id_count"] > 0) &
        (patents_df["Anode Material"].apply(lambda x: x != ["none"]))
    ]
    
    return cathode_patents, anode_patents

# Process LIB dataset
lib_cathode, lib_anode = process_dataset(LIB_patents, 'LIB', lib_cathode_materials, lib_anode_materials)

# Process SIB dataset
sib_cathode, sib_anode = process_dataset(SIB_patents, 'SIB', sib_cathode_materials, sib_anode_materials)

print(f"LIB cathode patents: {len(lib_cathode)}")
print(f"LIB anode patents: {len(lib_anode)}")
print(f"SIB cathode patents: {len(sib_cathode)}")
print(f"SIB anode patents: {len(sib_anode)}")

#==============================================================
# Create aggregated data for Material-Year nodes
#==============================================================

aggregated_data = []
node_counter = 0

# Process LIB cathode patents
for idx, (_, row) in enumerate(lib_cathode.iterrows()):
    material_list = row["Cathode Material"]
    for material in material_list:
        if material != "none":
            aggregated_data.append({
                'material': material,
                'battery_type': 'LIB',
                'material_type': 'cathode',
                'priority_date': row['earliest_priority_date'],
                'year': row['priority_year'],
                'node_num': node_counter,
                'original_index': idx,
                'dataset_type': 'lib_cathode',
                'family_citing_lens_id': row['family_citing_lens_id']
            })
            node_counter += 1

# Process SIB cathode patents
for idx, (_, row) in enumerate(sib_cathode.iterrows()):
    material_list = row["Cathode Material"]
    for material in material_list:
        if material != "none":
            aggregated_data.append({
                'material': material,
                'battery_type': 'SIB',
                'material_type': 'cathode',
                'priority_date': row['earliest_priority_date'],
                'year': row['priority_year'],
                'node_num': node_counter,
                'original_index': idx + len(lib_cathode),
                'dataset_type': 'sib_cathode',
                'family_citing_lens_id': row['family_citing_lens_id']
            })
            node_counter += 1

# Process LIB anode patents
for idx, (_, row) in enumerate(lib_anode.iterrows()):
    material_list = row["Anode Material"]
    for material in material_list:
        if material != "none":
            aggregated_data.append({
                'material': material,
                'battery_type': 'LIB',
                'material_type': 'anode',
                'priority_date': row['earliest_priority_date'],
                'year': row['priority_year'],
                'node_num': node_counter,
                'original_index': idx + len(lib_cathode) + len(sib_cathode),
                'dataset_type': 'lib_anode',
                'family_citing_lens_id': row['family_citing_lens_id']
            })
            node_counter += 1

# Process SIB anode patents
for idx, (_, row) in enumerate(sib_anode.iterrows()):
    material_list = row["Anode Material"]
    for material in material_list:
        if material != "none":
            aggregated_data.append({
                'material': material,
                'battery_type': 'SIB',
                'material_type': 'anode',
                'priority_date': row['earliest_priority_date'],
                'year': row['priority_year'],
                'node_num': node_counter,
                'original_index': idx + len(lib_cathode) + len(sib_cathode) + len(lib_anode),
                'dataset_type': 'sib_anode',
                'family_citing_lens_id': row['family_citing_lens_id']
            })
            node_counter += 1

# Create a DataFrame from the aggregated data
agg_df = pd.DataFrame(aggregated_data)

# Group by year, battery type, material type, and material to count patents
node_data = agg_df.groupby(['year', 'battery_type', 'material_type', 'material']).size().reset_index(name='patent_count')

#==============================================================
# Create node and edge mappings for the network
#==============================================================

# Create a directed graph
G = nx.DiGraph()

# Create a mapping of (year, battery_type, material_type, material) to node ID
node_mapping = {}
for i, row in node_data.iterrows():
    node_mapping[(row['year'], row['battery_type'], row['material_type'], row['material'])] = i

# Store the reverse mapping for later access
id_to_node = {i: (row['year'], row['battery_type'], row['material_type'], row['material']) for i, row in node_data.iterrows()}

#==============================================================
# Create patent citation networks
#==============================================================

# Create consistent combined dataset maintaining node numbering
lib_cathode_copy = lib_cathode.copy().reset_index(drop=True)
sib_cathode_copy = sib_cathode.copy().reset_index(drop=True)
lib_anode_copy = lib_anode.copy().reset_index(drop=True)
sib_anode_copy = sib_anode.copy().reset_index(drop=True)

# Set consistent indices
sib_cathode_copy.index = sib_cathode_copy.index + len(lib_cathode)
lib_anode_copy.index = lib_anode_copy.index + len(lib_cathode) + len(sib_cathode)
sib_anode_copy.index = sib_anode_copy.index + len(lib_cathode) + len(sib_cathode) + len(lib_anode)

combined_patents = pd.concat([lib_cathode_copy, sib_cathode_copy, lib_anode_copy, sib_anode_copy])

# Create a dictionary to map lens IDs to node numbers
lens_id_to_node = {}
for i, row in combined_patents.iterrows():
    # First try Simple Family Members
    if isinstance(row.get('Simple Family Members'), str) and row['Simple Family Members'].strip():
        for lens_id in row['Simple Family Members'].split(';;'):
            lens_id = lens_id.strip()
            if lens_id:
                lens_id_to_node[lens_id] = i
    # If no Simple Family Members, use the Lens ID itself
    elif isinstance(row.get('Lens ID'), str) and row['Lens ID'].strip():
        lens_id_to_node[row['Lens ID']] = i

# Extract citation links - ONLY CATHODE → ANODE citations
edge_data = []
for i, row in combined_patents.iterrows():
    if isinstance(row['family_citing_lens_id'], str) and row['family_citing_lens_id']:
        for citing_id in row['family_citing_lens_id'].split(', '):
            citing_id = citing_id.strip()
            if citing_id and citing_id in lens_id_to_node:
                cited_node = i
                citing_node = lens_id_to_node[citing_id]
                
                # Get material information directly from combined_patents
                cited_row = combined_patents.loc[cited_node]
                citing_row = combined_patents.loc[citing_node]
                
                # Extract materials from both patents
                cited_materials = []
                citing_materials = []
                
                # Get cited patent materials
                if isinstance(cited_row.get('Cathode Material'), list):
                    cited_materials.extend([(m, 'cathode') for m in cited_row['Cathode Material'] if m != 'none'])
                if isinstance(cited_row.get('Anode Material'), list):
                    cited_materials.extend([(m, 'anode') for m in cited_row['Anode Material'] if m != 'none'])
                
                # Get citing patent materials  
                if isinstance(citing_row.get('Cathode Material'), list):
                    citing_materials.extend([(m, 'cathode') for m in citing_row['Cathode Material'] if m != 'none'])
                if isinstance(citing_row.get('Anode Material'), list):
                    citing_materials.extend([(m, 'anode') for m in citing_row['Anode Material'] if m != 'none'])
                
                # Create edges ONLY for CATHODE → ANODE citations
                for citing_material, citing_mat_type in citing_materials:
                    for cited_material, cited_mat_type in cited_materials:
                        if citing_mat_type == 'cathode' and cited_mat_type == 'anode':  # ONLY cathode → anode
                            
                            # Find corresponding nodes in the graph
                            citing_node_key = (citing_row['priority_year'], citing_row['battery_type'], citing_mat_type, citing_material)
                            cited_node_key = (cited_row['priority_year'], cited_row['battery_type'], cited_mat_type, cited_material)
                            
                            citing_graph_node = node_mapping.get(citing_node_key)
                            cited_graph_node = node_mapping.get(cited_node_key)
                            
                            if citing_graph_node is not None and cited_graph_node is not None and citing_graph_node != cited_graph_node:
                                edge_data.append((citing_graph_node, cited_graph_node))

# Count edge frequencies
edge_weights = {}
for edge in edge_data:
    if edge in edge_weights:
        edge_weights[edge] += 1
    else:
        edge_weights[edge] = 1

print(f"Total cathode → anode citations found: {len(edge_data)}")
print(f"Unique cathode → anode citation edges: {len(edge_weights)}")

#==============================================================
# Create the network visualization
#==============================================================

# Define color mappings for materials (same as before)
lib_cathode_colors = {
    'LCO': '#FF0000',  'NMC': '#B22222',  'NCA': '#FF6B6B',
    'LFP': '#0000FF',  'LMO': '#00FFFF'
}

sib_cathode_colors = {
    'CFM': '#FF4500',  'NFM': '#DC143C',  'NMO': '#FF69B4',
    'NVPF': '#0066FF',  'NFPP/NFP': '#000080',
    'Fe-PBA': '#00CC00',  'Mn-PBA': '#006600'
}

lib_anode_colors = {
    'LTO': '#FFA500',  'Silicon/Carbon': '#666666',  'Graphite': '#333333'
}

sib_anode_colors = {
    'Hard Carbon': '#999999'
}

# Add nodes to the graph
for i, row in node_data.iterrows():
    year = row['year']
    battery_type = row['battery_type']
    material_type = row['material_type']
    material = row['material']
    patent_count = row['patent_count']
    
    # Select color based on battery type and material
    if battery_type == 'LIB' and material_type == 'cathode':
        color = lib_cathode_colors.get(material, 'grey')
    elif battery_type == 'SIB' and material_type == 'cathode':
        color = sib_cathode_colors.get(material, 'grey')
    elif battery_type == 'LIB' and material_type == 'anode':
        color = lib_anode_colors.get(material, 'grey')
    else:  # SIB anode
        color = sib_anode_colors.get(material, 'grey')
    
    # Add node with attributes
    G.add_node(i,
               year=year,
               battery_type=battery_type,
               material_type=material_type,
               material=material,
               size=patent_count,
               color=color)

# Filter edge_weights for only existing nodes
valid_edge_weights = {}
for (source, target), weight in edge_weights.items():
    if source in G.nodes() and target in G.nodes():
        valid_edge_weights[(source, target)] = weight

edge_weights = valid_edge_weights

# Add edges with weights
for (source, target), weight in edge_weights.items():
    G.add_edge(source, target, weight=weight)

#==============================================================
# Calculate node positions with the specified layout
#==============================================================

# Define material orders
lib_cathode_order = ['LCO', 'NMC', 'NCA', 'LFP', 'LMO']
sib_cathode_order = ['Fe-PBA', 'Mn-PBA', 'NVPF', 'NFPP/NFP', 'NFM', 'CFM', 'NMO']
lib_anode_order = ['Graphite', 'Silicon/Carbon', 'LTO']
sib_anode_order = ['Hard Carbon']

# Y-positions for materials
y_spacing = 60
total_cathode_materials = len(lib_cathode_order) + len(sib_cathode_order)
total_anode_materials = len(lib_anode_order) + len(sib_anode_order)

# Calculate positions to avoid overlap
cathode_section_start = 500
cathode_section_end = cathode_section_start - (total_cathode_materials * y_spacing)
gap_between_sections = 100
anode_section_start = cathode_section_end - gap_between_sections
anode_section_end = anode_section_start - (total_anode_materials * y_spacing) - 50

material_y_positions = {}

# Cathode materials (top section)
y_pos = cathode_section_start
for material in lib_cathode_order:
    material_y_positions[('LIB', 'cathode', material)] = y_pos
    y_pos -= y_spacing

for material in sib_cathode_order:
    material_y_positions[('SIB', 'cathode', material)] = y_pos
    y_pos -= y_spacing

# Anode materials (bottom section)
y_pos = anode_section_start
for material in lib_anode_order:
    material_y_positions[('LIB', 'anode', material)] = y_pos
    y_pos -= y_spacing

# Gap for SIB anodes
y_pos -= 50
for material in sib_anode_order:
    material_y_positions[('SIB', 'anode', material)] = y_pos
    y_pos -= y_spacing

# Calculate positions for each node
pos = {}
for node_id, (year, battery_type, material_type, material) in id_to_node.items():
    x = year
    y = material_y_positions.get((battery_type, material_type, material), 0)
    pos[node_id] = (x, y)

#==============================================================
# Define edge colors for cathode → anode citations
#==============================================================

def get_cathode_to_anode_edge_color(source_id, target_id):
    """Determine edge color for cathode → anode citations."""
    source_battery = G.nodes[source_id]['battery_type']
    target_battery = G.nodes[target_id]['battery_type']
    
    # 4 categories for cathode → anode
    if source_battery == 'LIB' and target_battery == 'LIB':
        return 'rgba(120, 120, 120, 0.8)'  # Dark Grey - LIB Cathode → LIB Anode
    elif source_battery == 'SIB' and target_battery == 'SIB':
        return 'rgba(180, 180, 180, 0.8)'  # Light Grey - SIB Cathode → SIB Anode
    elif source_battery == 'LIB' and target_battery == 'SIB':
        return 'rgba(255, 140, 0, 0.8)'    # Orange - LIB Cathode → SIB Anode
    else:  # SIB → LIB
        return 'rgba(255, 165, 0, 0.8)'    # Light Orange - SIB Cathode → LIB Anode

def get_cathode_to_anode_category_name(source_id, target_id):
    """Get descriptive name for cathode → anode edge category."""
    source_battery = G.nodes[source_id]['battery_type']
    target_battery = G.nodes[target_id]['battery_type']
    
    categories = {
        ('LIB', 'LIB'): 'LIB Cathode → LIB Anode',
        ('SIB', 'SIB'): 'SIB Cathode → SIB Anode', 
        ('LIB', 'SIB'): 'LIB Cathode → SIB Anode',
        ('SIB', 'LIB'): 'SIB Cathode → LIB Anode'
    }
    
    return categories.get((source_battery, target_battery), 'Unknown')

#==============================================================
# Create the Plotly visualization
#==============================================================

# Create edge traces
edge_traces = []
min_width = 1.0
max_width = 6.0

# Find maximum edge weight for scaling
max_weight = max([G.edges[edge]['weight'] for edge in G.edges()]) if G.edges() else 1

def scale_edge_width(weight):
    return min_width + (max_width - min_width) * ((weight / max_weight) ** 0.5)

# Create edge traces
for edge in G.edges():
    source, target = edge
    x0, y0 = pos[source]
    x1, y1 = pos[target]
    weight = G.edges[source, target]['weight']
    width = scale_edge_width(weight)
    
    edge_color = get_cathode_to_anode_edge_color(source, target)
    category_name = get_cathode_to_anode_category_name(source, target)
    
    edge_trace = go.Scatter(
        x=[x0, x1, None], 
        y=[y0, y1, None],
        mode='lines',
        line=dict(width=width, color=edge_color),
        hoverinfo='text',
        text=f"Citations: {weight}<br>From: {G.nodes[source]['battery_type']} {G.nodes[source]['material']} ({G.nodes[source]['year']})<br>To: {G.nodes[target]['battery_type']} {G.nodes[target]['material']} ({G.nodes[target]['year']})<br>Category: {category_name}",
        showlegend=False
    )
    edge_traces.append(edge_trace)

# Create node traces by material type and battery type
cathode_nodes = []
anode_nodes = []

min_node_size = 8
max_node_size = 35

max_patent_count = max([G.nodes[n]['size'] for n in G.nodes()]) if G.nodes() else 1

def scale_node_size(patent_count):
    return min_node_size + (max_node_size - min_node_size) * ((patent_count / max_patent_count) ** 0.5)

for node in G.nodes():
    x, y = pos[node]
    patent_count = G.nodes[node]['size']
    scaled_size = scale_node_size(patent_count)
    
    hover_text = (
        f"Material: {G.nodes[node]['material']}<br>"
        f"Battery Type: {G.nodes[node]['battery_type']}<br>"
        f"Material Type: {G.nodes[node]['material_type']}<br>"
        f"Year: {G.nodes[node]['year']}<br>"
        f"Patents: {G.nodes[node]['size']}"
    )
    
    node_data = {
        'x': x, 'y': y, 'size': scaled_size,
        'color': G.nodes[node]['color'], 'text': hover_text
    }
    
    if G.nodes[node]['material_type'] == 'cathode':
        cathode_nodes.append(node_data)
    else:
        anode_nodes.append(node_data)

# Create node traces
cathode_trace = go.Scatter(
    x=[node['x'] for node in cathode_nodes],
    y=[node['y'] for node in cathode_nodes],
    mode='markers',
    marker=dict(
        size=[node['size'] for node in cathode_nodes],
        color=[node['color'] for node in cathode_nodes],
        line=dict(width=1, color='black')
    ),
    text=[node['text'] for node in cathode_nodes],
    hoverinfo='text',
    showlegend=False
)

anode_trace = go.Scatter(
    x=[node['x'] for node in anode_nodes],
    y=[node['y'] for node in anode_nodes],
    mode='markers',
    marker=dict(
        size=[node['size'] for node in anode_nodes],
        color=[node['color'] for node in anode_nodes],
        line=dict(width=1, color='black')
    ),
    text=[node['text'] for node in anode_nodes],
    hoverinfo='text',
    showlegend=False
)

# Create legend entries
legend_entries = []

# Material legends
all_colors = {**lib_cathode_colors, **sib_cathode_colors, **lib_anode_colors, **sib_anode_colors}
for material, color in all_colors.items():
    if material in lib_cathode_materials:
        name = f'LIB Cathode: {material}'
    elif material in sib_cathode_materials:
        name = f'SIB Cathode: {material}'
    elif material in lib_anode_materials:
        name = f'LIB Anode: {material}'
    else:
        name = f'SIB Anode: {material}'
    
    legend_entries.append(go.Scatter(
        x=[None], y=[None], mode='markers',
        marker=dict(size=12, color=color),
        name=name, hoverinfo='none'
    ))

# Edge category legends
edge_legend_data = [
    ('LIB Cathode → LIB Anode', 'rgba(120, 120, 120, 0.8)'),
    ('SIB Cathode → SIB Anode', 'rgba(180, 180, 180, 0.8)'),
    ('LIB Cathode → SIB Anode', 'rgba(255, 140, 0, 0.8)'),
    ('SIB Cathode → LIB Anode', 'rgba(255, 165, 0, 0.8)')
]

for name, color in edge_legend_data:
    legend_entries.append(go.Scatter(
        x=[None], y=[None], mode='lines',
        line=dict(color=color, width=4),
        name=name, hoverinfo='none'
    ))

# Create the figure
fig = go.Figure(data=edge_traces + [cathode_trace, anode_trace] + legend_entries)

# Calculate layout parameters
min_year = min([G.nodes[n]['year'] for n in G.nodes()]) if G.nodes() else 2000
max_year = max([G.nodes[n]['year'] for n in G.nodes()]) if G.nodes() else 2025
x_ticks = list(range(min_year, max_year + 1, 5))

# Create y-axis ticks and labels
y_ticks = []
y_tick_labels = []

# Add cathode section label
all_cathode_materials = lib_cathode_order + sib_cathode_order
for i, material in enumerate(all_cathode_materials):
    y_pos = cathode_section_start - i * y_spacing
    y_ticks.append(y_pos)
    if material in lib_cathode_materials:
        y_tick_labels.append(f"LIB Cathode: {material}")
    else:
        y_tick_labels.append(f"SIB Cathode: {material}")

# Add anode section label
all_anode_materials = lib_anode_order + sib_anode_order
for i, material in enumerate(all_anode_materials):
    y_pos = anode_section_start - i * y_spacing
    if i == len(lib_anode_order):  # Add gap before SIB anodes
        y_pos -= 30
    y_ticks.append(y_pos)
    if material in lib_anode_materials:
        y_tick_labels.append(f"LIB Anode: {material}")
    else:
        y_tick_labels.append(f"SIB Anode: {material}")

# Add section dividers
cathode_anode_divider_y = (min(material_y_positions[key] for key in material_y_positions.keys() if key[1] == 'cathode') + 
                          max(material_y_positions[key] for key in material_y_positions.keys() if key[1] == 'anode')) / 2

lib_sib_cathode_divider_y = material_y_positions[('LIB', 'cathode', 'LMO')] - y_spacing/2
lib_sib_anode_divider_y = material_y_positions[('LIB', 'anode', 'LTO')] - y_spacing/2 - 15

# Add divider lines and annotations
fig.add_shape(
    type="line",
    x0=min_year-1, y0=cathode_anode_divider_y,
    x1=max_year+1, y1=cathode_anode_divider_y,
    line=dict(color="black", width=3, dash="solid")
)

fig.add_shape(
    type="line",
    x0=min_year-1, y0=lib_sib_cathode_divider_y,
    x1=max_year+1, y1=lib_sib_cathode_divider_y,
    line=dict(color="gray", width=2, dash="dash")
)

fig.add_shape(
    type="line",
    x0=min_year-1, y0=lib_sib_anode_divider_y,
    x1=max_year+1, y1=lib_sib_anode_divider_y,
    line=dict(color="gray", width=2, dash="dash")
)

# Add section annotations
fig.add_annotation(
    x=min_year-4, y=lib_sib_cathode_divider_y,
    text="POSITIVE ELECTRODE MATERIALS", showarrow=False,
    font=dict(size=18, color="black", family="Arial Black"),
    textangle=-90
)

fig.add_annotation(
    x=min_year-4, y=lib_sib_anode_divider_y,
    text="NEGATIVE ELECTRODE MATERIALS", showarrow=False,
    font=dict(size=18, color="black", family="Arial Black"),
    textangle=-90
)

fig.add_annotation(
    x=min_year-2, y=lib_sib_cathode_divider_y,
    text="LIB | SIB", showarrow=False,
    font=dict(size=18, color="gray"),
    textangle=-90
)

fig.add_annotation(
    x=min_year-2, y=lib_sib_anode_divider_y,
    text="LIB | SIB", showarrow=False,
    font=dict(size=18, color="gray"),
    textangle=-90
)

# Update layout
fig.update_layout(
    title=dict(
        text="Positive Electrode → Negative Electrode Knowledge Flow Network: How Positive Electrode Research Influences Negative Electrode Development",
        font=dict(size=26)
    ),
    showlegend=True,
    xaxis=dict(
        title=dict(text="Priority Year", font=dict(size=22)),
        tickmode='array',
        tickvals=x_ticks,
        ticktext=[str(year) for year in x_ticks],
        tickfont=dict(size=20),
        showgrid=True,
        gridwidth=1,
        gridcolor='lightgrey'
    ),
    yaxis=dict(
        tickmode='array',
        tickvals=y_ticks,
        ticktext=y_tick_labels,
        title=dict(text="Material Type and Specific Material", font=dict(size=22)),
        tickfont=dict(size=20),
        zeroline=False
    ),
    hovermode='closest',
    hoverlabel=dict(font=dict(size=16)),
    plot_bgcolor='rgba(0,0,0,0)',
    paper_bgcolor='rgba(0,0,0,0)',
    legend=dict(
        yanchor="top", y=0.99, xanchor="left", x=1.01,
        bgcolor="rgba(255, 255, 255, 0.9)", 
        font=dict(size=18),
        itemsizing="constant"
    ),
    width=2000,
    height=1200,
    margin=dict(l=200, r=300, t=100, b=80)
)

# Save the visualization
output_file = "Cathode_to_Anode_Patent_Network.html"
fig.write_html(output_file)
print(f"Cathode → Anode visualization saved to {output_file}")

# Print network statistics
print("\nCathode → Anode Citation Network Statistics:")
print(f"Total material-year nodes: {G.number_of_nodes()}")
print(f"Cathode material-year nodes: {len([n for n in G.nodes() if G.nodes[n]['material_type'] == 'cathode'])}")
print(f"Anode material-year nodes: {len([n for n in G.nodes() if G.nodes[n]['material_type'] == 'anode'])}")
print(f"Total cathode → anode citations: {G.number_of_edges()}")

# Count citations by category
citation_categories = {}
for edge in G.edges():
    source, target = edge
    category = get_cathode_to_anode_category_name(source, target)
    if category not in citation_categories:
        citation_categories[category] = 0
    citation_categories[category] += G.edges[edge]['weight']

print("\nCitation counts by category:")
for category, count in sorted(citation_categories.items()):
    print(f"  {category}: {count}")

print("\nCathode → Anode Analysis complete!")

#%%

#==========================================================================================================================
# Building Anode → Cathode citation network showing knowledge flows from anode to cathode materials
#==========================================================================================================================

import pandas as pd
import networkx as nx
import plotly.graph_objects as go
import numpy as np
from datetime import datetime

# File paths for both datasets
lib_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_full_data_classified_onlyLIB_27052025.xlsx"
sib_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_full_data_classified_final_onlySIB_26052025.xlsx"
sheet_name = "Sheet1"

# Load both Excel files
LIB_patents = pd.read_excel(lib_file_path, sheet_name=sheet_name)
SIB_patents = pd.read_excel(sib_file_path, sheet_name=sheet_name)

# Print basic info about both datasets
print(f"LIB dataset shape: {LIB_patents.shape}")
print(f"SIB dataset shape: {SIB_patents.shape}")

# Add a 'battery_type' column to identify the source in the combined dataset
LIB_patents['battery_type'] = 'LIB'
SIB_patents['battery_type'] = 'SIB'

#==============================================================
# Define material sets for both battery types
#==============================================================

# Define LIB cathode materials
lib_cathode_materials = {"LCO", "LMO", "LFP", "NMC", "NCA"}

# Define SIB cathode materials
sib_cathode_materials = {"Fe-PBA", "Mn-PBA", "NVPF", "NFPP/NFP", "NFM", "CFM", "NMO"}

# Define LIB anode materials
lib_anode_materials = {"Silicon/Carbon", "Graphite", "LTO"}

# Define SIB anode materials
sib_anode_materials = {"Hard Carbon"}

# Combine all materials
all_cathode_materials = lib_cathode_materials | sib_cathode_materials
all_anode_materials = lib_anode_materials | sib_anode_materials
all_materials = all_cathode_materials | all_anode_materials

#==============================================================
# Function to extract materials from material strings
#==============================================================

def extract_materials(material_str, material_set):
    """Extract materials that match the given material set from a comma-separated string."""
    materials = set(material_str.split(", ")) if isinstance(material_str, str) else set()
    extracted = list(materials.intersection(material_set))
    return extracted if extracted else ["none"]

def contains_material(material_str, material_set):
    """Check if the material string contains any material from the given set."""
    if not isinstance(material_str, str) or material_str == "none":
        return False
    materials = set(material_str.split(", "))
    return bool(materials.intersection(material_set))

#==============================================================
# Process both datasets for cathode and anode materials
#==============================================================

def process_dataset(patents_df, battery_type, cathode_materials, anode_materials):
    """Process dataset to extract both cathode and anode materials."""
    
    # Extract priority year
    patents_df["earliest_priority_date"] = pd.to_datetime(patents_df["Earliest Priority Date"])
    patents_df["priority_year"] = patents_df["earliest_priority_date"].dt.year
    patents_df = patents_df.sort_values(by=["priority_year"], ascending=True)
    
    # Create material columns
    patents_df["Cathode Material"] = patents_df["Material-Level"].apply(lambda x: extract_materials(x, cathode_materials))
    patents_df["Anode Material"] = patents_df["Material-Level"].apply(lambda x: extract_materials(x, anode_materials))
    
    # Create citation count
    patents_df["family_citing_lens_id_count"] = patents_df["family_citing_lens_id"].apply(
        lambda x: len(x.split(", ")) if isinstance(x, str) and x else 0
    )
    
    # Filter for patents containing cathode materials
    cathode_patents = patents_df[
        patents_df["Material-Level"].apply(lambda x: contains_material(x, cathode_materials)) &
        (patents_df["family_citing_lens_id_count"] > 0) &
        (patents_df["Cathode Material"].apply(lambda x: x != ["none"]))
    ]
    
    # Filter for patents containing anode materials
    anode_patents = patents_df[
        patents_df["Material-Level"].apply(lambda x: contains_material(x, anode_materials)) &
        (patents_df["family_citing_lens_id_count"] > 0) &
        (patents_df["Anode Material"].apply(lambda x: x != ["none"]))
    ]
    
    return cathode_patents, anode_patents

# Process LIB dataset
lib_cathode, lib_anode = process_dataset(LIB_patents, 'LIB', lib_cathode_materials, lib_anode_materials)

# Process SIB dataset
sib_cathode, sib_anode = process_dataset(SIB_patents, 'SIB', sib_cathode_materials, sib_anode_materials)

print(f"LIB cathode patents: {len(lib_cathode)}")
print(f"LIB anode patents: {len(lib_anode)}")
print(f"SIB cathode patents: {len(sib_cathode)}")
print(f"SIB anode patents: {len(sib_anode)}")

#==============================================================
# Create aggregated data for Material-Year nodes
#==============================================================

aggregated_data = []
node_counter = 0

# Process LIB cathode patents
for idx, (_, row) in enumerate(lib_cathode.iterrows()):
    material_list = row["Cathode Material"]
    for material in material_list:
        if material != "none":
            aggregated_data.append({
                'material': material,
                'battery_type': 'LIB',
                'material_type': 'cathode',
                'priority_date': row['earliest_priority_date'],
                'year': row['priority_year'],
                'node_num': node_counter,
                'original_index': idx,
                'dataset_type': 'lib_cathode',
                'family_citing_lens_id': row['family_citing_lens_id']
            })
            node_counter += 1

# Process SIB cathode patents
for idx, (_, row) in enumerate(sib_cathode.iterrows()):
    material_list = row["Cathode Material"]
    for material in material_list:
        if material != "none":
            aggregated_data.append({
                'material': material,
                'battery_type': 'SIB',
                'material_type': 'cathode',
                'priority_date': row['earliest_priority_date'],
                'year': row['priority_year'],
                'node_num': node_counter,
                'original_index': idx + len(lib_cathode),
                'dataset_type': 'sib_cathode',
                'family_citing_lens_id': row['family_citing_lens_id']
            })
            node_counter += 1

# Process LIB anode patents
for idx, (_, row) in enumerate(lib_anode.iterrows()):
    material_list = row["Anode Material"]
    for material in material_list:
        if material != "none":
            aggregated_data.append({
                'material': material,
                'battery_type': 'LIB',
                'material_type': 'anode',
                'priority_date': row['earliest_priority_date'],
                'year': row['priority_year'],
                'node_num': node_counter,
                'original_index': idx + len(lib_cathode) + len(sib_cathode),
                'dataset_type': 'lib_anode',
                'family_citing_lens_id': row['family_citing_lens_id']
            })
            node_counter += 1

# Process SIB anode patents
for idx, (_, row) in enumerate(sib_anode.iterrows()):
    material_list = row["Anode Material"]
    for material in material_list:
        if material != "none":
            aggregated_data.append({
                'material': material,
                'battery_type': 'SIB',
                'material_type': 'anode',
                'priority_date': row['earliest_priority_date'],
                'year': row['priority_year'],
                'node_num': node_counter,
                'original_index': idx + len(lib_cathode) + len(sib_cathode) + len(lib_anode),
                'dataset_type': 'sib_anode',
                'family_citing_lens_id': row['family_citing_lens_id']
            })
            node_counter += 1

# Create a DataFrame from the aggregated data
agg_df = pd.DataFrame(aggregated_data)

# Group by year, battery type, material type, and material to count patents
node_data = agg_df.groupby(['year', 'battery_type', 'material_type', 'material']).size().reset_index(name='patent_count')

#==============================================================
# Create node and edge mappings for the network
#==============================================================

# Create a directed graph
G = nx.DiGraph()

# Create a mapping of (year, battery_type, material_type, material) to node ID
node_mapping = {}
for i, row in node_data.iterrows():
    node_mapping[(row['year'], row['battery_type'], row['material_type'], row['material'])] = i

# Store the reverse mapping for later access
id_to_node = {i: (row['year'], row['battery_type'], row['material_type'], row['material']) for i, row in node_data.iterrows()}

#==============================================================
# Create patent citation networks
#==============================================================

# Create consistent combined dataset maintaining node numbering
lib_cathode_copy = lib_cathode.copy().reset_index(drop=True)
sib_cathode_copy = sib_cathode.copy().reset_index(drop=True)
lib_anode_copy = lib_anode.copy().reset_index(drop=True)
sib_anode_copy = sib_anode.copy().reset_index(drop=True)

# Set consistent indices
sib_cathode_copy.index = sib_cathode_copy.index + len(lib_cathode)
lib_anode_copy.index = lib_anode_copy.index + len(lib_cathode) + len(sib_cathode)
sib_anode_copy.index = sib_anode_copy.index + len(lib_cathode) + len(sib_cathode) + len(lib_anode)

combined_patents = pd.concat([lib_cathode_copy, sib_cathode_copy, lib_anode_copy, sib_anode_copy])

# Create a dictionary to map lens IDs to node numbers
lens_id_to_node = {}
for i, row in combined_patents.iterrows():
    # First try Simple Family Members
    if isinstance(row.get('Simple Family Members'), str) and row['Simple Family Members'].strip():
        for lens_id in row['Simple Family Members'].split(';;'):
            lens_id = lens_id.strip()
            if lens_id:
                lens_id_to_node[lens_id] = i
    # If no Simple Family Members, use the Lens ID itself
    elif isinstance(row.get('Lens ID'), str) and row['Lens ID'].strip():
        lens_id_to_node[row['Lens ID']] = i

# Extract citation links - ONLY ANODE → CATHODE citations
edge_data = []
for i, row in combined_patents.iterrows():
    if isinstance(row['family_citing_lens_id'], str) and row['family_citing_lens_id']:
        for citing_id in row['family_citing_lens_id'].split(', '):
            citing_id = citing_id.strip()
            if citing_id and citing_id in lens_id_to_node:
                cited_node = i
                citing_node = lens_id_to_node[citing_id]
                
                # Get material information directly from combined_patents
                cited_row = combined_patents.loc[cited_node]
                citing_row = combined_patents.loc[citing_node]
                
                # Extract materials from both patents
                cited_materials = []
                citing_materials = []
                
                # Get cited patent materials
                if isinstance(cited_row.get('Cathode Material'), list):
                    cited_materials.extend([(m, 'cathode') for m in cited_row['Cathode Material'] if m != 'none'])
                if isinstance(cited_row.get('Anode Material'), list):
                    cited_materials.extend([(m, 'anode') for m in cited_row['Anode Material'] if m != 'none'])
                
                # Get citing patent materials  
                if isinstance(citing_row.get('Cathode Material'), list):
                    citing_materials.extend([(m, 'cathode') for m in citing_row['Cathode Material'] if m != 'none'])
                if isinstance(citing_row.get('Anode Material'), list):
                    citing_materials.extend([(m, 'anode') for m in citing_row['Anode Material'] if m != 'none'])
                
                # Create edges ONLY for ANODE → CATHODE citations
                for citing_material, citing_mat_type in citing_materials:
                    for cited_material, cited_mat_type in cited_materials:
                        if citing_mat_type == 'anode' and cited_mat_type == 'cathode':  # ONLY anode → cathode
                            
                            # Find corresponding nodes in the graph
                            citing_node_key = (citing_row['priority_year'], citing_row['battery_type'], citing_mat_type, citing_material)
                            cited_node_key = (cited_row['priority_year'], cited_row['battery_type'], cited_mat_type, cited_material)
                            
                            citing_graph_node = node_mapping.get(citing_node_key)
                            cited_graph_node = node_mapping.get(cited_node_key)
                            
                            if citing_graph_node is not None and cited_graph_node is not None and citing_graph_node != cited_graph_node:
                                edge_data.append((citing_graph_node, cited_graph_node))

# Count edge frequencies
edge_weights = {}
for edge in edge_data:
    if edge in edge_weights:
        edge_weights[edge] += 1
    else:
        edge_weights[edge] = 1

print(f"Total anode → cathode citations found: {len(edge_data)}")
print(f"Unique anode → cathode citation edges: {len(edge_weights)}")

#==============================================================
# Create the network visualization
#==============================================================

# Define color mappings for materials (same as before)
lib_cathode_colors = {
    'LCO': '#FF0000',  'NMC': '#B22222',  'NCA': '#FF6B6B',
    'LFP': '#0000FF',  'LMO': '#00FFFF'
}

sib_cathode_colors = {
    'CFM': '#FF4500',  'NFM': '#DC143C',  'NMO': '#FF69B4',
    'NVPF': '#0066FF',  'NFPP/NFP': '#000080',
    'Fe-PBA': '#00CC00',  'Mn-PBA': '#006600'
}

lib_anode_colors = {
    'LTO': '#FFA500',  'Silicon/Carbon': '#666666',  'Graphite': '#333333'
}

sib_anode_colors = {
    'Hard Carbon': '#999999'
}

# Add nodes to the graph
for i, row in node_data.iterrows():
    year = row['year']
    battery_type = row['battery_type']
    material_type = row['material_type']
    material = row['material']
    patent_count = row['patent_count']
    
    # Select color based on battery type and material
    if battery_type == 'LIB' and material_type == 'cathode':
        color = lib_cathode_colors.get(material, 'grey')
    elif battery_type == 'SIB' and material_type == 'cathode':
        color = sib_cathode_colors.get(material, 'grey')
    elif battery_type == 'LIB' and material_type == 'anode':
        color = lib_anode_colors.get(material, 'grey')
    else:  # SIB anode
        color = sib_anode_colors.get(material, 'grey')
    
    # Add node with attributes
    G.add_node(i,
               year=year,
               battery_type=battery_type,
               material_type=material_type,
               material=material,
               size=patent_count,
               color=color)

# Filter edge_weights for only existing nodes
valid_edge_weights = {}
for (source, target), weight in edge_weights.items():
    if source in G.nodes() and target in G.nodes():
        valid_edge_weights[(source, target)] = weight

edge_weights = valid_edge_weights

# Add edges with weights
for (source, target), weight in edge_weights.items():
    G.add_edge(source, target, weight=weight)

#==============================================================
# Calculate node positions with the specified layout
#==============================================================

# Define material orders
lib_cathode_order = ['LCO', 'NMC', 'NCA', 'LFP', 'LMO']
sib_cathode_order = ['Fe-PBA', 'Mn-PBA', 'NVPF', 'NFPP/NFP', 'NFM', 'CFM', 'NMO']
lib_anode_order = ['Graphite', 'Silicon/Carbon', 'LTO']
sib_anode_order = ['Hard Carbon']

# Y-positions for materials
y_spacing = 60
total_cathode_materials = len(lib_cathode_order) + len(sib_cathode_order)
total_anode_materials = len(lib_anode_order) + len(sib_anode_order)

# Calculate positions to avoid overlap
cathode_section_start = 500
cathode_section_end = cathode_section_start - (total_cathode_materials * y_spacing)
gap_between_sections = 100
anode_section_start = cathode_section_end - gap_between_sections
anode_section_end = anode_section_start - (total_anode_materials * y_spacing) - 50

material_y_positions = {}

# Cathode materials (top section)
y_pos = cathode_section_start
for material in lib_cathode_order:
    material_y_positions[('LIB', 'cathode', material)] = y_pos
    y_pos -= y_spacing

for material in sib_cathode_order:
    material_y_positions[('SIB', 'cathode', material)] = y_pos
    y_pos -= y_spacing

# Anode materials (bottom section)
y_pos = anode_section_start
for material in lib_anode_order:
    material_y_positions[('LIB', 'anode', material)] = y_pos
    y_pos -= y_spacing

# Gap for SIB anodes
y_pos -= 50
for material in sib_anode_order:
    material_y_positions[('SIB', 'anode', material)] = y_pos
    y_pos -= y_spacing

# Calculate positions for each node
pos = {}
for node_id, (year, battery_type, material_type, material) in id_to_node.items():
    x = year
    y = material_y_positions.get((battery_type, material_type, material), 0)
    pos[node_id] = (x, y)

#==============================================================
# Define edge colors for anode → cathode citations
#==============================================================

def get_anode_to_cathode_edge_color(source_id, target_id):
    """Determine edge color for anode → cathode citations."""
    source_battery = G.nodes[source_id]['battery_type']
    target_battery = G.nodes[target_id]['battery_type']
    
    # 4 categories for anode → cathode
    if source_battery == 'LIB' and target_battery == 'LIB':
        return 'rgba(120, 120, 120, 0.8)'  # Dark Grey - LIB Anode → LIB Cathode
    elif source_battery == 'SIB' and target_battery == 'SIB':
        return 'rgba(180, 180, 180, 0.8)'  # Light Grey - SIB Anode → SIB Cathode
    elif source_battery == 'LIB' and target_battery == 'SIB':
        return 'rgba(255, 140, 0, 0.8)'    # Orange - LIB Anode → SIB Cathode
    else:  # SIB → LIB
        return 'rgba(255, 165, 0, 0.8)'    # Light Orange - SIB Anode → LIB Cathode

def get_anode_to_cathode_category_name(source_id, target_id):
    """Get descriptive name for anode → cathode edge category."""
    source_battery = G.nodes[source_id]['battery_type']
    target_battery = G.nodes[target_id]['battery_type']
    
    categories = {
        ('LIB', 'LIB'): 'LIB Anode → LIB Cathode',
        ('SIB', 'SIB'): 'SIB Anode → SIB Cathode', 
        ('LIB', 'SIB'): 'LIB Anode → SIB Cathode',
        ('SIB', 'LIB'): 'SIB Anode → LIB Cathode'
    }
    
    return categories.get((source_battery, target_battery), 'Unknown')

#==============================================================
# Create the Plotly visualization
#==============================================================

# Create edge traces
edge_traces = []
min_width = 1.0
max_width = 6.0

# Find maximum edge weight for scaling
max_weight = max([G.edges[edge]['weight'] for edge in G.edges()]) if G.edges() else 1

def scale_edge_width(weight):
    return min_width + (max_width - min_width) * ((weight / max_weight) ** 0.5)

# Create edge traces
for edge in G.edges():
    source, target = edge
    x0, y0 = pos[source]
    x1, y1 = pos[target]
    weight = G.edges[source, target]['weight']
    width = scale_edge_width(weight)
    
    edge_color = get_anode_to_cathode_edge_color(source, target)
    category_name = get_anode_to_cathode_category_name(source, target)
    
    edge_trace = go.Scatter(
        x=[x0, x1, None], 
        y=[y0, y1, None],
        mode='lines',
        line=dict(width=width, color=edge_color),
        hoverinfo='text',
        text=f"Citations: {weight}<br>From: {G.nodes[source]['battery_type']} {G.nodes[source]['material']} ({G.nodes[source]['year']})<br>To: {G.nodes[target]['battery_type']} {G.nodes[target]['material']} ({G.nodes[target]['year']})<br>Category: {category_name}",
        showlegend=False
    )
    edge_traces.append(edge_trace)

# Create node traces by material type and battery type
cathode_nodes = []
anode_nodes = []

min_node_size = 8
max_node_size = 35

max_patent_count = max([G.nodes[n]['size'] for n in G.nodes()]) if G.nodes() else 1

def scale_node_size(patent_count):
    return min_node_size + (max_node_size - min_node_size) * ((patent_count / max_patent_count) ** 0.5)

for node in G.nodes():
    x, y = pos[node]
    patent_count = G.nodes[node]['size']
    scaled_size = scale_node_size(patent_count)
    
    hover_text = (
        f"Material: {G.nodes[node]['material']}<br>"
        f"Battery Type: {G.nodes[node]['battery_type']}<br>"
        f"Material Type: {G.nodes[node]['material_type']}<br>"
        f"Year: {G.nodes[node]['year']}<br>"
        f"Patents: {G.nodes[node]['size']}"
    )
    
    node_data = {
        'x': x, 'y': y, 'size': scaled_size,
        'color': G.nodes[node]['color'], 'text': hover_text
    }
    
    if G.nodes[node]['material_type'] == 'cathode':
        cathode_nodes.append(node_data)
    else:
        anode_nodes.append(node_data)

# Create node traces
cathode_trace = go.Scatter(
    x=[node['x'] for node in cathode_nodes],
    y=[node['y'] for node in cathode_nodes],
    mode='markers',
    marker=dict(
        size=[node['size'] for node in cathode_nodes],
        color=[node['color'] for node in cathode_nodes],
        line=dict(width=1, color='black')
    ),
    text=[node['text'] for node in cathode_nodes],
    hoverinfo='text',
    showlegend=False
)

anode_trace = go.Scatter(
    x=[node['x'] for node in anode_nodes],
    y=[node['y'] for node in anode_nodes],
    mode='markers',
    marker=dict(
        size=[node['size'] for node in anode_nodes],
        color=[node['color'] for node in anode_nodes],
        line=dict(width=1, color='black')
    ),
    text=[node['text'] for node in anode_nodes],
    hoverinfo='text',
    showlegend=False
)

# Create legend entries
legend_entries = []

# Material legends
all_colors = {**lib_cathode_colors, **sib_cathode_colors, **lib_anode_colors, **sib_anode_colors}
for material, color in all_colors.items():
    if material in lib_cathode_materials:
        name = f'LIB Cathode: {material}'
    elif material in sib_cathode_materials:
        name = f'SIB Cathode: {material}'
    elif material in lib_anode_materials:
        name = f'LIB Anode: {material}'
    else:
        name = f'SIB Anode: {material}'
    
    legend_entries.append(go.Scatter(
        x=[None], y=[None], mode='markers',
        marker=dict(size=12, color=color),
        name=name, hoverinfo='none'
    ))

# Edge category legends
edge_legend_data = [
    ('LIB Anode → LIB Cathode', 'rgba(120, 120, 120, 0.8)'),
    ('SIB Anode → SIB Cathode', 'rgba(180, 180, 180, 0.8)'),
    ('LIB Anode → SIB Cathode', 'rgba(255, 140, 0, 0.8)'),
    ('SIB Anode → LIB Cathode', 'rgba(255, 165, 0, 0.8)')
]

for name, color in edge_legend_data:
    legend_entries.append(go.Scatter(
        x=[None], y=[None], mode='lines',
        line=dict(color=color, width=4),
        name=name, hoverinfo='none'
    ))

# Create the figure
fig = go.Figure(data=edge_traces + [cathode_trace, anode_trace] + legend_entries)

# Calculate layout parameters
min_year = min([G.nodes[n]['year'] for n in G.nodes()]) if G.nodes() else 2000
max_year = max([G.nodes[n]['year'] for n in G.nodes()]) if G.nodes() else 2025
x_ticks = list(range(min_year, max_year + 1, 5))

# Create y-axis ticks and labels
y_ticks = []
y_tick_labels = []

# Add cathode section label
all_cathode_materials = lib_cathode_order + sib_cathode_order
for i, material in enumerate(all_cathode_materials):
    y_pos = cathode_section_start - i * y_spacing
    y_ticks.append(y_pos)
    if material in lib_cathode_materials:
        y_tick_labels.append(f"LIB Cathode: {material}")
    else:
        y_tick_labels.append(f"SIB Cathode: {material}")

# Add anode section label
all_anode_materials = lib_anode_order + sib_anode_order
for i, material in enumerate(all_anode_materials):
    y_pos = anode_section_start - i * y_spacing
    if i == len(lib_anode_order):  # Add gap before SIB anodes
        y_pos -= 30
    y_ticks.append(y_pos)
    if material in lib_anode_materials:
        y_tick_labels.append(f"LIB Anode: {material}")
    else:
        y_tick_labels.append(f"SIB Anode: {material}")

# Add section dividers
cathode_anode_divider_y = (min(material_y_positions[key] for key in material_y_positions.keys() if key[1] == 'cathode') + 
                          max(material_y_positions[key] for key in material_y_positions.keys() if key[1] == 'anode')) / 2

lib_sib_cathode_divider_y = material_y_positions[('LIB', 'cathode', 'LMO')] - y_spacing/2
lib_sib_anode_divider_y = material_y_positions[('LIB', 'anode', 'LTO')] - y_spacing/2 - 15

# Add divider lines and annotations
fig.add_shape(
    type="line",
    x0=min_year-1, y0=cathode_anode_divider_y,
    x1=max_year+1, y1=cathode_anode_divider_y,
    line=dict(color="black", width=3, dash="solid")
)

fig.add_shape(
    type="line",
    x0=min_year-1, y0=lib_sib_cathode_divider_y,
    x1=max_year+1, y1=lib_sib_cathode_divider_y,
    line=dict(color="gray", width=2, dash="dash")
)

fig.add_shape(
    type="line",
    x0=min_year-1, y0=lib_sib_anode_divider_y,
    x1=max_year+1, y1=lib_sib_anode_divider_y,
    line=dict(color="gray", width=2, dash="dash")
)


# Add section annotations
fig.add_annotation(
    x=min_year-4, y=lib_sib_cathode_divider_y,
    text="POSITIVE ELECTRODE MATERIALS", showarrow=False,
    font=dict(size=18, color="black", family="Arial Black"),
    textangle=-90
)

fig.add_annotation(
    x=min_year-4, y=lib_sib_anode_divider_y,
    text="NEGATIVE ELECTRODE MATERIALS", showarrow=False,
    font=dict(size=18, color="black", family="Arial Black"),
    textangle=-90
)

fig.add_annotation(
    x=min_year-2, y=lib_sib_cathode_divider_y,
    text="LIB | SIB", showarrow=False,
    font=dict(size=18, color="gray"),
    textangle=-90
)

fig.add_annotation(
    x=min_year-2, y=lib_sib_anode_divider_y,
    text="LIB | SIB", showarrow=False,
    font=dict(size=18, color="gray"),
    textangle=-90
)

# Update layout
fig.update_layout(
    title=dict(
        text="Negative Electrode → Positive Electrode Knowledge Flow Network: How Negative Electrode Research Influences Positive Electrode Development",
        font=dict(size=26)
    ),
    showlegend=True,
    xaxis=dict(
        title=dict(text="Priority Year", font=dict(size=22)),
        tickmode='array',
        tickvals=x_ticks,
        ticktext=[str(year) for year in x_ticks],
        tickfont=dict(size=20),
        showgrid=True,
        gridwidth=1,
        gridcolor='lightgrey'
    ),
    yaxis=dict(
        tickmode='array',
        tickvals=y_ticks,
        ticktext=y_tick_labels,
        title=dict(text="Material Type and Specific Material", font=dict(size=22)),
        tickfont=dict(size=20),
        zeroline=False
    ),
    hovermode='closest',
    hoverlabel=dict(font=dict(size=16)),
    plot_bgcolor='rgba(0,0,0,0)',
    paper_bgcolor='rgba(0,0,0,0)',
    legend=dict(
        yanchor="top", y=0.99, xanchor="left", x=1.01,
        bgcolor="rgba(255, 255, 255, 0.9)", 
        font=dict(size=18),
        itemsizing="constant"
    ),
    width=2000,
    height=1200,
    margin=dict(l=200, r=300, t=100, b=80)
)


# Save the visualization
output_file = "Anode_to_Cathode_Patent_Network.html"
fig.write_html(output_file)
print(f"Anode → Cathode visualization saved to {output_file}")

# Print network statistics
print("\nAnode → Cathode Citation Network Statistics:")
print(f"Total material-year nodes: {G.number_of_nodes()}")
print(f"Cathode material-year nodes: {len([n for n in G.nodes() if G.nodes[n]['material_type'] == 'cathode'])}")
print(f"Anode material-year nodes: {len([n for n in G.nodes() if G.nodes[n]['material_type'] == 'anode'])}")
print(f"Total anode → cathode citations: {G.number_of_edges()}")

# Count citations by category
citation_categories = {}
for edge in G.edges():
    source, target = edge
    category = get_anode_to_cathode_category_name(source, target)
    if category not in citation_categories:
        citation_categories[category] = 0
    citation_categories[category] += G.edges[edge]['weight']

print("\nCitation counts by category:")
for category, count in sorted(citation_categories.items()):
    print(f"  {category}: {count}")

print("\nAnode → Cathode Analysis complete!")


#%%

#==================================================================================================================
# Overall network of LIB and SIB dataset with cathode and anode material color and shape codes, respectively
#==================================================================================================================

import pandas as pd
import networkx as nx
import plotly.graph_objects as go
import re
import numpy as np
from datetime import datetime

# File paths for both datasets
lib_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_full_data_classified_onlyLIB_27052025.xlsx"
sib_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_full_data_classified_final_onlySIB_26052025.xlsx"
sheet_name = "Sheet1"

# Load both Excel files
LIB_patents = pd.read_excel(lib_file_path, sheet_name=sheet_name)
SIB_patents = pd.read_excel(sib_file_path, sheet_name=sheet_name)

# Print basic info about both datasets
print(f"LIB dataset shape: {LIB_patents.shape}")
print(f"SIB dataset shape: {SIB_patents.shape}")

# Add a 'battery_type' column to identify the source in the combined dataset
LIB_patents['battery_type'] = 'LIB'
SIB_patents['battery_type'] = 'SIB'

#==============================================================
# Define material sets for both battery types
#==============================================================

# Define LIB cathode materials
lib_cathode_materials = {"LCO", "LMO", "LFP", "NMC", "NCA"}
lib_anode_materials = {"Silicon/Carbon", "Graphite", "LTO"}

# Define SIB cathode materials
sib_cathode_materials = {"Fe-PBA", "Mn-PBA", "NVPF", "NFPP/NFP", "NFM", "CFM", "NMO", "other cathode"}
sib_anode_materials = {"Hard Carbon", "other anode"}

#==============================================================
# Function to extract materials from material strings
#==============================================================

def extract_materials(material_str, material_set):
    """Extract materials that match the given material set from a comma-separated string."""
    materials = set(material_str.split(", ")) if isinstance(material_str, str) else set()
    extracted = list(materials.intersection(material_set))
    return extracted if extracted else ["none"]

#==============================================================
# Process LIB dataset
#==============================================================

# Extract priority year from earliest_priority_date for LIB
LIB_patents["earliest_priority_date"] = pd.to_datetime(LIB_patents["Earliest Priority Date"])
LIB_patents["priority_year"] = LIB_patents["earliest_priority_date"].dt.year
LIB_patents = LIB_patents.sort_values(by=["priority_year"], ascending=True)

# Create new columns for Cathode and Anode Materials for LIB
LIB_patents["Cathode Material"] = LIB_patents["Material-Level"].apply(lambda x: extract_materials(x, lib_cathode_materials))
LIB_patents["Anode Material"] = LIB_patents["Material-Level"].apply(lambda x: extract_materials(x, lib_anode_materials))

# Update Component-Level based on Material-Level for LIB
for index, row in LIB_patents.iterrows():
    if row["Material-Level"] != "none" and row["Component-Level"] == "none":
        materials = set(row["Material-Level"].split(", ")) if isinstance(row["Material-Level"], str) else set()

        cathode_match = materials & lib_cathode_materials
        anode_match = materials & lib_anode_materials

        new_component_level = []
        if cathode_match:
            new_component_level.append("Positive Electrode")
        if anode_match:
            new_component_level.append("Negative Electrode")

        LIB_patents.at[index, "Component-Level"] = ", ".join(new_component_level) if new_component_level else "none"

# Create family_citing_lens_id_count for LIB dataset by counting citations
LIB_patents["family_citing_lens_id_count"] = LIB_patents["family_citing_lens_id"].apply(
    lambda x: len(x.split(", ")) if isinstance(x, str) and x else 0
)

#==============================================================
# Process SIB dataset
#==============================================================

# Extract priority year from earliest_priority_date for SIB
SIB_patents["earliest_priority_date"] = pd.to_datetime(SIB_patents["Earliest Priority Date"])
SIB_patents["priority_year"] = SIB_patents["earliest_priority_date"].dt.year
SIB_patents = SIB_patents.sort_values(by=["priority_year"], ascending=True)

# Create new columns for Cathode and Anode Materials for SIB
SIB_patents["Cathode Material"] = SIB_patents["Material-Level"].apply(lambda x: extract_materials(x, sib_cathode_materials))
SIB_patents["Anode Material"] = SIB_patents["Material-Level"].apply(lambda x: extract_materials(x, sib_anode_materials))

# Update Component-Level based on Material-Level for SIB
for index, row in SIB_patents.iterrows():
    if row["Material-Level"] != "none" and row["Component-Level"] == "none":
        materials = set(row["Material-Level"].split(", ")) if isinstance(row["Material-Level"], str) else set()

        cathode_match = materials & sib_cathode_materials
        anode_match = materials & sib_anode_materials

        new_component_level = []
        if cathode_match:
            new_component_level.append("Positive Electrode")
        if anode_match:
            new_component_level.append("Negative Electrode")

        SIB_patents.at[index, "Component-Level"] = ", ".join(new_component_level) if new_component_level else "none"

# Create family_citing_lens_id_count for SIB dataset by counting citations
SIB_patents["family_citing_lens_id_count"] = SIB_patents["family_citing_lens_id"].apply(
    lambda x: len(x.split(", ")) if isinstance(x, str) and x else 0
)

#==============================================================
# Filter patents with citations and apply further processing
#==============================================================

# Filter LIB patents with at least one citation
lib_df = LIB_patents[LIB_patents["family_citing_lens_id_count"] > 0]

# Filter SIB patents with at least one citation
sib_df = SIB_patents[SIB_patents["family_citing_lens_id_count"] > 0]

# Extract lists of citing patents
lib_df["List of Citing Patents"] = lib_df["family_citing_lens_id"].fillna("")
lib_df["List of Citing Patents"] = lib_df["family_citing_lens_id"].apply(
    lambda x: [p.strip() for p in x.split(", ") if p.strip()]
)

sib_df["List of Citing Patents"] = sib_df["family_citing_lens_id"].fillna("")
sib_df["List of Citing Patents"] = sib_df["family_citing_lens_id"].apply(
    lambda x: [p.strip() for p in x.split(", ") if p.strip()]
)

# Remove patents that have neither material nor component information
lib_df = lib_df[~((lib_df['Material-Level'] == 'none') & (lib_df['Component-Level'] == 'none'))]
sib_df = sib_df[~((sib_df['Material-Level'] == 'none') & (sib_df['Component-Level'] == 'none'))]

#==============================================================
# Combine datasets for the network graph
#==============================================================

# Add origin identifiers to node numbers to prevent conflicts
lib_df["original_index"] = lib_df.index
sib_df["original_index"] = sib_df.index

# Combine datasets
combined_df = pd.concat([lib_df, sib_df], ignore_index=True)

# Create a continuous node numbering for the combined dataset
combined_df["node_num"] = range(len(combined_df))

#==============================================================
# Create mapping from Lens IDs to node numbers
#==============================================================

# Build dictionary to map Lens IDs to node numbers
lens_id_to_node = {}

# Process each patent family to map all IDs to their node number
for index, row in combined_df.iterrows():
    # Extract lens IDs from Simple Family Members
    if isinstance(row['Simple Family Members'], str):
        for lens_id in row['Simple Family Members'].split(';;'):
            lens_id = lens_id.strip()
            if lens_id:  # Ensure non-empty string
                lens_id_to_node[lens_id] = row["node_num"]

# Function to find node numbers for citing patents
def find_citing_nodes(citing_list):
    """Convert a list of citing patent IDs to their corresponding node numbers."""
    citing_nodes = set()
    for citing_patent in citing_list:
        if citing_patent in lens_id_to_node:
            citing_nodes.add(lens_id_to_node[citing_patent])
    return list(citing_nodes)

# Add citing node numbers to the dataframe
combined_df["Citing_node_numbers"] = combined_df["List of Citing Patents"].apply(find_citing_nodes)

#==============================================================
# Filter to include only patents involved in citation relationships
#==============================================================

# Get all nodes that are cited by other nodes in our dataset
all_citing_nodes = set(combined_df["Citing_node_numbers"].explode().dropna())

# Filter to keep only patents that cite or are cited
filtered_rows = []
for _, row in combined_df.iterrows():
    node = row["node_num"]
    citing_nodes = row["Citing_node_numbers"]
    
    # Include node if it's cited by another node or cites other nodes
    if node in all_citing_nodes or citing_nodes:
        filtered_rows.append(row)

# Create the filtered dataframe and reset indices
combined_df = pd.DataFrame(filtered_rows).reset_index(drop=True)

#==============================================================
# Define color and symbol mappings for visualization
#==============================================================

# Function to normalize node sizes
def normalize_size(value, min_size=5, max_size=25):
    """Scale the node size based on citation count."""
    min_val, max_val = combined_df["family_citing_lens_id_count"].min(), combined_df["family_citing_lens_id_count"].max()
    # Using sqrt for better visual scaling
    return min_size + (max_size - min_size) * ((value - min_val) / (max_val - min_val)) ** 0.5

# Define color mappings for cathode materials
# LIB cathode materials
lib_cathode_colors = {
    'LFP': 'rgba(0, 0, 255, 1)',      # Blue
    'NMC': 'rgba(0, 255, 0, 1)',       # Green
    'LCO': 'rgba(255, 0, 0, 1)',       # Red
    'LMO': 'rgba(0, 255, 255, 1)',     # Cyan
    'NCA': 'rgba(128, 0, 128, 1)',     # Purple
    'none': 'rgba(169, 169, 169, 1)'   # Grey
}

# SIB cathode materials
sib_cathode_colors = {
    'Fe-PBA': 'rgba(0, 0, 139, 1)',    # Dark Blue
    'Mn-PBA': 'rgba(0, 100, 0, 1)',    # Dark Green
    'NVPF': 'rgba(139, 0, 0, 1)',      # Dark Red
    'NFPP/NFP': 'rgba(139, 0, 139, 1)', # Dark Magenta
    'NFM': 'rgba(0, 139, 139, 1)',      # Dark Cyan
    'CFM': 'rgba(255, 140, 0, 1)',      # Dark Orange
    'NMO': 'rgba(70, 70, 70, 1)',       # Dark Grey
    'other cathode': 'rgba(169, 169, 169, 0.7)',  # Grey
    'none': 'rgba(169, 169, 169, 1)'   # Grey
}

# Function to mix colors for multiple cathode materials
def mix_colors(materials, battery_type):
    """Mix colors for multiple cathode materials in a patent."""
    # Choose the appropriate color map based on battery type
    color_map = lib_cathode_colors if battery_type == 'LIB' else sib_cathode_colors
    
    # If materials is a list with one element or only 'none', return the color for that material
    if len(materials) == 1:
        return color_map.get(materials[0], 'rgba(169, 169, 169, 1)')  # Default grey
    
    # Filter out 'none' entries
    valid_materials = [mat for mat in materials if mat != 'none']
    
    # If no valid materials remain, return grey
    if not valid_materials:
        return 'rgba(169, 169, 169, 1)'
    
    # If only one valid material remains after filtering
    if len(valid_materials) == 1:
        return color_map.get(valid_materials[0], 'rgba(169, 169, 169, 1)')
    
    # For multiple materials, mix the colors
    rgb_colors = []
    for mat in valid_materials:
        color = color_map.get(mat, 'rgba(169, 169, 169, 1)')
        match = re.match(r'rgba\((\d+), (\d+), (\d+)', color)
        if match:
            rgb_colors.append([int(match.group(1)), 
                              int(match.group(2)), 
                              int(match.group(3))])
    
    # Average the colors
    if rgb_colors:
        avg_r = sum(color[0] for color in rgb_colors) // len(rgb_colors)
        avg_g = sum(color[1] for color in rgb_colors) // len(rgb_colors)
        avg_b = sum(color[2] for color in rgb_colors) // len(rgb_colors)
        
        return f'rgba({avg_r}, {avg_g}, {avg_b}, 1)'
    
    return 'rgba(169, 169, 169, 1)'  # Default grey

# Define shape mappings for anode materials by battery type
# Using different symbols for LIB and SIB to help differentiate
def get_node_shape(anode_materials, battery_type):
    """Determine the node shape based on anode materials and battery type."""
    if battery_type == 'LIB':
        # LIB anode material combinations
        has_graphite = 'Graphite' in anode_materials
        has_silicon = 'Silicon/Carbon' in anode_materials
        has_lto = 'LTO' in anode_materials
        
        if has_graphite and has_silicon and has_lto:
            return 'star'
        elif has_graphite and has_silicon:
            return 'diamond-tall'
        elif has_graphite and has_lto:
            return 'hexagon'
        elif has_silicon and has_lto:
            return 'triangle-down'
        elif has_silicon:
            return 'diamond'
        elif has_lto:
            return 'triangle-up'
        elif has_graphite:
            return 'square'
        else:
            return 'circle'  # Default for no anode material or 'none'
    else:  # SIB
        # SIB anode material combinations
        has_hard_carbon = 'Hard Carbon' in anode_materials
        has_other_anode = 'other anode' in anode_materials
        
        if has_hard_carbon and has_other_anode:
            return 'cross'
        elif has_hard_carbon:
            return 'x'
        elif has_other_anode:
            return 'cross-thin'
        else:
            return 'circle-open'  # Default for no anode material or 'none'

#==============================================================
# Create the Network Graph
#==============================================================

# Create a directed graph
G = nx.DiGraph()

# Add nodes with metadata
for _, row in combined_df.iterrows():
    node_num = row["node_num"]
    battery_type = row["battery_type"]
    cathode_materials = row["Cathode Material"]
    anode_materials = row["Anode Material"]
    
    # Generate color based on cathode materials and battery type
    color = mix_colors(cathode_materials, battery_type)
    
    # Calculate node size based on citation count
    size = normalize_size(row["family_citing_lens_id_count"])
    
    # Determine shape based on anode materials and battery type
    shape = get_node_shape(anode_materials, battery_type)
    
    # Get the cathode materials string for grouping (ignoring 'none')
    cathode_str = ", ".join(sorted([m for m in cathode_materials if m != 'none']))
    if not cathode_str:
        cathode_str = "none"
    
    # Get the anode materials string for grouping (ignoring 'none')
    anode_str = ", ".join(sorted([m for m in anode_materials if m != 'none']))
    if not anode_str:
        anode_str = "none"
    
    # Combine battery type, cathode and anode materials for the group
    material_group = f"{battery_type} | C: {cathode_str} | A: {anode_str}"
    
    # Add node with all attributes
    G.add_node(
        node_num,
        year=row["earliest_priority_date"],
        battery_type=battery_type,
        color=color,
        size=size,
        shape=shape,
        cathode=cathode_materials,
        anode=anode_materials,
        material_group=material_group,
        cathode_str=cathode_str,
        anode_str=anode_str
    )
    
    # Add edges for citations
    for citing_patent in row["Citing_node_numbers"]:
        if citing_patent in combined_df["node_num"].values and citing_patent != node_num:
            G.add_edge(node_num, citing_patent)  # From cited to citing

#==============================================================
# Custom sort function for material groups
#==============================================================

def custom_material_sort(material_group):
    """Sort material groups by battery type, then cathode, then anode."""
    # Split the material group string
    parts = material_group.split(" | ")
    battery_type = parts[0]
    cathode_part = parts[1].replace("C: ", "")
    anode_part = parts[2].replace("A: ", "")
    
    # Battery type priority (LIB first, then SIB)
    battery_order = {'LIB': 0, 'SIB': 1}
    
    # Priority ordering for LIB cathode materials
    lib_cathode_order = {
        'LFP': 0,
        'LCO': 1, 
        'LMO': 2,
        'NMC': 3,
        'NCA': 4,
        'none': 100
    }
    
    # Priority ordering for SIB cathode materials
    sib_cathode_order = {
        'Fe-PBA': 0,
        'Mn-PBA': 1,
        'NVPF': 2,
        'NFPP/NFP': 3,
        'NFM': 4,
        'CFM': 5,
        'NMO': 6,
        'other cathode': 7,
        'none': 100
    }
    
    # Priority ordering for LIB anode materials
    lib_anode_order = {
        'Graphite': 0,
        'Silicon/Carbon': 1,
        'LTO': 2,
        'none': 100
    }
    
    # Priority ordering for SIB anode materials
    sib_anode_order = {
        'Hard Carbon': 0,
        'other anode': 1,
        'none': 100
    }
    
    # Choose cathode and anode order maps based on battery type
    cathode_order = lib_cathode_order if battery_type == 'LIB' else sib_cathode_order
    anode_order = lib_anode_order if battery_type == 'LIB' else sib_anode_order
    
    # Calculate cathode score
    cathode_materials = cathode_part.split(", ")
    if len(cathode_materials) == 1 and cathode_materials[0] in cathode_order:
        cathode_score = cathode_order[cathode_materials[0]]
    else:
        # For combinations, use the first material's order as base
        cathode_score = min((cathode_order.get(mat, 99) for mat in cathode_materials), default=99)
    
    # Calculate anode score
    anode_materials = anode_part.split(", ")
    if len(anode_materials) == 1 and anode_materials[0] in anode_order:
        anode_score = anode_order[anode_materials[0]]
    else:
        # For combinations, use the first material's order as base
        anode_score = min((anode_order.get(mat, 99) for mat in anode_materials), default=99)
    
    # Sort first by battery type, then by cathode, then by anode
    return (battery_order.get(battery_type, 99), cathode_score, anode_score, material_group)

#==============================================================
# Calculate Node Positions
#==============================================================

# Group nodes by combined battery type, cathode and anode material group
material_groups = {}
for node in G.nodes():
    material_group = G.nodes[node]["material_group"]
    if material_group not in material_groups:
        material_groups[material_group] = []
    material_groups[material_group].append(node)

# Sort material groups by custom order
sorted_groups = sorted(material_groups.keys(), key=custom_material_sort)

# Calculate positions
pos = {}
material_positions = {}
node_spacing = 60  # Increased spacing between material groups

# Calculate positions for each material group
for i, group in enumerate(sorted_groups):
    y_position = i * node_spacing + node_spacing
    material_positions[group] = y_position
    
    # Sort nodes in the group by year
    nodes_in_group = sorted(
        material_groups[group], 
        key=lambda n: G.nodes[n]["year"] if isinstance(G.nodes[n]["year"], datetime) else datetime.now()
    )
    
    # Assign positions based on year
    for j, node in enumerate(nodes_in_group):
        year_value = G.nodes[node]["year"].year if isinstance(G.nodes[node]["year"], datetime) else 2000
        # Add small horizontal offset for better visibility if patents have the same year
        if j > 0 and year_value == G.nodes[nodes_in_group[j-1]]["year"].year:
            prev_x, _ = pos[nodes_in_group[j-1]]
            x = prev_x + 0.2  # Small offset to prevent exact overlap
        else:
            x = year_value
        pos[node] = (x, y_position)

#==============================================================
# Create the Visualization
#==============================================================

# Create edge traces
edge_traces = []
for edge in G.edges():
    # Skip if either node doesn't have a position
    if edge[0] not in pos or edge[1] not in pos:
        print(f"Skipping edge {edge} because one of the nodes has no position")
        continue
        
    x0, y0 = pos[edge[0]]
    x1, y1 = pos[edge[1]]
    
    # Determine edge color based on battery types
    source_type = G.nodes[edge[0]].get('battery_type')
    target_type = G.nodes[edge[1]].get('battery_type')
    
    if source_type == target_type:
        # Same battery type citation - use a light gray
        edge_color = 'rgba(100, 100, 100, 0.3)'
    else:
        # Cross-battery citation - use orange to highlight
        edge_color = 'rgba(255, 165, 0, 0.5)'
    
    # Create edge trace
    edge_trace = go.Scatter(
        x=[x0, x1, None], 
        y=[y0, y1, None],
        mode='lines',
        line=dict(width=0.8, color=edge_color),
        hoverinfo='text',
        text=f"From: {source_type} | To: {target_type}",
        showlegend=False
    )
    edge_traces.append(edge_trace)

# Define shape names for anode material combinations
lib_shape_names = {
    'circle': 'LIB: No Anode',
    'square': 'LIB: Graphite',
    'diamond': 'LIB: Silicon/Carbon',
    'triangle-up': 'LIB: LTO',
    'diamond-tall': 'LIB: Graphite + Silicon/Carbon',
    'hexagon': 'LIB: Graphite + LTO',
    'triangle-down': 'LIB: Silicon/Carbon + LTO',
    'star': 'LIB: Graphite + Silicon/Carbon + LTO'
}

sib_shape_names = {
    'circle-open': 'SIB: No Anode',
    'x': 'SIB: Hard Carbon',
    'cross-thin': 'SIB: other anode',
    'cross': 'SIB: Hard Carbon + other anode'
}

# Combine shape mappings
shape_names = {**lib_shape_names, **sib_shape_names}

# Create node traces by shape
unique_shapes = set(G.nodes[n]["shape"] for n in G.nodes())
node_traces = []

for shape in unique_shapes:
    # Filter nodes with this shape
    shape_nodes = [n for n in G.nodes() if G.nodes[n]["shape"] == shape]
    
    # Skip if no nodes have this shape
    if not shape_nodes:
        continue
    
    # Collect node data
    node_x, node_y, node_color, node_size, node_text = [], [], [], [], []
    
    for node in shape_nodes:
        x, y = pos[node]
        node_x.append(x)
        node_y.append(y)
        node_color.append(G.nodes[node]["color"])
        node_size.append(G.nodes[node]["size"])
        
        # Create hover text with detailed information
        battery_type = G.nodes[node]["battery_type"]
        cathode_str = G.nodes[node]["cathode_str"]
        anode_str = G.nodes[node]["anode_str"]
            
        hover_text = (
            f"Patent: {node}<br>"
            f"Battery Type: {battery_type}<br>"
            f"Year: {G.nodes[node]['year'].year}<br>"
            f"Cathode: {cathode_str}<br>"
            f"Anode: {anode_str}"
        )
        node_text.append(hover_text)
    
    # Create node trace for this shape
    node_trace = go.Scatter(
        x=node_x, y=node_y, 
        mode='markers',
        marker=dict(
            symbol=shape,
            size=node_size, 
            color=node_color,
            line=dict(width=1, color='black')
        ),
        text=node_text, 
        hoverinfo='text',
        name=shape_names.get(shape, shape),  # Use readable names for the shapes
        legendgroup="anode",  # Group all anode legends together
        legendgrouptitle=dict(text="Anode Materials")  # Add title for the group
    )
    
    node_traces.append(node_trace)

# Create legend for cathode materials
cathode_legend_traces = []

# LIB cathode materials
for material, color in lib_cathode_colors.items():
    if material != 'none':
        cathode_legend_traces.append(
            go.Scatter(
                x=[None], y=[None], 
                mode='markers', 
                marker=dict(size=10, color=color),
                name=f'LIB Cathode: {material}',
                hoverinfo='none',
                legendgroup="lib_cathode",
                legendgrouptitle=dict(text="LIB Cathode Materials")
            )
        )

# SIB cathode materials
for material, color in sib_cathode_colors.items():
    if material != 'none':
        cathode_legend_traces.append(
            go.Scatter(
                x=[None], y=[None], 
                mode='markers', 
                marker=dict(size=10, color=color),
                name=f'SIB Cathode: {material}',
                hoverinfo='none',
                legendgroup="sib_cathode",
                legendgrouptitle=dict(text="SIB Cathode Materials")
            )
        )

# Add legend entries for citation types
citation_legend_traces = [
    go.Scatter(
        x=[None], y=[None],
        mode='lines',
        line=dict(color='rgba(100, 100, 100, 0.3)', width=3),
        name='Same Battery Type Citation',
        hoverinfo='none',
        legendgroup="citations",
        legendgrouptitle=dict(text="Citation Types")
    ),
    go.Scatter(
        x=[None], y=[None],
        mode='lines',
        line=dict(color='rgba(255, 165, 0, 0.5)', width=3),
        name='Cross Battery Type Citation',
        hoverinfo='none',
        legendgroup="citations"
    )
]

# Define x-axis range
min_year = min([G.nodes[n]["year"].year for n in G.nodes() if isinstance(G.nodes[n]["year"], datetime)], default=1990)
max_year = max([G.nodes[n]["year"].year for n in G.nodes() if isinstance(G.nodes[n]["year"], datetime)], default=2025)
x_ticks = list(range(min_year, max_year + 1, 5))

# Calculate y-ticks and labels for material groups
y_ticks = list(material_positions.values())
y_tick_labels = list(material_positions.keys())

# Create the figure with all traces
fig = go.Figure(data=edge_traces + node_traces + cathode_legend_traces + citation_legend_traces)

# Update layout
fig.update_layout(
    title="Lithium-Ion and Sodium-Ion Battery Patent Network by Material Combinations",
    showlegend=True,
    xaxis=dict(
        title="Priority Date",
        tickmode='array',
        tickvals=x_ticks,
        ticktext=[str(year) for year in x_ticks],
        tickfont=dict(size=14),
        showgrid=True,
        gridwidth=1,
        gridcolor='lightgrey',
        range=[min_year-1, max_year+1]  # Set range with slight padding
    ),
    yaxis=dict(
        tickmode='array',
        tickvals=y_ticks,
        ticktext=y_tick_labels,
        title="Material Combinations",
        tickfont=dict(size=12),
        showgrid=True,
        gridwidth=1,
        gridcolor='lightgrey'
    ),
    hovermode='closest',
    hoverlabel=dict(font=dict(size=14)),
    plot_bgcolor='rgba(248, 248, 248, 0.3)',  # Very light gray background
    paper_bgcolor='rgba(0,0,0,0)',
    legend=dict(
        yanchor="top", y=0.99, xanchor="left", x=1.01,
        bgcolor="rgba(255, 255, 255, 0.8)", 
        font=dict(size=12),
        groupclick="toggleitem"  # Toggle visibility of grouped items
    ),
    width=1800,
    height=len(sorted_groups) * 30 + 300,  # Dynamic height based on number of groups
    margin=dict(l=300, r=350, t=80, b=50)  # Increased left margin for y-axis labels
)

# Output the network to an HTML file
output_file = "LIB_SIB_Combined_Material_Patent_Network.html"
fig.write_html(output_file)
print(f"Visualization saved to {output_file}")

# Print network statistics
print("\nNetwork Statistics:")
print(f"Total patents in network: {G.number_of_nodes()}")
print(f"LIB patents: {len([n for n in G.nodes() if G.nodes[n]['battery_type'] == 'LIB'])}")
print(f"SIB patents: {len([n for n in G.nodes() if G.nodes[n]['battery_type'] == 'SIB'])}")
print(f"Total material groups: {len(sorted_groups)}")
print(f"Total citations: {G.number_of_edges()}")

# Count cross-battery citations
cross_citations = sum(1 for u, v in G.edges() 
                     if G.nodes[u]['battery_type'] != G.nodes[v]['battery_type'])
print(f"Cross-battery citations: {cross_citations}")

# Calculate citation directionality
lib_to_sib = sum(1 for u, v in G.edges() 
                if G.nodes[u]['battery_type'] == 'LIB' and G.nodes[v]['battery_type'] == 'SIB')
sib_to_lib = sum(1 for u, v in G.edges() 
                if G.nodes[u]['battery_type'] == 'SIB' and G.nodes[v]['battery_type'] == 'LIB')

print(f"LIB patents citing SIB patents: {lib_to_sib}")
print(f"SIB patents citing LIB patents: {sib_to_lib}")

print("\nAnalysis complete!")


#%%


#=====================================================================
# Bar charts for cathode and anode materials per priority year for LIB and SIB
#=====================================================================

import pandas as pd
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import datetime

# File paths for both datasets
lib_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_full_data_classified_onlyLIB_27052025.xlsx"
sib_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_full_data_classified_final_onlySIB_26052025.xlsx"
sheet_name = "Sheet1"

# Load both Excel files
LIB_patents = pd.read_excel(lib_file_path, sheet_name=sheet_name)
SIB_patents = pd.read_excel(sib_file_path, sheet_name=sheet_name)

# Print basic info about both datasets
print(f"LIB dataset shape: {LIB_patents.shape}")
print(f"SIB dataset shape: {SIB_patents.shape}")

#==============================================================
# Define material sets for both battery types
#==============================================================

# Define LIB material groups
lib_cathode_materials = {"LCO", "LMO", "LFP", "NMC", "NCA"}
lib_anode_materials = {"Silicon/Carbon", "Graphite", "LTO"}

# Define SIB material groups
sib_cathode_materials = {"Fe-PBA", "Mn-PBA", "NVPF", "NFPP/NFP", "NFM", "CFM", "NMO", "other cathode"}
sib_anode_materials = {"Hard Carbon", "other anode"}

#==============================================================
# Function to extract materials from material strings
#==============================================================

def extract_materials(material_str, material_set):
    """Extract materials that match the given material set from a comma-separated string."""
    materials = set(material_str.split(", ")) if isinstance(material_str, str) else set()
    extracted = list(materials.intersection(material_set))
    return extracted if extracted else ["none"]

#==============================================================
# Process LIB dataset
#==============================================================

# Extract priority year from earliest_priority_date for LIB
LIB_patents["earliest_priority_date"] = pd.to_datetime(LIB_patents["Earliest Priority Date"])
LIB_patents["priority_year"] = LIB_patents["earliest_priority_date"].dt.year
LIB_patents = LIB_patents.sort_values(by=["priority_year"], ascending=True)

# Create new columns for Cathode and Anode Materials for LIB
LIB_patents["Cathode Material"] = LIB_patents["Material-Level"].apply(lambda x: extract_materials(x, lib_cathode_materials))
LIB_patents["Anode Material"] = LIB_patents["Material-Level"].apply(lambda x: extract_materials(x, lib_anode_materials))

# Update Component-Level based on Material-Level for LIB
for index, row in LIB_patents.iterrows():
    if row["Material-Level"] != "none" and row["Component-Level"] == "none":
        materials = set(row["Material-Level"].split(", ")) if isinstance(row["Material-Level"], str) else set()

        cathode_match = materials & lib_cathode_materials
        anode_match = materials & lib_anode_materials

        new_component_level = []
        if cathode_match:
            new_component_level.append("Positive Electrode")
        if anode_match:
            new_component_level.append("Negative Electrode")

        LIB_patents.at[index, "Component-Level"] = ", ".join(new_component_level) if new_component_level else "none"

#==============================================================
# Process SIB dataset
#==============================================================

# Extract priority year from earliest_priority_date for SIB
SIB_patents["earliest_priority_date"] = pd.to_datetime(SIB_patents["Earliest Priority Date"])
SIB_patents["priority_year"] = SIB_patents["earliest_priority_date"].dt.year
SIB_patents = SIB_patents.sort_values(by=["priority_year"], ascending=True)

# Create new columns for Cathode and Anode Materials for SIB
SIB_patents["Cathode Material"] = SIB_patents["Material-Level"].apply(lambda x: extract_materials(x, sib_cathode_materials))
SIB_patents["Anode Material"] = SIB_patents["Material-Level"].apply(lambda x: extract_materials(x, sib_anode_materials))

# Update Component-Level based on Material-Level for SIB
for index, row in SIB_patents.iterrows():
    if row["Material-Level"] != "none" and row["Component-Level"] == "none":
        materials = set(row["Material-Level"].split(", ")) if isinstance(row["Material-Level"], str) else set()

        cathode_match = materials & sib_cathode_materials
        anode_match = materials & sib_anode_materials

        new_component_level = []
        if cathode_match:
            new_component_level.append("Positive Electrode")
        if anode_match:
            new_component_level.append("Negative Electrode")

        SIB_patents.at[index, "Component-Level"] = ", ".join(new_component_level) if new_component_level else "none"

#==============================================================
# Filter datasets for electrode materials
#==============================================================

# Filter LIB dataset for positive and negative electrode patents
lib_positive_electrode = LIB_patents[LIB_patents["Component-Level"].str.contains("Positive Electrode", na=False)]
lib_negative_electrode = LIB_patents[LIB_patents["Component-Level"].str.contains("Negative Electrode", na=False)]

# Filter SIB dataset for positive and negative electrode patents
sib_positive_electrode = SIB_patents[SIB_patents["Component-Level"].str.contains("Positive Electrode", na=False)]
sib_negative_electrode = SIB_patents[SIB_patents["Component-Level"].str.contains("Negative Electrode", na=False)]

print(f"LIB cathode patents: {len(lib_positive_electrode)}")
print(f"LIB anode patents: {len(lib_negative_electrode)}")
print(f"SIB cathode patents: {len(sib_positive_electrode)}")
print(f"SIB anode patents: {len(sib_negative_electrode)}")

#==============================================================
# Extract material counts per year
#==============================================================

# Define a function to process materials
def extract_material_counts(df, material_column):
    """Extract and count materials per priority year from dataframe."""
    materials_list = []
    
    for _, row in df.iterrows():
        priority_year = row['priority_year']
        materials = row[material_column]
        
        for material in materials:
            if material != "none":
                materials_list.append((priority_year, material))
    
    # Create DataFrame and count occurrences
    materials_df = pd.DataFrame(materials_list, columns=['priority_year', 'Material'])
    materials_per_year = materials_df.groupby(['priority_year', 'Material']).size().reset_index(name='count')
    
    return materials_per_year

# Apply the function to each dataset and material type
lib_cathode_counts = extract_material_counts(lib_positive_electrode, "Cathode Material")
lib_anode_counts = extract_material_counts(lib_negative_electrode, "Anode Material")
sib_cathode_counts = extract_material_counts(sib_positive_electrode, "Cathode Material")
sib_anode_counts = extract_material_counts(sib_negative_electrode, "Anode Material")

# Add battery type column for later identification
lib_cathode_counts["Battery Type"] = "LIB"
lib_anode_counts["Battery Type"] = "LIB"
sib_cathode_counts["Battery Type"] = "SIB"
sib_anode_counts["Battery Type"] = "SIB"

#==============================================================
# Define color maps for materials
#==============================================================

# LIB cathode materials color map
lib_cathode_colors = {
    'LFP': 'rgba(0, 0, 255, 1)',      # Blue
    'NMC': 'rgba(0, 255, 0, 1)',       # Green
    'LCO': 'rgba(255, 0, 0, 1)',       # Red
    'LMO': 'rgba(0, 255, 255, 1)',     # Cyan
    'NCA': 'rgba(128, 0, 128, 1)'      # Purple
}

# LIB anode materials color map
lib_anode_colors = {
    'Graphite': 'rgba(70, 130, 180, 1)',       # Steel Blue
    'Silicon/Carbon': 'rgba(255, 140, 0, 1)',  # Dark Orange
    'LTO': 'rgba(154, 205, 50, 1)'             # Yellow Green
}

# SIB cathode materials color map
sib_cathode_colors = {
    'Fe-PBA': 'rgba(0, 0, 139, 1)',      # Dark Blue
    'Mn-PBA': 'rgba(0, 100, 0, 1)',      # Dark Green
    'NVPF': 'rgba(139, 0, 0, 1)',        # Dark Red
    'NFPP/NFP': 'rgba(139, 0, 139, 1)',  # Dark Magenta
    'NFM': 'rgba(0, 139, 139, 1)',       # Dark Cyan
    'CFM': 'rgba(255, 140, 0, 1)',       # Dark Orange
    'NMO': 'rgba(70, 70, 70, 1)',        # Dark Grey
    'other cathode': 'rgba(169, 169, 169, 1)'  # Grey
}

# SIB anode materials color map
sib_anode_colors = {
    'Hard Carbon': 'rgba(139, 69, 19, 1)',    # Saddle Brown
    'other anode': 'rgba(105, 105, 105, 1)'   # Dim Grey
}

#==============================================================
# Create stacked bar charts for all materials
#==============================================================

# Function to create a stacked bar chart
def create_stacked_barchart(data, color_map, title, x_range=None, y_range=None):
    """Create a stacked bar chart for material counts per year."""
    fig = go.Figure()
    
    # Get unique materials
    materials = data["Material"].unique()
    
    # Define x-range if not provided
    if x_range is None:
        x_min = data["priority_year"].min()
        x_max = data["priority_year"].max()
        x_range = [x_min, x_max]
    
    # For each material, add a bar trace
    for material in materials:
        material_data = data[data["Material"] == material]
        fig.add_trace(go.Bar(
            x=material_data["priority_year"],
            y=material_data["count"],
            name=material,
            marker_color=color_map.get(material, 'rgba(169, 169, 169, 1)')
        ))
    
    # Update layout
    fig.update_layout(
        title=title,
        xaxis=dict(
            title="Priority Year",
            range=x_range,
            tickmode="auto",
            tick0=x_range[0],
            dtick=5
        ),
        yaxis=dict(
            title="Number of Patents",
            range=y_range
        ),
        barmode="stack",
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="center",
            x=0.5
        ),
        template="plotly_white"
    )
    
    return fig

# Define x-range and y-range for consistent scaling
x_range = [1990, 2025]

# Create stacked bar charts
lib_cathode_stacked = create_stacked_barchart(
    lib_cathode_counts, lib_cathode_colors, 
    "LIB Cathode Materials per Priority Year", 
    x_range=x_range
)

lib_anode_stacked = create_stacked_barchart(
    lib_anode_counts, lib_anode_colors, 
    "LIB Anode Materials per Priority Year", 
    x_range=x_range
)

sib_cathode_stacked = create_stacked_barchart(
    sib_cathode_counts, sib_cathode_colors, 
    "SIB Cathode Materials per Priority Year", 
    x_range=x_range
)

sib_anode_stacked = create_stacked_barchart(
    sib_anode_counts, sib_anode_colors, 
    "SIB Anode Materials per Priority Year", 
    x_range=x_range
)

# Save individual stacked bar charts
lib_cathode_stacked.write_html("LIB_cathode_stacked_barchart.html")
lib_anode_stacked.write_html("LIB_anode_stacked_barchart.html")
sib_cathode_stacked.write_html("SIB_cathode_stacked_barchart.html")
sib_anode_stacked.write_html("SIB_anode_stacked_barchart.html")

#==============================================================
# Create individual bar charts for each material
#==============================================================

# Function to determine the maximum count across all materials for scaling
def get_max_count(data_list):
    """Get the maximum count across multiple datasets for consistent scaling."""
    max_counts = []
    for data in data_list:
        max_counts.append(data["count"].max())
    return max(max_counts) if max_counts else 10

# Function to create individual bar charts for each material
def create_material_barcharts(data, color_map, title_prefix, x_range=None, y_range=None):
    """Create individual bar charts for each material in the dataset."""
    # Get unique materials
    materials = data["Material"].unique()
    
    # Create figures for each material
    figs = {}
    
    for material in materials:
        material_data = data[data["Material"] == material]
        
        fig = go.Figure()
        fig.add_trace(go.Bar(
            x=material_data["priority_year"],
            y=material_data["count"],
            marker_color=color_map.get(material, 'rgba(169, 169, 169, 1)')
        ))
        
        fig.update_layout(
            title=f"{title_prefix}: {material}",
            xaxis=dict(
                title="Priority Year",
                range=x_range,
                tickmode="auto",
                tick0=x_range[0],
                dtick=5
            ),
            yaxis=dict(
                title="Number of Patents",
                range=y_range
            ),
            template="plotly_white"
        )
        
        figs[material] = fig
    
    return figs

# Calculate maximum counts for each material type for consistent scaling
lib_cathode_max = get_max_count([lib_cathode_counts])
lib_anode_max = get_max_count([lib_anode_counts])
sib_cathode_max = get_max_count([sib_cathode_counts])
sib_anode_max = get_max_count([sib_anode_counts])

# Create y-ranges with some headroom
lib_cathode_y_range = [0, lib_cathode_max * 1.1]
lib_anode_y_range = [0, lib_anode_max * 1.1]
sib_cathode_y_range = [0, sib_cathode_max * 1.1]
sib_anode_y_range = [0, sib_anode_max * 1.1]

# Create individual bar charts for each material type
lib_cathode_figs = create_material_barcharts(
    lib_cathode_counts, lib_cathode_colors, 
    "LIB Cathode Material", 
    x_range=x_range, y_range=lib_cathode_y_range
)

lib_anode_figs = create_material_barcharts(
    lib_anode_counts, lib_anode_colors, 
    "LIB Anode Material", 
    x_range=x_range, y_range=lib_anode_y_range
)

sib_cathode_figs = create_material_barcharts(
    sib_cathode_counts, sib_cathode_colors, 
    "SIB Cathode Material", 
    x_range=x_range, y_range=sib_cathode_y_range
)

sib_anode_figs = create_material_barcharts(
    sib_anode_counts, sib_anode_colors, 
    "SIB Anode Material", 
    x_range=x_range, y_range=sib_anode_y_range
)

#==============================================================
# Create combined subplots for all material types
#==============================================================

# Function to organize figures into subplots
def create_material_subplots(figs, title, rows, cols):
    """Create a subplot figure containing multiple material bar charts."""
    subplot_titles = list(figs.keys())
    fig = make_subplots(rows=rows, cols=cols, subplot_titles=subplot_titles)
    
    # Add each figure to the subplots
    for i, (material, material_fig) in enumerate(figs.items()):
        row = i // cols + 1
        col = i % cols + 1
        
        # Extract the trace from the individual figure and add to subplot
        for trace in material_fig['data']:
            fig.add_trace(trace, row=row, col=col)
        
        # Copy layout settings for each subplot
        fig.update_xaxes(
            title_text=material_fig.layout.xaxis.title.text,
            range=material_fig.layout.xaxis.range,
            tickmode=material_fig.layout.xaxis.tickmode,
            tick0=material_fig.layout.xaxis.tick0,
            dtick=material_fig.layout.xaxis.dtick,
            row=row, col=col
        )
        
        fig.update_yaxes(
            title_text=material_fig.layout.yaxis.title.text,
            range=material_fig.layout.yaxis.range,
            row=row, col=col
        )
    
    # Update overall layout
    fig.update_layout(
        title_text=title,
        showlegend=False,
        height=300 * rows,
        width=500 * cols,
        template="plotly_white"
    )
    
    return fig

# Calculate dimensions for subplots
lib_cathode_rows = (len(lib_cathode_figs) + 1) // 2
lib_anode_rows = (len(lib_anode_figs) + 1) // 2
sib_cathode_rows = (len(sib_cathode_figs) + 1) // 2
sib_anode_rows = (len(sib_anode_figs) + 1) // 2

# Create subplots for each material type
lib_cathode_subplots = create_material_subplots(
    lib_cathode_figs, 
    "LIB Cathode Materials by Priority Year", 
    lib_cathode_rows, 2
)

lib_anode_subplots = create_material_subplots(
    lib_anode_figs, 
    "LIB Anode Materials by Priority Year", 
    lib_anode_rows, 2
)

sib_cathode_subplots = create_material_subplots(
    sib_cathode_figs, 
    "SIB Cathode Materials by Priority Year", 
    sib_cathode_rows, 2
)

sib_anode_subplots = create_material_subplots(
    sib_anode_figs, 
    "SIB Anode Materials by Priority Year", 
    sib_anode_rows, 2
)

# Save subplot figures
lib_cathode_subplots.write_html("LIB_cathode_individual_barcharts.html")
lib_anode_subplots.write_html("LIB_anode_individual_barcharts.html")
sib_cathode_subplots.write_html("SIB_cathode_individual_barcharts.html")
sib_anode_subplots.write_html("SIB_anode_individual_barcharts.html")

#==============================================================
# Create master figure combining all subplot types
#==============================================================

# Create master subplot with 4 sections (2x2 grid)
master_fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=[
        "LIB Cathode Materials (Stacked)",
        "LIB Anode Materials (Stacked)",
        "SIB Cathode Materials (Stacked)",
        "SIB Anode Materials (Stacked)"
    ],
    vertical_spacing=0.1,
    horizontal_spacing=0.05
)

# Add stacked bar charts to master plot
# LIB Cathode
for trace in lib_cathode_stacked['data']:
    master_fig.add_trace(trace, row=1, col=1)

# LIB Anode
for trace in lib_anode_stacked['data']:
    master_fig.add_trace(trace, row=1, col=2)

# SIB Cathode
for trace in sib_cathode_stacked['data']:
    master_fig.add_trace(trace, row=2, col=1)

# SIB Anode
for trace in sib_anode_stacked['data']:
    master_fig.add_trace(trace, row=2, col=2)

# Update layout for each subplot
master_fig.update_xaxes(title_text="Priority Year", range=x_range, row=1, col=1)
master_fig.update_xaxes(title_text="Priority Year", range=x_range, row=1, col=2)
master_fig.update_xaxes(title_text="Priority Year", range=x_range, row=2, col=1)
master_fig.update_xaxes(title_text="Priority Year", range=x_range, row=2, col=2)

master_fig.update_yaxes(title_text="Number of Patents", row=1, col=1)
master_fig.update_yaxes(title_text="Number of Patents", row=1, col=2)
master_fig.update_yaxes(title_text="Number of Patents", row=2, col=1)
master_fig.update_yaxes(title_text="Number of Patents", row=2, col=2)

# Update overall layout
master_fig.update_layout(
    title_text="Battery Materials Patent Analysis by Priority Year",
    height=1000,
    width=1200,
    barmode='stack',
    legend=dict(
        orientation="h",
        yanchor="bottom",
        y=-0.2,
        xanchor="center",
        x=0.5
    ),
    template="plotly_white"
)

# Save the master figure
master_fig.write_html("Battery_Materials_Master_Barchart.html")
print("All charts have been created and saved successfully!")


#%%

# Figure 1a: 

#=====================================================================
# Optimized Line charts and Stacked Area charts for cathode and anode materials per priority year for LIB and SIB
#=====================================================================

import pandas as pd
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import datetime
import time

# Configuration for faster processing
SHOW_CHARTS = False  # Set to True if you want to display charts
SAVE_PNG = False     # Set to True if you want PNG export (slower)
SAVE_HTML = True     # HTML files are fast to save

print(f"Starting processing at {time.strftime(#H:%M:%S')}")
print(f"Config: SHOW_CHARTS={SHOW_CHARTS}, SAVE_PNG={SAVE_PNG}, SAVE_HTML={SAVE_HTML}")
start_time = time.time()

# File paths for both datasets
lib_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_full_data_classified_onlyLIB_27052025.xlsx"
sib_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_full_data_classified_final_onlySIB_26052025.xlsx"
sheet_name = "Sheet1"

# Load both Excel files
print("Loading Excel files...")
LIB_patents = pd.read_excel(lib_file_path, sheet_name=sheet_name)
SIB_patents = pd.read_excel(sib_file_path, sheet_name=sheet_name)

# Print basic info about both datasets
print(f"LIB dataset shape: {LIB_patents.shape[0]:,} rows x {LIB_patents.shape[1]} columns")
print(f"SIB dataset shape: {SIB_patents.shape[0]:,} rows x {SIB_patents.shape[1]} columns")
print(f"Data loading completed: {time.time() - start_time:.1f}s")

#==============================================================
# Define material sets for both battery types
#==============================================================

# Define LIB material groups
lib_cathode_materials = {"LCO", "LMO", "LFP", "NMC", "NCA"}
lib_anode_materials = {"Silicon/Carbon", "Graphite", "LTO"}

# Define SIB material groups
sib_cathode_materials = {"Fe-PBA", "Mn-PBA", "NVPF", "NFPP/NFP", "NFM", "CFM", "NMO"}
sib_anode_materials = {"Hard Carbon"}

#==============================================================
# Optimized data processing functions
#==============================================================

def process_dataset_optimized(df, cathode_materials, anode_materials, dataset_name):
    """Optimized processing of patent dataset."""
    print(f"Processing {dataset_name} dataset...")
    
    # Early filtering of irrelevant data
    df = df.dropna(subset=['Earliest Priority Date']).copy()
    df = df[df['Material-Level'].notna()].copy()
    
    # Extract priority year
    df["priority_year"] = pd.to_datetime(df["Earliest Priority Date"]).dt.year
    
    # Filter to reasonable year range (speeds up processing)
    df = df[(df["priority_year"] >= 1990) & (df["priority_year"] <= 2025)].copy()
    
    print(f"  After filtering: {len(df):,} rows")
    
    # Vectorized material extraction
    def extract_materials_vectorized(material_str, material_set):
        """Vectorized version of material extraction."""
        if pd.isna(material_str) or material_str == "none":
            return []
        materials = set(str(material_str).split(", "))
        extracted = list(materials.intersection(material_set))
        return extracted if extracted else []
    
    # Apply material extraction
    df["cathode_materials"] = df["Material-Level"].apply(
        lambda x: extract_materials_vectorized(x, cathode_materials)
    )
    df["anode_materials"] = df["Material-Level"].apply(
        lambda x: extract_materials_vectorized(x, anode_materials)
    )
    
    # Optimized component level update
    def update_component_level(row):
        if row["Material-Level"] != "none" and (pd.isna(row["Component-Level"]) or row["Component-Level"] == "none"):
            new_components = []
            if row["cathode_materials"]:
                new_components.append("Positive Electrode")
            if row["anode_materials"]:
                new_components.append("Negative Electrode")
            return ", ".join(new_components) if new_components else row["Component-Level"]
        return row["Component-Level"]
    
    df["Component-Level"] = df.apply(update_component_level, axis=1)
    
    return df

#==============================================================
# Process both datasets
#==============================================================

LIB_patents = process_dataset_optimized(LIB_patents, lib_cathode_materials, lib_anode_materials, "LIB")
print(f"LIB processing completed: {time.time() - start_time:.1f}s")

SIB_patents = process_dataset_optimized(SIB_patents, sib_cathode_materials, sib_anode_materials, "SIB")
print(f"SIB processing completed: {time.time() - start_time:.1f}s")

#==============================================================
# Optimized material counting
#==============================================================

def extract_material_counts_optimized(df, material_column_name, electrode_type):
    """Optimized material counting function."""
    
    # Filter for electrode type
    if electrode_type == "cathode":
        filtered_df = df[df["Component-Level"].str.contains("Positive Electrode", na=False)]
        material_column = "cathode_materials"
    else:  # anode
        filtered_df = df[df["Component-Level"].str.contains("Negative Electrode", na=False)]
        material_column = "anode_materials"
    
    print(f"  {electrode_type.capitalize()} patents: {len(filtered_df):,}")
    
    # Explode materials lists into individual rows
    exploded_data = []
    for _, row in filtered_df.iterrows():
        year = row['priority_year']
        materials = row[material_column]
        for material in materials:
            if material and material != "none":
                exploded_data.append({'priority_year': year, 'Material': material})
    
    # Convert to DataFrame and count
    if exploded_data:
        materials_df = pd.DataFrame(exploded_data)
        counts = materials_df.groupby(['priority_year', 'Material']).size().reset_index(name='count')
        return counts
    else:
        return pd.DataFrame(columns=['priority_year', 'Material', 'count'])

# Extract counts for all material types
print("Extracting material counts...")
lib_cathode_counts = extract_material_counts_optimized(LIB_patents, "cathode_materials", "cathode")
lib_anode_counts = extract_material_counts_optimized(LIB_patents, "anode_materials", "anode")
sib_cathode_counts = extract_material_counts_optimized(SIB_patents, "cathode_materials", "cathode")
sib_anode_counts = extract_material_counts_optimized(SIB_patents, "anode_materials", "anode")

print(f"Material counting completed: {time.time() - start_time:.1f}s")

#==============================================================
# Define color maps for materials (organized by structure)
#==============================================================

# LIB cathode colors - reds for layered oxides, blues for polyanionic, cyan for spinel
lib_cathode_colors = {
    # Layered oxides (reds) - better distinguishable
    'LCO': '#FF0000',  # Bright red
    'NMC': '#B22222',  # Fire brick red
    'NCA': '#FF6B6B',  # Light red/coral
    
    # Polyanionic materials (blues)
    'LFP': '#0000FF',  # Blue
    
    # Spinel (cyan)
    'LMO': '#00FFFF',  # Cyan
}

# LIB anode colors - orange for LTO, grays for carbon
lib_anode_colors = {
    'LTO': '#FFA500',        # Orange
    'Silicon/Carbon': '#666666',  # Dark gray
    'Graphite': '#333333',        # Darker gray
}

# SIB cathode colors - reds for layered oxides, blues for polyanionic, greens for PBA
sib_cathode_colors = {
    # Layered oxides (reds) - better distinguishable
    'CFM': '#FF4500',    # Orange red
    'NFM': '#DC143C',    # Crimson
    'NMO': '#FF69B4',    # Hot pink (distinct from reds)
    
    # Polyanionic materials (blues) - better distinguishable
    'NVPF': '#0066FF',      # Royal blue
    'NFPP/NFP': '#000080',  # Navy blue
    
    # PBA (greens)
    'Fe-PBA': '#00CC00',    # Green
    'Mn-PBA': '#006600',    # Dark green
}

# SIB anode colors - gray for Hard Carbon
sib_anode_colors = {
    'Hard Carbon': '#999999',  # Gray
}

#==============================================================
# Optimized line chart creation for separate cathode/anode plots
#==============================================================

def create_separate_line_chart(data, color_map, title, electrode_type, x_range=None):
    """Create line chart for either cathode or anode materials."""
    fig = go.Figure()
    
    if data.empty:
        print(f"Warning: No data for {title}")
        return fig
    
    # Define x-range if not provided
    if x_range is None:
        x_min = max(1990, data["priority_year"].min())
        x_max = min(2025, data["priority_year"].max())
        x_range = [x_min, x_max]
    
    # Get unique materials
    materials = sorted(data["Material"].unique())
    
    # Create complete year range
    all_years = list(range(x_range[0], x_range[1] + 1))
    
    # Pre-process data for all materials
    material_data_dict = {}
    for material in materials:
        material_subset = data[data["Material"] == material]
        year_counts = dict(zip(material_subset["priority_year"], material_subset["count"]))
        
        # Create complete series with zeros for missing years
        y_values = [year_counts.get(year, 0) for year in all_years]
        material_data_dict[material] = y_values
    
    # Add traces for each material
    for material in materials:
        fig.add_trace(go.Scatter(
            x=all_years,
            y=material_data_dict[material],
            name=material,
            mode='lines+markers',
            line=dict(color=color_map.get(material, '#17becf'), width=2),
            marker=dict(size=4),
            connectgaps=True
        ))
    
    # Update layout
    fig.update_layout(
        # title=dict(text=title, font=dict(size=16)),
        xaxis=dict(
            title="Priority Year",
            range=x_range,
            tickmode="linear",
            tick0=x_range[0],
            dtick=5,
            titlefont=dict(size=18),
            tickfont=dict(size=16)
        ),
        yaxis=dict(
            title="Number of Patents",
            range=[0, 200],  # Fixed y-axis range
            titlefont=dict(size=18),
            tickfont=dict(size=16)
        ),
        legend=dict(
            orientation="v",
            yanchor="top",
            y=1,
            xanchor="left",
            x=1.02,
            font=dict(size=16)
        ),
        template="plotly_white",
        width=1000,
        height=600,
        margin=dict(r=150)
    )
    
    return fig

#==============================================================
# Optimized stacked area chart creation
#==============================================================

def create_stacked_area_chart(data, color_map, title, electrode_type, x_range=None):
    """Create stacked area chart for either cathode or anode materials."""
    fig = go.Figure()
    
    if data.empty:
        print(f"Warning: No data for {title}")
        return fig
    
    # Define x-range if not provided
    if x_range is None:
        x_min = max(1990, data["priority_year"].min())
        x_max = min(2025, data["priority_year"].max())
        x_range = [x_min, x_max]
    
    # Get unique materials and define custom order
    materials = sorted(data["Material"].unique())
    
    # Define custom stacking order (bottom to top)
    # You can customize these orders as needed
    if electrode_type == "cathode":
        if "LCO" in materials:  # LIB cathode
            custom_order = ["LCO", "NMC", "NCA", "LMO", "LFP"]
        else:  # SIB cathode
            custom_order = ["NFM", "CFM", "NMO", "NVPF", "NFPP/NFP", "Fe-PBA", "Mn-PBA"]
    else:  # anode
        if "Graphite" in materials:  # LIB anode
            custom_order = ["Graphite", "Silicon/Carbon", "LTO"]
        else:  # SIB anode
            custom_order = ["Hard Carbon"]
    
    # Filter custom_order to only include materials present in data
    materials = [mat for mat in custom_order if mat in materials]
    
    # Create complete year range
    all_years = list(range(x_range[0], x_range[1] + 1))
    
    # Pre-process data for all materials
    material_data_dict = {}
    for material in materials:
        material_subset = data[data["Material"] == material]
        year_counts = dict(zip(material_subset["priority_year"], material_subset["count"]))
        
        # Create complete series with zeros for missing years
        y_values = [year_counts.get(year, 0) for year in all_years]
        material_data_dict[material] = y_values
    
    # Add traces for each material (stacked areas)
    for material in materials:
        fig.add_trace(go.Scatter(
            x=all_years,
            y=material_data_dict[material],
            name=material,
            mode='lines',
            line=dict(width=0),
            fill='tonexty' if material != materials[0] else 'tozeroy',
            fillcolor=color_map.get(material, '#17becf'),
            stackgroup='one'  # This creates the stacking
        ))
    
    # Calculate total patents for each year (sum across all materials)
    total_patents = []
    for year in all_years:
        year_total = sum(material_data_dict[material][all_years.index(year)] for material in materials)
        total_patents.append(year_total)
    
    # Add dashed gray line for total patents
    fig.add_trace(go.Scatter(
        x=all_years,
        y=total_patents,
        name="Total Patents",
        mode='lines',
        line=dict(color='black', width=3, dash='dash'),
        showlegend=True
    ))
    
    # Update layout
    fig.update_layout(
        # title=dict(text=title, font=dict(size=16)),
        xaxis=dict(
            title="Priority Year",
            range=x_range,
            tickmode="linear",
            tick0=x_range[0],
            dtick=5,
            titlefont=dict(size=18),
            tickfont=dict(size=16)
        ),
        yaxis=dict(
            title="Number of Patents",
            titlefont=dict(size=18),
            tickfont=dict(size=16)
        ),
        legend=dict(
            orientation="v",
            yanchor="top",
            y=1,
            xanchor="left",
            x=1.02,
            font=dict(size=16)
        ),
        template="plotly_white",
        width=1000,
        height=600,
        margin=dict(r=150)
    )
    
    return fig

#==============================================================
# Create separate line charts for cathodes and anodes
#==============================================================

print("Creating separate cathode and anode line charts...")

# Define x-range for consistent scaling
x_range = [1990, 2025]

# Create LIB cathode chart
lib_cathode_chart = create_separate_line_chart(
    lib_cathode_counts, lib_cathode_colors,
    "Lithium-Ion Battery Cathode Materials Patent Trends by Priority Year",
    "cathode", x_range=x_range
)

# Create LIB anode chart
lib_anode_chart = create_separate_line_chart(
    lib_anode_counts, lib_anode_colors,
    "Lithium-Ion Battery Anode Materials Patent Trends by Priority Year",
    "anode", x_range=x_range
)

# Create SIB cathode chart
sib_cathode_chart = create_separate_line_chart(
    sib_cathode_counts, sib_cathode_colors,
    "Sodium-Ion Battery Cathode Materials Patent Trends by Priority Year",
    "cathode", x_range=x_range
)

# Create SIB anode chart
sib_anode_chart = create_separate_line_chart(
    sib_anode_counts, sib_anode_colors,
    "Sodium-Ion Battery Anode Materials Patent Trends by Priority Year",
    "anode", x_range=x_range
)

print(f"Line chart creation completed: {time.time() - start_time:.1f}s")

#==============================================================
# Create stacked area charts
#==============================================================

print("Creating stacked area charts...")

# Create LIB cathode stacked area chart
lib_cathode_area_chart = create_stacked_area_chart(
    lib_cathode_counts, lib_cathode_colors,
    "Lithium-Ion Battery Cathode Materials Patent Trends by Priority Year (Stacked)",
    "cathode", x_range=x_range
)

# Create LIB anode stacked area chart
lib_anode_area_chart = create_stacked_area_chart(
    lib_anode_counts, lib_anode_colors,
    "Lithium-Ion Battery Anode Materials Patent Trends by Priority Year (Stacked)",
    "anode", x_range=x_range
)

# Create SIB cathode stacked area chart
sib_cathode_area_chart = create_stacked_area_chart(
    sib_cathode_counts, sib_cathode_colors,
    "Sodium-Ion Battery Cathode Materials Patent Trends by Priority Year (Stacked)",
    "cathode", x_range=x_range
)

# Create SIB anode stacked area chart
sib_anode_area_chart = create_stacked_area_chart(
    sib_anode_counts, sib_anode_colors,
    "Sodium-Ion Battery Anode Materials Patent Trends by Priority Year (Stacked)",
    "anode", x_range=x_range
)

print(f"Stacked area chart creation completed: {time.time() - start_time:.1f}s")

# Display line charts
if SHOW_CHARTS:
    print("Displaying line charts...")
    lib_cathode_chart.show()
    lib_anode_chart.show()
    sib_cathode_chart.show()
    sib_anode_chart.show()
    
    print("Displaying stacked area charts...")
    lib_cathode_area_chart.show()
    lib_anode_area_chart.show()
    sib_cathode_area_chart.show()
    sib_anode_area_chart.show()
else:
    print("Charts created but not displayed for faster processing")

# Save line charts (optimized)
if SAVE_HTML:
    print("Saving line chart HTML files...")
    save_start = time.time()
    lib_cathode_chart.write_html("LIB_cathode_materials_patent_trends.html")
    lib_anode_chart.write_html("LIB_anode_materials_patent_trends.html")
    sib_cathode_chart.write_html("SIB_cathode_materials_patent_trends.html")
    sib_anode_chart.write_html("SIB_anode_materials_patent_trends.html")
    print(f"  Line chart HTML files saved: {time.time() - save_start:.1f}s")
    
    print("Saving stacked area chart HTML files...")
    save_start = time.time()
    lib_cathode_area_chart.write_html("LIB_cathode_materials_patent_trends_stacked.html")
    lib_anode_area_chart.write_html("LIB_anode_materials_patent_trends_stacked.html")
    sib_cathode_area_chart.write_html("SIB_cathode_materials_patent_trends_stacked.html")
    sib_anode_area_chart.write_html("SIB_anode_materials_patent_trends_stacked.html")
    print(f"  Stacked area chart HTML files saved: {time.time() - save_start:.1f}s")
else:
    print("Skipping HTML export")

# Optional PNG saving (can be slow)
if SAVE_PNG:
    try:
        print("Saving line chart PNG files...")
        png_start = time.time()
        lib_cathode_chart.write_image("LIB_cathode_materials_patent_trends.png", width=1000, height=600, scale=1)
        lib_anode_chart.write_image("LIB_anode_materials_patent_trends.png", width=1000, height=600, scale=1)
        sib_cathode_chart.write_image("SIB_cathode_materials_patent_trends.png", width=1000, height=600, scale=1)
        sib_anode_chart.write_image("SIB_anode_materials_patent_trends.png", width=1000, height=600, scale=1)
        print(f"  Line chart PNG files saved: {time.time() - png_start:.1f}s")
        
        print("Saving stacked area chart PNG files...")
        png_start = time.time()
        lib_cathode_area_chart.write_image("LIB_cathode_materials_patent_trends_stacked.png", width=1000, height=600, scale=1)
        lib_anode_area_chart.write_image("LIB_anode_materials_patent_trends_stacked.png", width=1000, height=600, scale=1)
        sib_cathode_area_chart.write_image("SIB_cathode_materials_patent_trends_stacked.png", width=1000, height=600, scale=1)
        sib_anode_area_chart.write_image("SIB_anode_materials_patent_trends_stacked.png", width=1000, height=600, scale=1)
        print(f"  Stacked area chart PNG files saved: {time.time() - png_start:.1f}s")
        print("All charts saved as PNG files successfully!")
    except Exception as e:
        print(f"Could not save PNG files: {e}")
        print("Install kaleido for PNG export: pip install kaleido")
else:
    print("Skipping PNG export for faster processing")

#==============================================================
# Summary statistics
#==============================================================

print("\n" + "="*60)
print("SUMMARY STATISTICS")
print("="*60)

# LIB materials summary
if not lib_cathode_counts.empty or not lib_anode_counts.empty:
    lib_combined = pd.concat([lib_cathode_counts, lib_anode_counts], ignore_index=True)
    if not lib_combined.empty:
        lib_total_patents = lib_combined.groupby('Material')['count'].sum().sort_values(ascending=False)
        print("\nLIB Materials - Total Patents:")
        for material, count in lib_total_patents.items():
            print(f"  {material}: {count:,}")

# SIB materials summary
if not sib_cathode_counts.empty or not sib_anode_counts.empty:
    sib_combined = pd.concat([sib_cathode_counts, sib_anode_counts], ignore_index=True)
    if not sib_combined.empty:
        sib_total_patents = sib_combined.groupby('Material')['count'].sum().sort_values(ascending=False)
        print("\nSIB Materials - Total Patents:")
        for material, count in sib_total_patents.items():
            print(f"  {material}: {count:,}")

print(f"\nTotal processing time: {time.time() - start_time:.1f} seconds")
print("Optimized line charts and stacked area charts created successfully!")
print("="*60)

#%%
# Figure 1b1: Market shares per LIB cathode chemistry

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.ticker as ticker

# Read data
file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Figure 1\LIB market share by chemistry over time.xlsx"
df = pd.read_excel(file_path, header=3)  # Header starts at row 4 (index 3)

# Clean column names (remove spaces if present)
df.columns = df.columns.str.strip()

# Remove percentage signs and convert to numeric values, then multiply by 100
chemistry_columns = ['LCO', 'NMC', 'NCA', 'LMO', 'LFP']
for col in chemistry_columns:
    if df[col].dtype == 'object':
        # Remove percentage signs and convert to decimal values
        df[col] = df[col].astype(str).str.replace('%', '').astype(float)
    # Values are given as percentages, multiply by 100 to get proper percentage values
    df[col] = df[col] * 100

# Market size column is "LIB"
market_size_col = 'LIB'

print(f"Available columns: {list(df.columns)}")
print(f"Used market size column: {market_size_col}")

# Clean market size column (handle commas as decimal separators if present)
if df[market_size_col].dtype == 'object':
    df[market_size_col] = df[market_size_col].astype(str).str.replace(',', '.').astype(float)

# Handle missing values
df = df.dropna(subset=['Year'] + chemistry_columns + [market_size_col])

# Create plot
fig, ax1 = plt.subplots(figsize=(14, 8))

# Define colors for battery chemistries
colors = {
    'LCO': '#FF0000',  # Bright red (layered oxide)
    'NMC': '#B22222',  # Fire brick red (layered oxide)
    'NCA': '#FF6B6B',  # Light red/coral (layered oxide)
    'LFP': '#0000FF',  # Blue (polyanionic)
    'LMO': '#00FFFF'   # Cyan (spinel)
}

# Stacked area chart for market shares
ax1.stackplot(df['Year'], 
              df['LCO'], df['LFP'], df['NMC'], df['NCA'], df['LMO'],
              labels=['LCO', 'LFP', 'NMC', 'NCA', 'LMO'],
              colors=[colors[chem] for chem in chemistry_columns],
              alpha=0.8)

# Configure first Y-axis (market shares)
ax1.set_xlabel('Year', fontsize=16, fontweight='bold')
ax1.set_ylabel('Market Share (%)', fontsize=16, fontweight='bold')
ax1.set_ylim(0, 100)
ax1.grid(True, alpha=0.3)
ax1.tick_params(axis='y', labelcolor='black')

# Create second Y-axis for market size
ax2 = ax1.twinx()
ax2.plot(df['Year'], df[market_size_col], 
         color='yellow', linewidth=4.5, alpha=1, 
         label='Market Size (Million USD)')
ax2.set_ylabel('Market Size (Million USD)', fontsize=16, fontweight='bold')
ax2.tick_params(axis='y', labelcolor='black')

# Y-axis for market size starts at 0
ax2.set_ylim(bottom=0)

# Title
plt.title('Global Lithium-Ion Battery Market Share by Chemistry (1990-2024)', 
          fontsize=16, fontweight='bold', pad=20)

# Legend for battery chemistries
legend1 = ax1.legend(loc='upper left', bbox_to_anchor=(0.02, 0.98), 
                    frameon=True, fancybox=True, shadow=True, fontsize=14)

# Legend for market size
legend2 = ax2.legend(loc='upper right', bbox_to_anchor=(0.53, 0.2),
                    frameon=True, fancybox=True, shadow=True, fontsize=14)

# X-axis and Y-axis tick font sizes
ax1.tick_params(axis='x', labelsize=14)
ax1.tick_params(axis='y', labelsize=14) 
ax2.tick_params(axis='y', labelsize=14)

# Adjust X-axis
ax1.set_xlim(df['Year'].min(), df['Year'].max())
ax1.set_xticks(range(int(df['Year'].min()), int(df['Year'].max()) + 1, 5))

# Add thousand separator to y2-axis (market size)
ax2.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: f'{x:,.0f}'))

# Optimize layout
plt.tight_layout()

# Show plot
plt.show()

# Optional: Save plot
save_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Figure 1\LIB_market_share_chart.png"
try:
    fig.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"Chart saved at: {save_path}")
except:
    print("Chart could not be saved - check file path")

# Print data overview
print("\nData overview:")
print(f"Years: {df['Year'].min()} - {df['Year'].max()}")
print(f"Number of data points: {len(df)}")
print(f"Market size range: {df[market_size_col].min():.0f} - {df[market_size_col].max():.0f} Million USD")

# Show example values from first 5 rows
print("\nSample data (first 5 rows):")
print(df[['Year'] + chemistry_columns + [market_size_col]].head())


#%%

# Figure 1b2: Market shares per LIB anode chemistry

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.ticker as ticker

# Read data from LIB anode sheet
file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Figure 1\LIB market share by chemistry over time.xlsx"
df = pd.read_excel(file_path, sheet_name='LIB anode', header=4)  # Header starts at row 4 (index 3)

# Clean column names (remove spaces if present)
df.columns = df.columns.str.strip()

# Remove percentage signs and convert to numeric values, then multiply by 100
chemistry_columns = ['Graphite', 'Si/C', 'LTO']
for col in chemistry_columns:
    if df[col].dtype == 'object':
        # Remove percentage signs and convert to decimal values
        df[col] = df[col].astype(str).str.replace('%', '').astype(float)
    # Values are given as percentages, multiply by 100 to get proper percentage values
    df[col] = df[col] * 100

# Market size column is "LIB"
market_size_col = 'LIB'

print(f"Available columns: {list(df.columns)}")
print(f"Used market size column: {market_size_col}")

# Clean market size column (handle commas as decimal separators if present)
if df[market_size_col].dtype == 'object':
    df[market_size_col] = df[market_size_col].astype(str).str.replace(',', '.').astype(float)

# Handle missing values
df = df.dropna(subset=['Year'] + chemistry_columns + [market_size_col])

# Create plot
fig, ax1 = plt.subplots(figsize=(14, 8))

# Define colors for anode materials
colors = {
    'Graphite': '#333333',        # Darker gray (carbon)
    'Si/C': '#666666',           # Dark gray (carbon composite)
    'LTO': '#FFA500'             # Orange (titanate)
}

# Stacked area chart for market shares
ax1.stackplot(df['Year'], 
              df['Graphite'], df['Si/C'], df['LTO'],
              labels=['Graphite', 'Si/C Composites', 'LTO'],
              colors=[colors[chem] for chem in chemistry_columns],
              alpha=0.8)

# Configure first Y-axis (market shares)
ax1.set_xlabel('Year', fontsize=16, fontweight='bold')
ax1.set_ylabel('Market Share (%)', fontsize=16, fontweight='bold')
ax1.set_ylim(0, 100)
ax1.grid(True, alpha=0.3)
ax1.tick_params(axis='y', labelcolor='black')

# Create second Y-axis for market size
ax2 = ax1.twinx()
ax2.plot(df['Year'], df[market_size_col], 
         color='yellow', linewidth=4.5, alpha=1, 
         label='Market Size (Million USD)')
ax2.set_ylabel('Market Size (Million USD)', fontsize=16, fontweight='bold')
ax2.tick_params(axis='y', labelcolor='black')

# Y-axis for market size starts at 0
ax2.set_ylim(bottom=0)

# Title
plt.title('Global Lithium-Ion Battery Anode Market Share by Material (1990-2024)', 
          fontsize=16, fontweight='bold', pad=20)

# Legend for anode materials
legend1 = ax1.legend(loc='upper left', bbox_to_anchor=(0.02, 0.98), 
                    frameon=True, fancybox=True, shadow=True, fontsize=14)

# Legend for market size
legend2 = ax2.legend(loc='upper right', bbox_to_anchor=(0.53, 0.2),
                    frameon=True, fancybox=True, shadow=True, fontsize=14)

# X-axis and Y-axis tick font sizes
ax1.tick_params(axis='x', labelsize=14)
ax1.tick_params(axis='y', labelsize=14) 
ax2.tick_params(axis='y', labelsize=14)

# Adjust X-axis
ax1.set_xlim(df['Year'].min(), df['Year'].max())
ax1.set_xticks(range(int(df['Year'].min()), int(df['Year'].max()) + 1, 5))

# Add thousand separator to y2-axis (market size)
ax2.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: f'{x:,.0f}'))

# Optimize layout
plt.tight_layout()

# Show plot
plt.show()

# Optional: Save plot
save_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Figure 1\LIB_anode_market_share_chart.png"
try:
    fig.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"Chart saved at: {save_path}")
except:
    print("Chart could not be saved - check file path")

# Print data overview
print("\nData overview:")
print(f"Years: {df['Year'].min()} - {df['Year'].max()}")
print(f"Number of data points: {len(df)}")
print(f"Market size range: {df[market_size_col].min():.0f} - {df[market_size_col].max():.0f} Million USD")

# Show example values from first 5 rows
print("\nSample data (first 5 rows):")
print(df[['Year'] + chemistry_columns + [market_size_col]].head())

#%%

#=============================================================================================================================================
# Complete Knowledge Flow Analysis with Material-Level AND Structure Family-Level analysis
# CORRECTED VERSION with Backward Citation Normalization using family_citing_lens_id_count
# Includes dynamic y-axis scaling and citation lag visualization
# Y-axis: Percentage of knowledge received (Backward Citation Normalization), X-axis: Priority years of citing patents
# "Where does the knowledge receiver get its knowledge from?" - Shows what percentage of a receiver's total backward citations come from each provider
#=============================================================================================================================================

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from matplotlib.lines import Line2D
import itertools
import matplotlib.gridspec as gridspec
from datetime import datetime

# File paths for both datasets
lib_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\Leo_full_data\API data\LIB_full_data_classified_onlyLIB_27052025.xlsx"
sib_file_path = r"C:\Users\andre\OneDrive\0. Promotion\Eigene Forschung\ETH Zürich\Dataset\Lens\SIB_lens_André Paper\SIB_full_data_classified_final_onlySIB_26052025.xlsx"
sheet_name = "Sheet1"

# Load both Excel files
LIB_patents = pd.read_excel(lib_file_path, sheet_name=sheet_name)
SIB_patents = pd.read_excel(sib_file_path, sheet_name=sheet_name)

# Print basic info about both datasets
print(f"LIB dataset shape: {LIB_patents.shape}")
print(f"SIB dataset shape: {SIB_patents.shape}")

# Add a 'battery_type' column to identify the source in the combined dataset
LIB_patents['battery_type'] = 'LIB'
SIB_patents['battery_type'] = 'SIB'

#==============================================================
# Define material sets for both battery types
#==============================================================

# Define LIB material groups
lib_cathode_materials = {"LCO", "LMO", "LFP", "NMC", "NCA"}
lib_anode_materials = {"Silicon/Carbon", "Graphite", "LTO"}

# Define SIB material groups
sib_cathode_materials = {"Fe-PBA", "Mn-PBA", "NVPF", "NFPP/NFP", "NFM", "CFM", "NMO"}
sib_anode_materials = {"Hard Carbon"}

#==============================================================
# STRUCTURE FAMILY DEFINITIONS AND FUNCTIONS
#==============================================================

def get_structure_families_for_materials(materials_list):
    """
    Map individual materials to their corresponding structure families.
    """
    
    # Define material to structure family mappings
    material_to_family = {
        # LIB Cathode Materials -> Structure Families
        'LCO': 'LIB_Layered_Oxides',
        'NMC': 'LIB_Layered_Oxides', 
        'NCA': 'LIB_Layered_Oxides',
        'LFP': 'LIB_Polyanionic',
        'LMO': 'LIB_Spinel',
        
        # LIB Anode Materials -> Structure Families
        'Graphite': 'LIB_Anode_Graphite_based',
        'Silicon/Carbon': 'LIB_Anode_Graphite_based',
        'LTO': 'LIB_Anode_LTO',
        
        # SIB Cathode Materials -> Structure Families
        'NFM': 'SIB_Layered_Oxides',
        'CFM': 'SIB_Layered_Oxides',
        'NMO': 'SIB_Layered_Oxides',
        'NVPF': 'SIB_Polyanionic',
        'NFPP/NFP': 'SIB_Polyanionic',
        'Fe-PBA': 'SIB_PBA',
        'Mn-PBA': 'SIB_PBA',
        
        # SIB Anode Materials -> Structure Families
        'Hard Carbon': 'SIB_Anode_Hard_Carbon'
    }
    
    # Extract unique structure families for the given materials
    families = set()
    
    if isinstance(materials_list, list):
        for material in materials_list:
            if material in material_to_family:
                families.add(material_to_family[material])
    else:
        # Handle case where materials_list might be a single string or other type
        print(f"Warning: Expected list but got {type(materials_list)}: {materials_list}")
    
    return list(families)

# Define structure family metadata
structure_family_types = {
    'LIB_Layered_Oxides': 'cathode',
    'LIB_Polyanionic': 'cathode',
    'LIB_Spinel': 'cathode',
    'LIB_Anode_Graphite_based': 'anode',
    'LIB_Anode_LTO': 'anode',
    'SIB_Layered_Oxides': 'cathode',
    'SIB_Polyanionic': 'cathode',
    'SIB_PBA': 'cathode',
    'SIB_Anode_Hard_Carbon': 'anode'
}

structure_family_battery_types = {
    'LIB_Layered_Oxides': 'LIB',
    'LIB_Polyanionic': 'LIB',
    'LIB_Spinel': 'LIB',
    'LIB_Anode_Graphite_based': 'LIB',
    'LIB_Anode_LTO': 'LIB',
    'SIB_Layered_Oxides': 'SIB',
    'SIB_Polyanionic': 'SIB',
    'SIB_PBA': 'SIB',
    'SIB_Anode_Hard_Carbon': 'SIB'
}

#==============================================================
# Function to extract materials from material strings
#==============================================================

def extract_materials(material_str, material_set):
    """Extract materials that match the given material set from a comma-separated string."""
    materials = set(material_str.split(", ")) if isinstance(material_str, str) else set()
    extracted = list(materials.intersection(material_set))
    return extracted if extracted else ["none"]

#==============================================================
# Function to create ordered material lists for visualization
#==============================================================

def get_ordered_materials(available_materials):
    """Order materials according to the specified sequence."""
    # Define the ideal order for all materials
    ideal_order = [
        # Cathode layered oxides
        'LCO', 'NMC', 'NCA', 'NFM', 'CFM', 'NMO',
        # Cathode polyanionic materials
        'LFP', 'NVPF', 'NFPP/NFP',
        # Cathode spinel
        'LMO',
        # Cathode PBA
        'Fe-PBA', 'Mn-PBA',
        # Anode materials
        'LTO', 'Silicon/Carbon', 'Graphite', 'Hard Carbon'
    ]
    
    # Return ordered list of available materials
    return [m for m in ideal_order if m in available_materials]

def get_ordered_structure_families(available_families):
    """Order structure families according to the specified sequence."""
    # Define the ideal order for structure families
    ideal_order = [
        # LIB Cathode families
        'LIB_Layered_Oxides', 'LIB_Polyanionic', 'LIB_Spinel',
        # LIB Anode families
        'LIB_Anode_Graphite_based', 'LIB_Anode_LTO',
        # SIB Cathode families
        'SIB_Layered_Oxides', 'SIB_Polyanionic', 'SIB_PBA',
        # SIB Anode families
        'SIB_Anode_Hard_Carbon'
    ]
    
    # Return ordered list of available families
    return [f for f in ideal_order if f in available_families]

#==============================================================
# Process LIB dataset
#==============================================================

# Extract priority year from earliest_priority_date for LIB
LIB_patents["earliest_priority_date"] = pd.to_datetime(LIB_patents["Earliest Priority Date"])
LIB_patents["priority_year"] = LIB_patents["earliest_priority_date"].dt.year
LIB_patents = LIB_patents.sort_values(by=["priority_year"], ascending=True)

# Create new columns for Cathode and Anode Materials for LIB
LIB_patents["Cathode Material"] = LIB_patents["Material-Level"].apply(lambda x: extract_materials(x, lib_cathode_materials))
LIB_patents["Anode Material"] = LIB_patents["Material-Level"].apply(lambda x: extract_materials(x, lib_anode_materials))

# Update Component-Level based on Material-Level for LIB
for index, row in LIB_patents.iterrows():
    if row["Material-Level"] != "none" and row["Component-Level"] == "none":
        materials = set(row["Material-Level"].split(", ")) if isinstance(row["Material-Level"], str) else set()

        cathode_match = materials & lib_cathode_materials
        anode_match = materials & lib_anode_materials

        new_component_level = []
        if cathode_match:
            new_component_level.append("Positive Electrode")
        if anode_match:
            new_component_level.append("Negative Electrode")

        LIB_patents.at[index, "Component-Level"] = ", ".join(new_component_level) if new_component_level else "none"

# Create family_citing_lens_id_count for LIB dataset by counting citations
LIB_patents["family_citing_lens_id_count"] = LIB_patents["family_citing_lens_id"].apply(
    lambda x: len(x.split(", ")) if isinstance(x, str) and x else 0
)

#==============================================================
# Process SIB dataset
#==============================================================

# Extract priority year from earliest_priority_date for SIB
SIB_patents["earliest_priority_date"] = pd.to_datetime(SIB_patents["Earliest Priority Date"])
SIB_patents["priority_year"] = SIB_patents["earliest_priority_date"].dt.year
SIB_patents = SIB_patents.sort_values(by=["priority_year"], ascending=True)

# Create new columns for Cathode and Anode Materials for SIB
SIB_patents["Cathode Material"] = SIB_patents["Material-Level"].apply(lambda x: extract_materials(x, sib_cathode_materials))
SIB_patents["Anode Material"] = SIB_patents["Material-Level"].apply(lambda x: extract_materials(x, sib_anode_materials))

# Update Component-Level based on Material-Level for SIB
for index, row in SIB_patents.iterrows():
    if row["Material-Level"] != "none" and row["Component-Level"] == "none":
        materials = set(row["Material-Level"].split(", ")) if isinstance(row["Material-Level"], str) else set()

        cathode_match = materials & sib_cathode_materials
        anode_match = materials & sib_anode_materials

        new_component_level = []
        if cathode_match:
            new_component_level.append("Positive Electrode")
        if anode_match:
            new_component_level.append("Negative Electrode")

        SIB_patents.at[index, "Component-Level"] = ", ".join(new_component_level) if new_component_level else "none"

# Create family_citing_lens_id_count for SIB dataset by counting citations
SIB_patents["family_citing_lens_id_count"] = SIB_patents["family_citing_lens_id"].apply(
    lambda x: len(x.split(", ")) if isinstance(x, str) and x else 0
)

#==============================================================
# Filter patents with citations and apply further processing
#==============================================================

lib_df = LIB_patents
sib_df = SIB_patents

# Extract lists of citing patents
lib_df["List of Citing Patents"] = lib_df["family_citing_lens_id"].fillna("")
lib_df["List of Citing Patents"] = lib_df["family_citing_lens_id"].apply(
    lambda x: [p.strip() for p in x.split(", ") if p.strip()]
)

sib_df["List of Citing Patents"] = sib_df["family_citing_lens_id"].fillna("")
sib_df["List of Citing Patents"] = sib_df["family_citing_lens_id"].apply(
    lambda x: [p.strip() for p in x.split(", ") if p.strip()]
)

# Remove patents that have neither material nor component information
lib_df = lib_df[~((lib_df['Material-Level'] == 'none') & (lib_df['Component-Level'] == 'none'))]
sib_df = sib_df[~((sib_df['Material-Level'] == 'none') & (sib_df['Component-Level'] == 'none'))]

#==============================================================
# Combine datasets for the citation analysis
#==============================================================

# Add origin identifiers to node numbers to prevent conflicts
lib_df["original_index"] = lib_df.index
sib_df["original_index"] = sib_df.index + len(lib_df)  # Ensure unique node numbers

# Combine datasets
combined_df = pd.concat([lib_df, sib_df], ignore_index=True)

#==============================================================
# Filter patents that have either LIB or SIB marked as "Yes"
#==============================================================
print("Original combined_df shape:", combined_df.shape)

# Filter to keep only rows where either LIB or SIB is "Yes"
combined_df = combined_df[(combined_df['LIB'] == 'Yes') | (combined_df['SIB'] == 'Yes')]
print("After LIB/SIB filter - combined_df shape:", combined_df.shape)

#==============================================================
# Filter to remove rows without material information
#==============================================================
# Filter to remove rows where both Cathode Material and Anode Material are only ["none"]
combined_df = combined_df[~((combined_df['Cathode Material'].apply(lambda x: x == ['none'])) & 
                          (combined_df['Anode Material'].apply(lambda x: x == ['none'])))]
print("After removing rows with only ['none'] materials - combined_df shape:", combined_df.shape)

#==============================================================
# Create a continuous node numbering for the combined dataset
#==============================================================
combined_df["node_num"] = range(len(combined_df))

#==============================================================
# Create mapping from Lens IDs to node numbers
#==============================================================
# Build dictionary to map Lens IDs to node numbers
lens_id_to_node = {}

# Process each patent family to map all IDs to their node number
for index, row in combined_df.iterrows():
    # Extract lens IDs from Simple Family Members
    if isinstance(row['Simple Family Members'], str):
        for lens_id in row['Simple Family Members'].split(';;'):
            lens_id = lens_id.strip()
            if lens_id:  # Ensure non-empty string
                lens_id_to_node[lens_id] = row["node_num"]

# Function to find node numbers for citing patents
def find_citing_nodes(citing_list):
    """Convert a list of citing patent IDs to their corresponding node numbers."""
    citing_nodes = set()
    for citing_patent in citing_list:
        if citing_patent in lens_id_to_node:
            citing_nodes.add(lens_id_to_node[citing_patent])
    return list(citing_nodes)

# Add citing node numbers to the dataframe
combined_df["Citing_node_numbers"] = combined_df["List of Citing Patents"].apply(find_citing_nodes)

#==============================================================
# Citation Analysis Functions
#==============================================================

# Function to classify citation lag
def classify_citation_lag(citing_year, cited_year):
    """Classify citation lag as short (<=5 years) or long (>5 years)."""
    lag = citing_year - cited_year
    return "short" if lag <= 5 else "long"

# Function to create a two-shade color gradient
def create_citation_lag_colors(base_color):
    """Create light and dark shades for citation lag visualization."""
    # Create a palette with multiple shades of the base color
    color_palette = sns.light_palette(base_color, n_colors=5)
    return {
        'short': color_palette[1],  # Lighter shade of the material color
        'long': color_palette[4]    # Darker shade of the material color
    }

#==============================================================
# MATERIAL-LEVEL CITATION DATASET CREATION
#==============================================================

def create_citation_dataset(df):
    """Create a dataset with all citations between different materials."""
    
    # Lists for data storage
    citation_data = []
    
    # Create lookup dictionaries
    cathode_materials_dict = df.set_index('node_num')['Cathode Material'].to_dict()
    anode_materials_dict = df.set_index('node_num')['Anode Material'].to_dict()
    priority_date_dict = df.set_index('node_num')['earliest_priority_date'].to_dict()
    priority_year_dict = df.set_index('node_num')['priority_year'].to_dict()
    battery_type_dict = df.set_index('node_num')['battery_type'].to_dict()
    
    
    # Iterate through all rows
    for _, row in df.iterrows():
        cited_patent = row['node_num']
        cited_cathode_materials = row['Cathode Material']
        cited_anode_materials = row['Anode Material']
        cited_priority_date = row['earliest_priority_date']
        cited_priority_year = row['priority_year']
        cited_battery_type = row['battery_type']

        # Iterate through all citing patents
        for citing_patent in row['Citing_node_numbers']:
            # Check if the citing patent exists in the dataset
            if citing_patent in cathode_materials_dict and citing_patent in anode_materials_dict:
                citing_cathode_materials = cathode_materials_dict[citing_patent]
                citing_anode_materials = anode_materials_dict[citing_patent]
                citing_priority_date = priority_date_dict[citing_patent]
                citing_priority_year = priority_year_dict[citing_patent]
                citing_battery_type = battery_type_dict[citing_patent]

                
                # Skip self-citations
                if cited_patent == citing_patent:
                    continue
                
                # Calculate citation lag
                citation_lag = classify_citation_lag(citing_priority_year, cited_priority_year)
                
                # All combinations of cathode-cathode materials
                for cited_cathode in cited_cathode_materials:
                    for citing_cathode in citing_cathode_materials:
                        if cited_cathode != "none" and citing_cathode != "none":
                            citation_data.append({
                                'cited_patent': cited_patent,
                                'cited_material': cited_cathode,
                                'cited_material_type': 'cathode',
                                'cited_priority_date': cited_priority_date,
                                'cited_priority_year': cited_priority_year,
                                'cited_battery_type': cited_battery_type,
                                'citing_patent': citing_patent,
                                'citing_material': citing_cathode,
                                'citing_material_type': 'cathode',
                                'citing_priority_date': citing_priority_date,
                                'citing_priority_year': citing_priority_year,
                                'citing_battery_type': citing_battery_type,
                                'citation_lag': citation_lag
                            })
                
                # All combinations of anode-anode materials
                for cited_anode in cited_anode_materials:
                    for citing_anode in citing_anode_materials:
                        if cited_anode != "none" and citing_anode != "none":
                            citation_data.append({
                                'cited_patent': cited_patent,
                                'cited_material': cited_anode,
                                'cited_material_type': 'anode',
                                'cited_priority_date': cited_priority_date,
                                'cited_priority_year': cited_priority_year,
                                'cited_battery_type': cited_battery_type,
                                'citing_patent': citing_patent,
                                'citing_material': citing_anode,
                                'citing_material_type': 'anode',
                                'citing_priority_date': citing_priority_date,
                                'citing_priority_year': citing_priority_year,
                                'citing_battery_type': citing_battery_type,
                                'citation_lag': citation_lag
                            })
                
                # All cross-component combinations: cathode-anode
                for cited_cathode in cited_cathode_materials:
                    for citing_anode in citing_anode_materials:
                        if cited_cathode != "none" and citing_anode != "none":
                            citation_data.append({
                                'cited_patent': cited_patent,
                                'cited_material': cited_cathode,
                                'cited_material_type': 'cathode',
                                'cited_priority_date': cited_priority_date,
                                'cited_priority_year': cited_priority_year,
                                'cited_battery_type': cited_battery_type,
                                'citing_patent': citing_patent,
                                'citing_material': citing_anode,
                                'citing_material_type': 'anode',
                                'citing_priority_date': citing_priority_date,
                                'citing_priority_year': citing_priority_year,
                                'citing_battery_type': citing_battery_type,
                                'citation_lag': citation_lag
                            })
                
                # All cross-component combinations: anode-cathode
                for cited_anode in cited_anode_materials:
                    for citing_cathode in citing_cathode_materials:
                        if cited_anode != "none" and citing_cathode != "none":
                            citation_data.append({
                                'cited_patent': cited_patent,
                                'cited_material': cited_anode,
                                'cited_material_type': 'anode',
                                'cited_priority_date': cited_priority_date,
                                'cited_priority_year': cited_priority_year,
                                'cited_battery_type': cited_battery_type,
                                'citing_patent': citing_patent,
                                'citing_material': citing_cathode,
                                'citing_material_type': 'cathode',
                                'citing_priority_date': citing_priority_date,
                                'citing_priority_year': citing_priority_year,
                                'citing_battery_type': citing_battery_type,
                                'citation_lag': citation_lag
                            })
    
    # Create DataFrame from collected data
    citations_df = pd.DataFrame(citation_data)
    
    print(f"Material-level citation dataset created with {len(citations_df)} citations")
    return citations_df


def count_individual_citations_between_families(citing_family_node, cited_family_node, combined_df):
    """
    Count how many individual patent-to-patent citations exist between two patent families.
    """
    # Get the cited family's row
    cited_family_row = combined_df[combined_df['node_num'] == cited_family_node]
    if cited_family_row.empty:
        return 0
    
    cited_family_row = cited_family_row.iloc[0]
    
    # Get all individual patent IDs in the cited family
    if isinstance(cited_family_row["Simple Family Members"], str):
        cited_individual_patents = set(cited_family_row["Simple Family Members"].split(";;"))
    else:
        return 0
    
    # Get the citing family's row
    citing_family_row = combined_df[combined_df['node_num'] == citing_family_node]
    if citing_family_row.empty:
        return 0
    
    citing_family_row = citing_family_row.iloc[0]
    
    # Get all citations made by the citing family
    if isinstance(citing_family_row["List of Citing Patents"], list):
        individual_citations_made = set(citing_family_row["List of Citing Patents"])
    else:
        return 0
    
    # Count how many citations from citing family target the cited family
    citation_count = len(individual_citations_made.intersection(cited_individual_patents))
    
    return citation_count

#==============================================================
# STRUCTURE FAMILY CITATION DATASET CREATION
#==============================================================

def create_structure_family_citation_dataset(df):
    """Create a dataset with all citations between different chemical structure families."""
    
    print("Creating structure family citation dataset...")
    citation_data = []
    
    # Create lookup dictionaries
    cathode_materials_dict = df.set_index('node_num')['Cathode Material'].to_dict()
    anode_materials_dict = df.set_index('node_num')['Anode Material'].to_dict()
    priority_date_dict = df.set_index('node_num')['earliest_priority_date'].to_dict()
    priority_year_dict = df.set_index('node_num')['priority_year'].to_dict()
    battery_type_dict = df.set_index('node_num')['battery_type'].to_dict()
    
    family_level_dict = df.set_index('node_num')['family_level_cited_patent_counts'].to_dict()
    cites_count_dict = df.set_index('node_num')['Cites Patent Count'].to_dict()

    # Funktion zur Bestimmung der backward citations
    def get_backward_citations(node_num):
        """Bestimmt die Anzahl der backward citations für ein Patent."""
        family_level = family_level_dict.get(node_num, 0)
        cites_count = cites_count_dict.get(node_num, 0)
        
        # Prüfung: family_level > cites_count verwenden, sonst cites_count
        if family_level > cites_count:
            result = family_level
        else:
            result = cites_count
        
        # Wenn beide 0 sind, dann 1 setzen (da citing patent mindestens 1 Zitation hat)
        if result == 0:
            result = 1
        
        return result
    
    # Lookup-Dictionary erstellen
    backward_citations_dict = {node_num: get_backward_citations(node_num) for node_num in df['node_num']}
    
    print(f"Processing {len(df)} patents for structure family citations...")
    
    # Iterate through all rows
    for idx, row in df.iterrows():
        if idx % 1000 == 0:  # Progress indicator
            print(f"Processing patent {idx+1}/{len(df)}")
            
        cited_patent = row['node_num']
        cited_cathode_materials = row['Cathode Material']
        cited_anode_materials = row['Anode Material']
        cited_priority_date = row['earliest_priority_date']
        cited_priority_year = row['priority_year']
        cited_battery_type = row['battery_type']
        
        # Combine cathode and anode materials
        cited_all_materials = []
        if isinstance(cited_cathode_materials, list):
            cited_all_materials.extend(cited_cathode_materials)
        if isinstance(cited_anode_materials, list):
            cited_all_materials.extend(cited_anode_materials)
        
        # Get structure families for cited patent
        cited_families = get_structure_families_for_materials(cited_all_materials)
        
        # Skip if no valid structure families found
        if not cited_families:
            continue
        
        # Check if 'Citing_node_numbers' exists and is not empty
        citing_patents = row.get('Citing_node_numbers', [])
        if not isinstance(citing_patents, list):
            citing_patents = []
        
        # Iterate through all citing patents
        for citing_patent in citing_patents:
            # Check if the citing patent exists in the dataset
            if citing_patent in cathode_materials_dict and citing_patent in anode_materials_dict:
                citing_cathode_materials = cathode_materials_dict[citing_patent]
                citing_anode_materials = anode_materials_dict[citing_patent]
                citing_priority_date = priority_date_dict[citing_patent]
                citing_priority_year = priority_year_dict[citing_patent]
                citing_battery_type = battery_type_dict[citing_patent]
                
                # Skip self-citations
                if cited_patent == citing_patent:
                    continue
                
                # Combine citing materials
                citing_all_materials = []
                if isinstance(citing_cathode_materials, list):
                    citing_all_materials.extend(citing_cathode_materials)
                if isinstance(citing_anode_materials, list):
                    citing_all_materials.extend(citing_anode_materials)
                
                # Get structure families for citing patent
                citing_families = get_structure_families_for_materials(citing_all_materials)
                
                # Skip if no valid structure families found
                if not citing_families:
                    continue
                
                # Calculate citation lag
                citation_lag = classify_citation_lag(citing_priority_year, cited_priority_year)
                
                # NEW: Count individual citations between these two families
                individual_citation_count = count_individual_citations_between_families(
                    citing_patent, cited_patent, df
                )
                
                # Create citations between all combinations of structure families
                for cited_family in cited_families:
                    for citing_family in citing_families:
                        citation_data.append({
                            'cited_patent': cited_patent,
                            'cited_structure_family': cited_family,
                            'cited_family_type': structure_family_types[cited_family],
                            'cited_battery_type': structure_family_battery_types[cited_family],
                            'cited_priority_date': cited_priority_date,
                            'cited_priority_year': cited_priority_year,
                            'citing_patent': citing_patent,
                            'citing_structure_family': citing_family,
                            'citing_family_type': structure_family_types[citing_family],
                            'citing_battery_type': structure_family_battery_types[citing_family],
                            'citing_priority_date': citing_priority_date,
                            'citing_priority_year': citing_priority_year,
                            'citation_lag': citation_lag,
                            'citing_patent_backward_citations': backward_citations_dict.get(citing_patent, 1),
                            'individual_citation_count': max(1, individual_citation_count)
                        })
    
    # Create DataFrame from collected data
    citations_df = pd.DataFrame(citation_data)
    
    print(f"Structure family citation dataset created with {len(citations_df)} citations")
    
    # Print summary statistics
    if len(citations_df) > 0:
        print("\nStructure Family Citation Summary:")
        print(f"Unique cited families: {citations_df['cited_structure_family'].nunique()}")
        print(f"Unique citing families: {citations_df['citing_structure_family'].nunique()}")
        print(f"Citation lag distribution:")
        print(citations_df['citation_lag'].value_counts())
    
    return citations_df

#==============================================================
# CORRECTED AGGREGATION FUNCTIONS FOR BACKWARD CITATION NORMALIZATION
#==============================================================

def create_material_aggregated_df(combined_df):
    """Create aggregated DataFrame at material level with backward citation counts."""
    material_data = []
    
    for _, row in combined_df.iterrows():
        # Process cathode materials
        for material in row['Cathode Material']:
            if material != 'none':
                material_data.append({
                    'material': material,
                    'material_type': 'cathode',
                    'battery_type': row['battery_type'],
                    'backward_citations': row['family_citing_lens_id_count']
                })
        
        # Process anode materials
        for material in row['Anode Material']:
            if material != 'none':
                material_data.append({
                    'material': material,
                    'material_type': 'anode', 
                    'battery_type': row['battery_type'],
                    'backward_citations': row['family_citing_lens_id_count']
                })
    
    # Create DataFrame and aggregate by material
    material_df = pd.DataFrame(material_data)
    
    # Sum backward citations for each unique material
    material_aggregated = material_df.groupby(['material', 'material_type', 'battery_type']).agg({
        'backward_citations': 'sum'
    }).reset_index()
    
    return material_aggregated

def create_structure_family_aggregated_df(combined_df):
    """Create aggregated DataFrame at structure family level with backward citation counts."""
    family_data = []
    
    for _, row in combined_df.iterrows():
        # Get all materials for this patent
        all_materials = []
        
        # Add cathode materials
        for material in row['Cathode Material']:
            if material != 'none':
                all_materials.append(material)
        
        # Add anode materials  
        for material in row['Anode Material']:
            if material != 'none':
                all_materials.append(material)
        
        # Get structure families for these materials
        families = get_structure_families_for_materials(all_materials)
        
        # Add entry for each family
        for family in families:
            family_data.append({
                'structure_family': family,
                'family_type': structure_family_types[family],
                'battery_type': structure_family_battery_types[family],
                'backward_citations': row['family_citing_lens_id_count']
            })
    
    # Create DataFrame and aggregate by family
    family_df = pd.DataFrame(family_data)
    
    # Sum backward citations for each unique family
    family_aggregated = family_df.groupby(['structure_family', 'family_type', 'battery_type']).agg({
        'backward_citations': 'sum'
    }).reset_index()
    
    return family_aggregated

#==============================================================
# CORRECTED KNOWLEDGE FLOW VISUALIZATION FUNCTION
#==============================================================

def create_knowledge_flow_matrix(
    citations_df, 
    combined_df,  # Original combined DataFrame
    provider_items, provider_battery_type,
    receiver_items, receiver_battery_type,
    color_map, title_base, 
    item_type="material"
):
    """Create knowledge flow matrix with correct backward citation normalization."""
    
    if citations_df.empty:
        print("No citation data available for visualization")
        return
    
    # Create aggregated DataFrames for looking up backward citation counts
    if item_type == "material":
        aggregated_df = create_material_aggregated_df(combined_df)
        cited_col = 'cited_material'
        citing_col = 'citing_material'
        cited_type_col = 'cited_material_type'
        citing_type_col = 'citing_material_type'
        ordering_func = get_ordered_materials
        lookup_col = 'material'
        lookup_type_col = 'material_type'
    else:  # structure_family
        aggregated_df = create_structure_family_aggregated_df(combined_df)
        cited_col = 'cited_structure_family'
        citing_col = 'citing_structure_family'
        cited_type_col = 'cited_family_type'
        citing_type_col = 'citing_family_type'
        ordering_func = get_ordered_structure_families
        lookup_col = 'structure_family'
        lookup_type_col = 'family_type'
    
    # Filter citations
    if receiver_battery_type is None:
        focused_df = citations_df[
            (citations_df[cited_col].isin(provider_items)) &
            (citations_df['cited_battery_type'] == provider_battery_type) &
            (citations_df[citing_col].isin(receiver_items))
        ]
    else:
        focused_df = citations_df[
            (citations_df[cited_col].isin(provider_items)) &
            (citations_df['cited_battery_type'] == provider_battery_type) &
            (citations_df[citing_col].isin(receiver_items)) &
            (citations_df['citing_battery_type'] == receiver_battery_type)
        ]
    
    if focused_df.empty:
        print(f"No citations found for the selected {item_type} groups")
        return
    
    # Order items
    provider_items_ordered = ordering_func(provider_items)
    receiver_items_ordered = ordering_func(receiver_items)
    
    # Years for x-axis
    year_min = int(focused_df['citing_priority_year'].min())
    year_max = int(focused_df['citing_priority_year'].max())
    year_values = list(range(year_min, year_max + 1))
    
    # Calculate total backward citations for each receiver
    total_receiver_backward_citations = {}
    
    for receiver_item in receiver_items_ordered:
        # Look up in aggregated DataFrame
        receiver_data = aggregated_df[
            (aggregated_df[lookup_col] == receiver_item) &
            (aggregated_df['battery_type'] == receiver_battery_type if receiver_battery_type else True)
        ]
        
        if not receiver_data.empty:
            total_receiver_backward_citations[receiver_item] = receiver_data['backward_citations'].sum()
        else:
            print(f"Warning: No data found for receiver {receiver_item}")
            total_receiver_backward_citations[receiver_item] = 1
    
    # Calculate figure dimensions
    cell_size = 3
    provider_count = len(provider_items_ordered)
    receiver_count = len(receiver_items_ordered)
    
    left_margin = 1.5
    right_margin = 1.0
    top_margin = 1.0
    bottom_margin = 2.0
    
    fig_width = left_margin + (cell_size * provider_count) + right_margin
    fig_height = top_margin + (cell_size * receiver_count) + bottom_margin
    
    fig = plt.figure(figsize=(fig_width, fig_height))
    
    grid_left = left_margin / fig_width
    grid_right = 1 - (right_margin / fig_width)
    grid_bottom = bottom_margin / fig_height
    grid_top = 1 - (top_margin / fig_height)
    
    outer_grid = gridspec.GridSpec(
        receiver_count, provider_count, 
        left=grid_left, right=grid_right,
        bottom=grid_bottom, top=grid_top,
        wspace=0.7, hspace=0.6
    )
    
    # Calculate dynamic y_max
    max_percentage = 0.0
    
    for receiver_item in receiver_items_ordered:
        for provider_item in provider_items_ordered:
            flow_data = focused_df[
                (focused_df[citing_col] == receiver_item) &
                (focused_df[cited_col] == provider_item)
            ]
            
            if not flow_data.empty:
                grouped_data = flow_data.groupby(['citing_priority_year', 'citation_lag']).size().reset_index(name='count')
                total_backward_citations = total_receiver_backward_citations.get(receiver_item, 1)
                
                if total_backward_citations > 0:
                    for year in year_values:
                        year_data = grouped_data[grouped_data['citing_priority_year'] == year]
                        if not year_data.empty:
                            total_count = year_data['count'].sum()
                            percentage = (total_count / total_backward_citations) * 100
                            max_percentage = max(max_percentage, percentage)
    
    y_max = max(5.0, max_percentage * 1.1)
    
    # Get item types from the citation data
    item_types = {}
    for _, row in focused_df.iterrows():
        item_types[row[cited_col]] = row[cited_type_col]
        item_types[row[citing_col]] = row[citing_type_col]
    
    # Create subplots
    for receiver_idx, receiver_item in enumerate(receiver_items_ordered):
        for provider_idx, provider_item in enumerate(provider_items_ordered):
            ax = plt.Subplot(fig, outer_grid[receiver_idx, provider_idx])
            fig.add_subplot(ax)
            
            # Get types
            receiver_type = item_types.get(receiver_item, 'unknown')
            provider_type = item_types.get(provider_item, 'unknown')
            
            # Colors
            provider_color = color_map.get(provider_item, 'gray')
            lag_colors = create_citation_lag_colors(provider_color)
            
            # Filter data for this combination
            flow_data = focused_df[
                (focused_df[citing_col] == receiver_item) &
                (focused_df[cited_col] == provider_item)
            ]
            
            grouped_data = flow_data.groupby(['citing_priority_year', 'citation_lag']).size().reset_index(name='count')
            total_backward_citations = total_receiver_backward_citations.get(receiver_item, 1)
            
            # Plot data
            if not grouped_data.empty and total_backward_citations > 0:
                for year in year_values:
                    year_data = grouped_data[grouped_data['citing_priority_year'] == year]
                    
                    if year_data.empty:
                        continue
                    
                    short_lag_data = year_data[year_data['citation_lag'] == 'short']
                    long_lag_data = year_data[year_data['citation_lag'] == 'long']
                    
                    short_count = short_lag_data['count'].sum() if not short_lag_data.empty else 0
                    long_count = long_lag_data['count'].sum() if not long_lag_data.empty else 0
                    
                    percentage_short = (short_count / total_backward_citations) * 100
                    percentage_long = (long_count / total_backward_citations) * 100
                    
                    if percentage_short > 0 or percentage_long > 0:
                        if percentage_short > 0:
                            ax.bar(year, percentage_short, width=0.8, 
                                   color=lag_colors['short'], alpha=0.8,
                                   label='≤5 years' if year == year_values[0] else "")
                        
                        if percentage_long > 0:
                            ax.bar(year, percentage_long, width=0.8, bottom=percentage_short,
                                   color=lag_colors['long'], alpha=0.8,
                                   label='>5 years' if year == year_values[0] else "")
            
            # Configure plot
            ax.set_xlim(year_min - 1, year_max + 1)
            ax.set_ylim(0, y_max)
            
            if y_max <= 10:
                y_ticks = np.linspace(0, y_max, 6)
            elif y_max <= 50:
                y_ticks = np.arange(0, y_max + 1, 10)
            else:
                y_ticks = np.arange(0, y_max + 1, 20)
            
            ax.set_yticks(y_ticks)
            ax.set_yticklabels([f"{tick:.1f}%" for tick in y_ticks], fontsize=7)
            
            year_step = max(1, (year_max - year_min) // 10)
            x_ticks = list(range(year_min, year_max + 1, year_step))
            ax.set_xticks(x_ticks)
            ax.set_xticklabels([str(year) for year in x_ticks], rotation=90, fontsize=7)
            ax.set_xlabel('Knowledge Transfer Year', fontsize=8)
            
            if provider_idx == 0:
                ax.set_ylabel('% of Total Backward\nCitations', fontsize=8)
            
            ax.grid(axis='both', linestyle='--', alpha=0.3)
            
            # Titles
            provider_type_abbr = 'C' if provider_type == 'cathode' else 'A'
            receiver_type_abbr = 'C' if receiver_type == 'cathode' else 'A'
            
            if item_type == "material":
                ax.set_title(f"{provider_item} ({provider_type_abbr}) → {receiver_item} ({receiver_type_abbr})", 
                             fontsize=9, fontweight='bold')
            else:
                # Format structure family names
                def format_structure_name(name):
                    clean_name = name.replace('LIB_', '').replace('SIB_', '')
                    if 'LTO' in clean_name:
                        clean_name = clean_name.replace('Lto', 'LTO').replace('_LTO', ' LTO')
                    if 'PBA' in clean_name:
                        clean_name = clean_name.replace('Pba', 'PBA').replace('_PBA', ' PBA')
                    formatted = clean_name.replace('_', ' ').title()
                    formatted = formatted.replace('Lto', 'LTO').replace('Pba', 'PBA')
                    return formatted
                
                provider_battery_prefix = 'LIB' if provider_item.startswith('LIB_') else 'SIB'
                receiver_battery_prefix = 'LIB' if receiver_item.startswith('LIB_') else 'SIB'
                provider_display = format_structure_name(provider_item)
                receiver_display = format_structure_name(receiver_item)
                
                ax.set_title(f"{provider_battery_prefix} {provider_display} ({provider_type_abbr}) → {receiver_battery_prefix} {receiver_display} ({receiver_type_abbr})", 
                             fontsize=7, fontweight='bold')
    
    # Legend
    legend_handles = [
        Line2D([0], [0], color='lightblue', lw=4, label='≤5 years (short lag)'),
        Line2D([0], [0], color='darkblue', lw=4, label='>5 years (long lag)')
    ]
    
    fig.legend(legend_handles, ['≤5 years (short lag)', '>5 years (long lag)'], 
               title="Citation Lag",
               loc='lower center', bbox_to_anchor=(0.5, 0.02),
               ncol=2, fontsize='medium')
    
    # Title
    level_text = "Material-Level" if item_type == "material" else "Structure Family-Level"
    title = f"{title_base} - Knowledge Flow Analysis ({level_text})"
    subtitle = f'(% of total backward citations from each provider - Max: {y_max:.1f}%)'
    
    plt.suptitle(title + '\n' + subtitle, fontsize=12, y=0.98)
    
    # Save
    level_suffix = "_Material_Level" if item_type == "material" else "_Structure_Family_Level"
    filename = f"{title_base.replace(' ', '_').replace('-', '_')}{level_suffix}_BackwardCitationNorm.png"
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    
    print(f"Knowledge Flow matrix saved to '{filename}' (Max: {y_max:.1f}%)")
    plt.close(fig)

#==============================================================
# CORRECTED METRICS DATAFRAME FUNCTION
#==============================================================

def create_knowledge_flow_metrics_dataframe(citations_df, combined_df, item_type="material"):
    """Create DataFrame with Knowledge Flow metrics using correct backward citation normalization."""
    
    print(f"Creating Knowledge Flow metrics DataFrame for {item_type} level...")
    
    # Create aggregated DataFrames
    if item_type == "material":
        aggregated_df = create_material_aggregated_df(combined_df)
        cited_col = 'cited_material'
        citing_col = 'citing_material'
        cited_type_col = 'cited_material_type'
        citing_type_col = 'citing_material_type'
        lookup_col = 'material'
        lookup_type_col = 'material_type'
    else:
        aggregated_df = create_structure_family_aggregated_df(combined_df)
        cited_col = 'cited_structure_family'
        citing_col = 'citing_structure_family'
        cited_type_col = 'cited_family_type'
        citing_type_col = 'citing_family_type'
        lookup_col = 'structure_family'
        lookup_type_col = 'family_type'
    
    citation_metrics = []
    
    # Get unique items
    cited_items = set()
    citing_items = set()
    
    for _, row in citations_df.iterrows():
        cited_items.add((row[cited_col], row[cited_type_col], row['cited_battery_type']))
        citing_items.add((row[citing_col], row[citing_type_col], row['citing_battery_type']))
    
    # Calculate total backward citations for each citing item
    total_citing_item_backward_citations = {}
    for citing_info in citing_items:
        citing_item, citing_type, citing_battery = citing_info
        
        if citing_item == "none":
            continue
        
        # Look up in aggregated DataFrame
        citing_item_data = aggregated_df[
            (aggregated_df[lookup_col] == citing_item) &
            (aggregated_df[lookup_type_col] == citing_type) &
            (aggregated_df['battery_type'] == citing_battery)
        ]
        
        if not citing_item_data.empty:
            total_citing_item_backward_citations[citing_info] = citing_item_data['backward_citations'].sum()
        else:
            print(f"Warning: No data found for citing item {citing_item}")
            total_citing_item_backward_citations[citing_info] = 1
    
    # Process all combinations
    for citing_info in citing_items:
        citing_item, citing_type, citing_battery = citing_info
        
        if citing_item == "none":
            continue
            
        total_backward_citations = total_citing_item_backward_citations.get(citing_info, 0)
        
        for cited_info in cited_items:
            cited_item, cited_type, cited_battery = cited_info
            
            if cited_item == "none":
                continue
            
            # Filter citations for this combination
            filtered_citations = citations_df[
                (citations_df[cited_col] == cited_item) &
                (citations_df[cited_type_col] == cited_type) &
                (citations_df['cited_battery_type'] == cited_battery) &
                (citations_df[citing_col] == citing_item) &
                (citations_df[citing_type_col] == citing_type) &
                (citations_df['citing_battery_type'] == citing_battery)
            ]
            
            citation_count = len(filtered_citations)
            
            if citation_count > 0 and total_backward_citations > 0:
                knowledge_flow_percentage = (citation_count / total_backward_citations) * 100
                
                short_lag_count = len(filtered_citations[filtered_citations['citation_lag'] == 'short'])
                long_lag_count = len(filtered_citations[filtered_citations['citation_lag'] == 'long'])
                
                short_lag_percentage = (short_lag_count / total_backward_citations) * 100
                long_lag_percentage = (long_lag_count / total_backward_citations) * 100
                
                metrics_dict = {
                    f'provider_{item_type}': cited_item,
                    f'provider_{item_type}_type': cited_type, 
                    'provider_battery_type': cited_battery,
                    f'receiver_{item_type}': citing_item,
                    f'receiver_{item_type}_type': citing_type,
                    'receiver_battery_type': citing_battery,
                    'receiver_total_backward_citations': total_backward_citations,
                    'citation_count_from_provider': citation_count,
                    'short_lag_citations_from_provider': short_lag_count,
                    'long_lag_citations_from_provider': long_lag_count,
                    'knowledge_flow_percentage': knowledge_flow_percentage,
                    'short_lag_percentage': short_lag_percentage,
                    'long_lag_percentage': long_lag_percentage
                }
                
                citation_metrics.append(metrics_dict)
    
    metrics_df = pd.DataFrame(citation_metrics)
    metrics_df = metrics_df.sort_values(by='knowledge_flow_percentage', ascending=False)
    
    print(f"Created {item_type}-level Knowledge Flow metrics DataFrame with {len(metrics_df)} item pairs")
    return metrics_df

#==============================================================
# Define color maps for materials (PROVIDER colors for knowledge sources)
#==============================================================

# LIB cathode colors - reds for layered oxides, blues for polyanionic, cyan for spinel
lib_cathode_colors = {
    # Layered oxides (reds)
    'LCO': '#FF0000',  # Bright red
    'NMC': '#CC0000',  # Dark red
    'NCA': '#990000',  # Deeper red
    
    # Polyanionic materials (blues)
    'LFP': '#0000FF',  # Blue
    
    # Spinel (cyan)
    'LMO': '#00FFFF',  # Cyan
}

# LIB anode colors - orange for LTO, grays for carbon
lib_anode_colors = {
    'LTO': '#FFA500',        # Orange
    'Silicon/Carbon': '#666666',  # Dark gray
    'Graphite': '#333333',        # Darker gray
}

# SIB cathode colors - reds for layered oxides, blues for polyanionic, greens for PBA
sib_cathode_colors = {
    # Layered oxides (reds)
    'NFM': '#FF6666',    # Light red
    'CFM': '#FF3333',    # Medium red
    'NMO': '#FF9999',    # Pale red
    
    # Polyanionic materials (blues)
    'NVPF': '#0066FF',      # Medium blue
    'NFPP/NFP': '#003399',  # Dark blue
    
    # PBA (greens)
    'Fe-PBA': '#00CC00',    # Green
    'Mn-PBA': '#006600',    # Dark green
}

# SIB anode colors - gray for Hard Carbon
sib_anode_colors = {
    'Hard Carbon': '#999999',  # Gray
}

# Structure family colors
structure_family_colors = {
    # LIB Structure families
    'LIB_Layered_Oxides': '#FF0000',     # Red
    'LIB_Polyanionic': '#0000FF',        # Blue
    'LIB_Spinel': '#00FFFF',             # Cyan
    'LIB_Anode_Graphite_based': '#666666',  # Dark gray
    'LIB_Anode_LTO': '#FFA500',          # Orange
    
    # SIB Structure families
    'SIB_Layered_Oxides': '#FF6666',     # Light red
    'SIB_Polyanionic': '#0066FF',        # Medium blue
    'SIB_PBA': '#00CC00',                # Green
    'SIB_Anode_Hard_Carbon': '#999999',  # Gray
}

#==============================================================
# MAIN EXECUTION WITH CORRECTED FUNCTIONS
#==============================================================

print("="*80)
print("CREATING CITATION DATASETS")
print("="*80)

# Create citation datasets
print("Creating material-level citation dataset...")
material_citations_df = create_citation_dataset(combined_df)

print("Creating structure family citation dataset...")
structure_citations_df = create_structure_family_citation_dataset(combined_df)

print("="*80)
print("CREATING CORRECTED KNOWLEDGE FLOW VISUALIZATIONS")
print("="*80)

# Get available structure families for each battery type and component
lib_cathode_families = [f for f in structure_family_colors.keys() 
                       if f.startswith('LIB_') and structure_family_types[f] == 'cathode']
lib_anode_families = [f for f in structure_family_colors.keys() 
                     if f.startswith('LIB_') and structure_family_types[f] == 'anode']
sib_cathode_families = [f for f in structure_family_colors.keys() 
                       if f.startswith('SIB_') and structure_family_types[f] == 'cathode']
sib_anode_families = [f for f in structure_family_colors.keys() 
                     if f.startswith('SIB_') and structure_family_types[f] == 'anode']

# 1. LIB Cathode Knowledge Flow
print("Creating LIB Cathode Knowledge Flow visualization...")
create_knowledge_flow_matrix(
    material_citations_df, combined_df,
    lib_cathode_colors.keys(), 'LIB',
    lib_cathode_colors.keys(), 'LIB',
    lib_cathode_colors,
    "LIB Cathode Knowledge Flow",
    item_type="material"
)

# 2. LIB Anode Knowledge Flow
print("Creating LIB Anode Knowledge Flow visualization...")
create_knowledge_flow_matrix(
    material_citations_df, combined_df,
    lib_anode_colors.keys(), 'LIB',
    lib_anode_colors.keys(), 'LIB',
    lib_anode_colors,
    "LIB Anode Knowledge Flow",
    item_type="material"
)

# 3. SIB Cathode Knowledge Flow
print("Creating SIB Cathode Knowledge Flow visualization...")
create_knowledge_flow_matrix(
    material_citations_df, combined_df,
    sib_cathode_colors.keys(), 'SIB',
    sib_cathode_colors.keys(), 'SIB',
    sib_cathode_colors,
    "SIB Cathode Knowledge Flow",
    item_type="material"
)

# 4. SIB Anode Knowledge Flow
print("Creating SIB Anode Knowledge Flow visualization...")
create_knowledge_flow_matrix(
    material_citations_df, combined_df,
    sib_anode_colors.keys(), 'SIB',
    sib_anode_colors.keys(), 'SIB',
    sib_anode_colors,
    "SIB Anode Knowledge Flow",
    item_type="material"
)

# 5. LIB to SIB Cathode Knowledge Transfer
print("Creating LIB to SIB Cathode Knowledge Transfer visualization...")
create_knowledge_flow_matrix(
    material_citations_df, combined_df,
    lib_cathode_colors.keys(), 'LIB',
    sib_cathode_colors.keys(), 'SIB',
    lib_cathode_colors,
    "LIB to SIB Cathode Knowledge Transfer",
    item_type="material"
)

# 6. LIB to SIB Anode Knowledge Transfer
print("Creating LIB to SIB Anode Knowledge Transfer visualization...")
create_knowledge_flow_matrix(
    material_citations_df, combined_df,
    lib_anode_colors.keys(), 'LIB',
    sib_anode_colors.keys(), 'SIB',
    lib_anode_colors,
    "LIB to SIB Anode Knowledge Transfer",
    item_type="material"
)

print("="*80)
print("CREATING STRUCTURE FAMILY-LEVEL KNOWLEDGE FLOW VISUALIZATIONS")
print("="*80)

# 1. LIB Cathode Structure Families
create_knowledge_flow_matrix(
    structure_citations_df, combined_df,
    lib_cathode_families, 'LIB',
    lib_cathode_families, 'LIB',
    structure_family_colors,
    "LIB Cathode Structure Family Knowledge Flow",
    item_type="structure_family"
)

# 2. LIB Anode Structure Families
create_knowledge_flow_matrix(
    structure_citations_df, combined_df,
    lib_anode_families, 'LIB',
    lib_anode_families, 'LIB',
    structure_family_colors,
    "LIB Anode Structure Family Knowledge Flow",
    item_type="structure_family"
)

# 3. SIB Cathode Structure Families
create_knowledge_flow_matrix(
    structure_citations_df, combined_df,
    sib_cathode_families, 'SIB',
    sib_cathode_families, 'SIB',
    structure_family_colors,
    "SIB Cathode Structure Family Knowledge Flow",
    item_type="structure_family"
)

# 4. SIB Anode Structure Families
create_knowledge_flow_matrix(
    structure_citations_df, combined_df,
    sib_anode_families, 'SIB',
    sib_anode_families, 'SIB',
    structure_family_colors,
    "SIB Anode Structure Family Knowledge Flow",
    item_type="structure_family"
)

# 5. LIB to SIB Structure Family Transfer
create_knowledge_flow_matrix(
    structure_citations_df, combined_df,
    lib_cathode_families, 'LIB',
    sib_cathode_families, 'SIB',
    structure_family_colors,
    "LIB to SIB Cathode Structure Family Knowledge Transfer",
    item_type="structure_family"
)

# 6. LIB to SIB Anode Structure Family Transfer
create_knowledge_flow_matrix(
    structure_citations_df, combined_df,
    lib_anode_families, 'LIB',
    sib_anode_families, 'SIB',
    structure_family_colors,
    "LIB to SIB Anode Structure Family Knowledge Transfer",
    item_type="structure_family"
)

print("="*80)
print("CREATING SPECIAL LIB POLYANIONIC KNOWLEDGE RECEIVER VISUALIZATION")
print("="*80)

# Special: Knowledge flow TO LIB Polyanionic
create_knowledge_flow_matrix(
    structure_citations_df, combined_df,
    [f for f in structure_family_colors.keys() if f.startswith('LIB_')], 'LIB',  # All LIB as providers
    ['LIB_Polyanionic'], 'LIB',                                                   # LIB Polyanionic as receiver
    structure_family_colors,                                                      # Use PROVIDER colors (knowledge sources)
    "Knowledge Flow TO LIB Polyanionic",
    item_type="structure_family"
)

print("="*80)
print("CREATING CORRECTED METRICS DATAFRAMES")
print("="*80)

# Create metrics with corrected function
material_metrics_df = create_knowledge_flow_metrics_dataframe(
    material_citations_df, combined_df, "material"
)

structure_metrics_df = create_knowledge_flow_metrics_dataframe(
    structure_citations_df, combined_df, "structure_family"
)

# Display results
print("\nTop material pairs by Knowledge Flow Percentage (Backward Citation Normalized):")
display_cols = ['provider_material', 'receiver_material', 'citation_count_from_provider', 
               'receiver_total_backward_citations', 'knowledge_flow_percentage',
               'short_lag_percentage', 'long_lag_percentage']
print(material_metrics_df.head(10)[display_cols])

print("\nTop structure family pairs by Knowledge Flow Percentage (Backward Citation Normalized):")
display_cols_struct = ['provider_structure_family', 'receiver_structure_family', 'citation_count_from_provider', 
                      'receiver_total_backward_citations', 'knowledge_flow_percentage',
                      'short_lag_percentage', 'long_lag_percentage']
print(structure_metrics_df.head(10)[display_cols_struct])

# Save corrected CSV files
material_csv_filename = 'knowledge_flow_metrics_material_level_backward_citation_norm.csv'
structure_csv_filename = 'knowledge_flow_metrics_structure_family_level_backward_citation_norm.csv'

material_metrics_df.to_csv(material_csv_filename, index=False)
structure_metrics_df.to_csv(structure_csv_filename, index=False)

print(f"Corrected material-level metrics saved to {material_csv_filename}")
print(f"Corrected structure family-level metrics saved to {structure_csv_filename}")

print("\n" + "="*80)
print("CORRECTED NORMALIZATION METHODOLOGY")
print("="*80)

print("NORMALIZATION METHOD: Backward Citation Normalization")
print("Formula: Knowledge_Flow_Percentage(i→j) = Citations(i→j) / Total_Backward_Citations(j) × 100%")
print("Where:")
print("  i = Knowledge Provider (cited material/family)")
print("  j = Knowledge Receiver (citing material/family)")
print("  Citations(i→j) = Number of citations from provider i to receiver j")
print("  Total_Backward_Citations(j) = Sum of family_citing_lens_id_count for all patents of receiver j")

print("\nKEY IMPROVEMENT:")
print("- Previous: Only counted LIB/SIB citations as denominator")
print("- Corrected: Uses TOTAL backward citations (family_citing_lens_id_count) including all technologies")
print("- Result: More realistic percentages showing true knowledge flow proportions")

print("\nEXAMPLE:")
print("SIB Material receives:")
print("- 50 citations from LIB materials")
print("- 150 citations from other technologies (lead-acid, NiMH, fuel cells, etc.)")
print("- Total backward citations (family_citing_lens_id_count): 200")
print("- LIB→SIB knowledge flow: 50/200 = 25% (realistic)")
print("- Previous method would show: 50/50 = 100% (misleading)")

print("\nMETHODOLOGY DETAILS:")
print("1. Material Aggregation:")
print("   - Extract all materials (cathode + anode) from each patent")
print("   - Sum family_citing_lens_id_count for each unique material")
print("   - This gives total backward citations per material across all patents")

print("\n2. Structure Family Aggregation:")
print("   - Map materials to structure families for each patent")
print("   - Sum family_citing_lens_id_count for each unique family")
print("   - This gives total backward citations per family across all patents")

print("\n3. Knowledge Flow Calculation:")
print("   - For each provider→receiver pair:")
print("   - Count citations between them in citation dataset")
print("   - Divide by receiver's total backward citations")
print("   - Result: Realistic percentage of knowledge flow")

print("\nOUTPUT FILES:")
print("MATERIAL LEVEL:")
print("- LIB_Cathode_Knowledge_Flow_Material_Level_BackwardCitationNorm.png")
print("- LIB_Anode_Knowledge_Flow_Material_Level_BackwardCitationNorm.png")
print("- SIB_Cathode_Knowledge_Flow_Material_Level_BackwardCitationNorm.png")
print("- SIB_Anode_Knowledge_Flow_Material_Level_BackwardCitationNorm.png")
print("- LIB_to_SIB_Cathode_Knowledge_Transfer_Material_Level_BackwardCitationNorm.png")
print("- LIB_to_SIB_Anode_Knowledge_Transfer_Material_Level_BackwardCitationNorm.png")

print("\nSTRUCTURE FAMILY LEVEL:")
print("- LIB_Cathode_Structure_Family_Knowledge_Flow_Structure_Family_Level_BackwardCitationNorm.png")
print("- LIB_Anode_Structure_Family_Knowledge_Flow_Structure_Family_Level_BackwardCitationNorm.png")
print("- SIB_Cathode_Structure_Family_Knowledge_Flow_Structure_Family_Level_BackwardCitationNorm.png")
print("- SIB_Anode_Structure_Family_Knowledge_Flow_Structure_Family_Level_BackwardCitationNorm.png")
print("- LIB_to_SIB_Cathode_Structure_Family_Knowledge_Transfer_Structure_Family_Level_BackwardCitationNorm.png")
print("- LIB_to_SIB_Anode_Structure_Family_Knowledge_Transfer_Structure_Family_Level_BackwardCitationNorm.png")

print("\nSPECIAL ANALYSIS:")
print("- Knowledge_Flow_TO_LIB_Polyanionic_Structure_Family_Level_BackwardCitationNorm.png")

print("\nMETRICS:")
print("- knowledge_flow_metrics_material_level_backward_citation_norm.csv")
print("- knowledge_flow_metrics_structure_family_level_backward_citation_norm.csv")

print("\n" + "="*80)
print("CORRECTED KNOWLEDGE FLOW ANALYSIS COMPLETE!")
print("="*80)


#%%

#==============================================================
# SIB Knowledge Receiver Analysis - Share of LIB Citations
# Neue Normierung: LIB citation count / Total backward citations of SIB receiver
# Zeigt den Anteil der LIB-Zitationen an allen Zitationen der SIB-Technologie
# X-axis truncated from 2012 to 2022
#==============================================================

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from matplotlib.lines import Line2D
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import warnings
warnings.filterwarnings('ignore')

def determine_linear_fit(x_data, y_data):
   """
   Simple linear fit for guidance lines.
   """
   if len(x_data) < 2:
       return np.array(x_data), np.array(y_data)
   
   x_array = np.array(x_data).reshape(-1, 1)
   y_array = np.array(y_data)
   
   # Generate smooth x values for trend line (within data range only)
   x_smooth = np.linspace(min(x_data), max(x_data), 100).reshape(-1, 1)
   
   try:
       # Linear fit only
       linear_reg = LinearRegression()
       linear_reg.fit(x_array, y_array)
       y_smooth_linear = linear_reg.predict(x_smooth)
       
       return x_smooth.flatten(), y_smooth_linear
               
   except Exception as e:
       # Fallback to simple linear interpolation
       return np.array(x_data), np.array(y_data)


def create_sib_knowledge_receiver_share_analysis(structure_citations_df, structure_family_colors, combined_df):
   """
   Create visualization showing LIB citation share of total backward citations for each SIB receiver.
   Normalization: LIB_citation_count / total_backward_citations_of_SIB_receiver_per_year
   X-axis truncated from 2012 to 2022.
   """
   
   # Define knowledge providers (LIB structure families) and receivers (SIB structure families)
   lib_providers = [
       'LIB_Layered_Oxides',
       'LIB_Polyanionic', 
       'LIB_Spinel',
       'LIB_Anode_Graphite_based',
       'LIB_Anode_LTO'
   ]
   
   sib_receivers = [
       'SIB_Layered_Oxides',
       'SIB_Polyanionic',
       'SIB_PBA', 
       'SIB_Anode_Hard_Carbon'
   ]
   
   # Display names for better readability
   display_names = {
       'SIB_Layered_Oxides': 'SIB Layered Oxides (4)',
       'SIB_Polyanionic': 'SIB Polyanionic (5)',
       'SIB_PBA': 'SIB PBA (6)',
       'SIB_Anode_Hard_Carbon': 'SIB Hard Carbon (9)'
   }
   
   # Convert RGBA to RGB for matplotlib (since we'll handle alpha separately)
   sib_receiver_colors_rgb = {
       'SIB_Layered_Oxides': (255/255, 102/255, 102/255),    # Light Red
       'SIB_Polyanionic': (0/255, 102/255, 255/255),         # Medium Blue
       'SIB_PBA': (0/255, 204/255, 0/255),                   # Green
       'SIB_Anode_Hard_Carbon': (153/255, 153/255, 153/255)  # Light Gray
   }
   
   # Filter to LIB→SIB flows only
   lib_to_sib_citations = structure_citations_df[
       (structure_citations_df['cited_structure_family'].isin(lib_providers)) &
       (structure_citations_df['citing_structure_family'].isin(sib_receivers))
   ]
   
   # Get ALL citations for SIB receivers (not just from LIB) to calculate total backward citations
   all_sib_citations = structure_citations_df[
       structure_citations_df['citing_structure_family'].isin(sib_receivers)
   ]
   
   if lib_to_sib_citations.empty:
       print("No LIB→SIB citations found in the dataset")
       return
   
   if all_sib_citations.empty:
       print("No SIB citations found in the dataset")
       return
   
   print(f"Found {len(lib_to_sib_citations)} LIB→SIB citations for analysis")
   print(f"Found {len(all_sib_citations)} total SIB citations for normalization")
   
   # Year range - TRUNCATED FROM 2012 TO 2022 with extended limits
   year_min, year_max = 2011.5, 2022.5  # Plot limits extended for visibility
   year_range = list(range(2012, 2023))  # Data range (2012-2022 inclusive)
   year_ticks = list(range(2012, 2023))  # Tick labels
   
   # Create list to store all plotted data for verification
   plotted_data_list = []
   
   # Create figure with single plot - REDUCED SIZE
   fig, ax = plt.subplots(1, 1, figsize=(10, 6.67))  # Reduced from 15x10 to 10x6.67 (same ratio)
   
   # Collect all data for each SIB receiver
   all_plot_data = {}
   global_max_y = 0
   
   for sib_receiver in sib_receivers:
       print(f"\nProcessing {display_names[sib_receiver]}...")
       
       # Filter LIB→SIB citations for this specific SIB receiver
       lib_receiver_citations = lib_to_sib_citations[
           lib_to_sib_citations['citing_structure_family'] == sib_receiver
       ]
       
       # Filter ALL citations for this specific SIB receiver (for total backward citations)
       all_receiver_citations = all_sib_citations[
           all_sib_citations['citing_structure_family'] == sib_receiver
       ]
       
       if lib_receiver_citations.empty:
           print(f"No LIB citations found for {sib_receiver}")
           continue
           
       if all_receiver_citations.empty:
           print(f"No total citations found for {sib_receiver}")
           continue
       
       # Calculate citation counts and backward citations for each year
       year_data = {}
       for year in year_range:
           # Count LIB→SIB citations for this year
           lib_year_citations = lib_receiver_citations[
               lib_receiver_citations['citing_priority_year'] == year
           ]
           
           # Count WEIGHTED individual citations (sum of individual_citation_count)
           lib_citation_count = lib_year_citations['individual_citation_count'].sum()
           
           # Calculate total backward citations for this SIB receiver in this year
           # Sum of citing_patent_backward_citations for UNIQUE citing patents
           all_year_citations = all_receiver_citations[
               all_receiver_citations['citing_priority_year'] == year
           ]
           
           if len(all_year_citations) == 0:
               total_backward_citations = 0
           else:
               # Get unique citing patents and sum their backward citations
               unique_patents_data = all_year_citations.drop_duplicates(subset=['citing_patent'])
               total_backward_citations = unique_patents_data['citing_patent_backward_citations'].sum()
           
           year_data[year] = {
               'lib_citation_count': lib_citation_count,
               'total_backward_citations': total_backward_citations
           }
       
       # Calculate share values for each year
       years_to_plot = []
       share_values = []
       
       for year in year_range:
           lib_count = year_data[year]['lib_citation_count']
           total_backward = year_data[year]['total_backward_citations']
           
           # Only plot years where we have both LIB citations AND total backward citations
           if lib_count > 0 and total_backward > 0:
               # NEW NORMALIZATION: lib_citation_count / total_backward_citations
               share_value = lib_count / total_backward
               
               years_to_plot.append(year)
               share_values.append(share_value)
               
               # Store data for verification
               plotted_data_list.append({
                   'receiver': sib_receiver,
                   'year': year,
                   'lib_citation_count': lib_count,
                   'total_backward_citations': total_backward,
                   'share_value': share_value
               })
       
       # Store data if we have any
       if years_to_plot:
           print(f"  Found {len(years_to_plot)} data points")
           print(f"  Share range: {min(share_values):.3f} - {max(share_values):.3f}")
           
           # Calculate linear fit
           if len(years_to_plot) >= 2:
               trend_x, trend_y = determine_linear_fit(years_to_plot, share_values)
               max_trend_y = max(trend_y) if len(trend_y) > 0 else 0
               global_max_y = max(global_max_y, max(share_values), max_trend_y)
           else:
               trend_x, trend_y = years_to_plot, share_values
               global_max_y = max(global_max_y, max(share_values))
           
           all_plot_data[sib_receiver] = {
               'years': years_to_plot,
               'share_values': share_values,
               'color': sib_receiver_colors_rgb[sib_receiver],
               'label': display_names[sib_receiver],
               'trend_x': trend_x,
               'trend_y': trend_y
           }
   
   # Set global y-axis limit to 30%
   global_y_max = 0.3  # 30% limit
   outlier_threshold = 0.3  # Values above 30% will be marked as outliers
   
   # Plot data for all SIB receivers
   legend_elements = []
   outliers = []  # Track outliers for annotation
   
   for sib_receiver, data in all_plot_data.items():
       # Plot scatter points with transparency
       scatter = ax.scatter(data['years'], data['share_values'], 
                           c=[data['color']], s=60, alpha=0.6,
                           edgecolors='black', linewidth=0.8,
                           label=data['label'])
       
       # Check for outliers and collect them
       for i, (year, share_value) in enumerate(zip(data['years'], data['share_values'])):
           if share_value > outlier_threshold:
               outliers.append({
                   'year': year,
                   'share_value': share_value,
                   'receiver': data['label'],
                   'color': data['color']
               })
       
       # Add linear trend line
       if len(data['years']) >= 2:
           ax.plot(data['trend_x'], data['trend_y'], '--', 
                   color=data['color'], alpha=1.0, linewidth=2)
       
       # Create legend element
       legend_elements.append(scatter)
       
       print(f"  {data['label']}: {len(data['years'])} data points plotted")
   
   # Configure plot
   ax.set_xlim(year_min, year_max)
   ax.set_ylim(0, global_y_max)
   
   # Annotate high-dependency outliers if any
   if outliers:
       outlier_y_position = global_y_max * 0.95  # Position near top
       for outlier in outliers:
           # Position outlier point
           ax.scatter(outlier['year'], outlier_y_position, c=outlier['color'], s=80, alpha=0.8,
                     marker='^', edgecolors='black', linewidth=1.2)
           
           # Add annotation with actual value
           ax.annotate(f'{outlier["share_value"]*100:.1f}%', 
                       xy=(outlier['year'], outlier_y_position), 
                       xytext=(0, -15), textcoords='offset points',
                       ha='center', va='top', fontsize=10, fontweight='bold',
                       bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor='gray'))
           
           print(f"  High LIB dependency: {outlier['receiver']} in {outlier['year']} with {outlier['share_value']*100:.1f}%")
   
   # Set labels with updated y-axis description
   ax.set_xlabel('Year of knowledge flow', fontsize=16, fontweight='bold', labelpad=8)
   ax.set_ylabel('Percentage of LIB knowledge flows\nwithin the total knowledge base (%)', 
                 fontsize=16, fontweight='bold', labelpad=8)
   
   # Set ticks
   ax.set_xticks(year_ticks)
   ax.set_xticklabels([str(year) for year in year_ticks], rotation=45, fontsize=12)
   
   # Set y-axis ticks as percentages (0% to 30% in 5% steps)
   y_ticks = np.arange(0, global_y_max + 0.05, 0.05)  # 5% Schritte: 0%, 5%, 10%, 15%, 20%, 25%, 30%
   ax.set_yticks(y_ticks)
   ax.set_yticklabels([f'{y*100:.0f}%' for y in y_ticks], fontsize=12)
   
   # Add grid
   ax.grid(True, alpha=0.3, color='lightgray')
   for year in year_ticks:
       ax.axvline(x=year, color='lightgray', alpha=0.2, linewidth=0.5)
   
   # LEGEND MOVED TO BOTTOM RIGHT - outside the plot area
   legend = ax.legend(handles=legend_elements, title='SIB knowledge receiver:', 
                     loc='center', bbox_to_anchor=(0.785, -0.29), ncol=2,
                     fontsize=10, title_fontsize=11,
                     framealpha=0.9, edgecolor='gray')
   
   # Add subtitle explaining normalization
   fig.text(-0.11, -0.29, 
            'Normalization:\nLIB to SIB knowledge flows / Total knowledge base of SIB receiver\n'
            'Shows the share of LIB knowledge in total knowledge absorption\n'
            'Higher values indicate stronger dependence on LIB knowledge',
            ha='left', va='center', fontsize=11,
            style='italic', color='black', transform=ax.transAxes)
   
   # Adjust layout to accommodate bottom legend and text
   plt.tight_layout()
   plt.subplots_adjust(bottom=0.28)  # More room for legend and annotation
   
   # Save the figure
   filename = 'SIB_Knowledge_Receivers_LIB_Share_2012-2022.png'
   plt.savefig(filename, dpi=300, bbox_inches='tight')
   print(f"\nVisualization saved as '{filename}'")
   
   # Create DataFrame with all plotted data for verification
   plotted_data_df = pd.DataFrame(plotted_data_list)
   
   # Save plotted data to CSV for inspection
   plotted_data_filename = 'plotted_data_LIB_share_2012-2022.csv'
   plotted_data_df.to_csv(plotted_data_filename, index=False)
   print(f"Plotted data saved to '{plotted_data_filename}' for verification")
   
   # Print verification statistics
   print("\n" + "="*60)
   print("LIB SHARE ANALYSIS VERIFICATION (2012-2022)")
   print("="*60)
   
   if not plotted_data_df.empty:
       print(f"Total plotted data points: {len(plotted_data_df)}")
       print(f"Year range: {plotted_data_df['year'].min()} - {plotted_data_df['year'].max()}")
       print(f"Share value range: {plotted_data_df['share_value'].min():.3f} - {plotted_data_df['share_value'].max():.3f}")
       
       # Show summary by receiver
       print(f"\nSummary by receiver:")
       receiver_summary = plotted_data_df.groupby('receiver').agg({
           'share_value': ['count', 'min', 'max', 'mean'],
           'lib_citation_count': 'sum',
           'total_backward_citations': 'sum'
       }).round(3)
       print(receiver_summary)
       
       # Show interpretation guide
       print(f"\nInterpretation:")
       print(f"- Value = 0%: No LIB citations (complete independence)")
       print(f"- Value = 50%: 50% of citations come from LIB")
       print(f"- Value = 100%: 100% of citations come from LIB (complete dependence)")
       print(f"- Higher values: Stronger dependence on LIB knowledge")
       print(f"- Lower values: More diverse knowledge sources")
   
   plt.show()
   
   return fig, plotted_data_df


# Execute the analysis with new share-based normalization
print("Creating SIB Knowledge Receiver Analysis with LIB share normalization (2012-2022)...")
print("Normalization: LIB_citation_count / total_backward_citations_of_SIB_receiver")
fig, plotted_data_df = create_sib_knowledge_receiver_share_analysis(structure_citations_df, structure_family_colors, combined_df)

print("\n" + "="*80)
print("SIB KNOWLEDGE RECEIVER ANALYSIS COMPLETE (LIB SHARE NORMALIZATION, 2012-2022)")
print("="*80)
print("Created plot showing LIB citation share of total backward citations for SIB structure families:")
print("- Knowledge Provider: Material-specific LIB chemistries")
print("- Knowledge Receivers: SIB Layered Oxides, SIB Polyanionic, SIB PBA, SIB Hard Carbon")
print("- Normalization: LIB_citation_count / total_backward_citations_of_SIB_receiver")
print("\nVisualization features:")
print("- X-axis: Years of knowledge transfer (2012-2022)")
print("- Y-axis: Share of LIB citations (0% to 100%)")
print("- Point colors: Different SIB knowledge receivers")
print("- Dashed lines: Linear trend lines")
print("- Higher values: Stronger dependence on LIB knowledge")
print("- Lower values: More diverse knowledge sources beyond LIB")
print("="*80)

#%%

#==============================================================
# LIB Knowledge Receiver Analysis - Single Visualization
# Neue Normierung: citation_count_year_t / citation_count_2022
# Zeigt ob Wissensflüsse größer oder kleiner als "aktuell" (2022) waren
# X-axis truncated from 2000 to 2022
#==============================================================

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from matplotlib.lines import Line2D
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import warnings
warnings.filterwarnings('ignore')

def determine_linear_fit(x_data, y_data):
    """
    Simple linear fit for guidance lines.
    """
    if len(x_data) < 2:
        return np.array(x_data), np.array(y_data)
    
    x_array = np.array(x_data).reshape(-1, 1)
    y_array = np.array(y_data)
    
    # Generate smooth x values for trend line (within data range only)
    x_smooth = np.linspace(min(x_data), max(x_data), 100).reshape(-1, 1)
    
    try:
        # Linear fit only
        linear_reg = LinearRegression()
        linear_reg.fit(x_array, y_array)
        y_smooth_linear = linear_reg.predict(x_smooth)
        
        return x_smooth.flatten(), y_smooth_linear
                
    except Exception as e:
        # Fallback to simple linear interpolation
        return np.array(x_data), np.array(y_data)


def create_lib_knowledge_receiver_single_plot_new_normalization(structure_citations_df, structure_family_colors, combined_df):
    """
    Create single visualization showing all LIB knowledge receivers with new normalization.
    Normalization: citation_count_year_t / citation_count_2022
    X-axis truncated from 2000 to 2022.
    """
    
    # Define all LIB structure families
    all_lib_families = [
        'LIB_Layered_Oxides',
        'LIB_Polyanionic', 
        'LIB_Spinel',
        'LIB_Anode_Graphite_based',
        'LIB_Anode_LTO'
    ]
    
    # Display names for better readability
    display_names = {
        'LIB_Layered_Oxides': 'LIB Layered Oxides (1)',
        'LIB_Polyanionic': 'LIB Polyanionic (2)',
        'LIB_Spinel': 'LIB Spinel (3)',
        'LIB_Anode_Graphite_based': 'LIB Graphite-based (7)',
        'LIB_Anode_LTO': 'LIB LTO (8)'
    }
    
    # Define colors for LIB receivers (specified color scheme)
    lib_receiver_colors_rgb = {
        'LIB_Layered_Oxides': (255/255, 0/255, 0/255),        # Red
        'LIB_Polyanionic': (0/255, 0/255, 255/255),           # Blue  
        'LIB_Spinel': (0/255, 255/255, 255/255),              # Cyan
        'LIB_Anode_Graphite_based': (102/255, 102/255, 102/255), # Gray
        'LIB_Anode_LTO': (255/255, 165/255, 0/255)            # Orange
    }
    
    # Year range - TRUNCATED FROM 2000 TO 2022 with extended limits
    year_min, year_max = 1999.5, 2022.5  # Plot limits extended for visibility
    year_range = list(range(2000, 2023))  # Data range (2000-2022 inclusive)
    year_ticks = list(range(2000, 2023, 2))  # Tick labels every 2 years for better readability
    reference_year = 2022  # Reference year for normalization
    
    # Create list to store all plotted data for verification
    plotted_data_list = []
    
    # Create figure with single plot - REDUCED SIZE
    fig, ax = plt.subplots(1, 1, figsize=(10, 6.67))  # Reduced from 15x10 to 10x6.67 (same ratio)
    
    # Collect all data for each LIB receiver
    all_plot_data = {}
    global_max_y = 0
    
    for lib_receiver in all_lib_families:
        print(f"\nProcessing {display_names[lib_receiver]}...")
        
        # Define providers for this receiver (all other LIB families)
        lib_providers_for_this_receiver = [fam for fam in all_lib_families if fam != lib_receiver]
        
        print(f"  Providers: {[display_names[prov] for prov in lib_providers_for_this_receiver]}")
        
        # Filter to LIB→LIB flows for this specific receiver
        lib_to_lib_citations = structure_citations_df[
            (structure_citations_df['cited_structure_family'].isin(lib_providers_for_this_receiver)) &
            (structure_citations_df['citing_structure_family'] == lib_receiver)
        ]
        
        if lib_to_lib_citations.empty:
            print(f"No citations found for {lib_receiver}")
            continue
        
        # Calculate citation counts for each year
        year_citation_counts = {}
        for year in year_range:
            year_citations = lib_to_lib_citations[
                lib_to_lib_citations['citing_priority_year'] == year
            ]
            year_citation_counts[year] = len(year_citations)
        
        # Get reference year - use 2022 as maximum, but fall back to latest available year
        available_years_with_data = [year for year, count in year_citation_counts.items() if count > 0]
        if not available_years_with_data:
            print(f"Warning: No citations found for any year for {lib_receiver}")
            continue
            
        # Find reference year: latest year up to 2022 that has data
        reference_year_for_receiver = min(max(available_years_with_data), 2022)
        reference_count = year_citation_counts.get(reference_year_for_receiver, 0)
        
        if reference_count == 0:
            print(f"Warning: No citations in calculated reference year {reference_year_for_receiver} for {lib_receiver}")
            continue
        
        print(f"Reference year for {lib_receiver}: {reference_year_for_receiver} (citation count: {reference_count})")
        
        # Calculate normalized values for each year
        years_to_plot = []
        normalized_values = []
        
        for year in year_range:
            citation_count = year_citation_counts[year]
            
            if citation_count > 0:  # Only plot years with citations
                # NEW NORMALIZATION: citation_count_year_t / citation_count_2022
                normalized_value = citation_count / reference_count
                
                years_to_plot.append(year)
                normalized_values.append(normalized_value)
                
                # Store data for verification
                plotted_data_list.append({
                    'receiver': lib_receiver,
                    'provider': 'other LIB chemistries',
                    'year': year,
                    'citation_count': citation_count,
                    'reference_count': reference_count,
                    'reference_year': reference_year_for_receiver,
                    'normalized_value': normalized_value
                })
        
        # Store data if we have any
        if years_to_plot:
            print(f"  Found {len(years_to_plot)} data points")
            print(f"  Normalized range: {min(normalized_values):.2f} - {max(normalized_values):.2f}")
            
            # Calculate linear fit
            if len(years_to_plot) >= 2:
                trend_x, trend_y = determine_linear_fit(years_to_plot, normalized_values)
                max_trend_y = max(trend_y) if len(trend_y) > 0 else 0
                global_max_y = max(global_max_y, max(normalized_values), max_trend_y)
            else:
                trend_x, trend_y = years_to_plot, normalized_values
                global_max_y = max(global_max_y, max(normalized_values))
            
            all_plot_data[lib_receiver] = {
                'years': years_to_plot,
                'normalized_values': normalized_values,
                'color': lib_receiver_colors_rgb[lib_receiver],
                'label': display_names[lib_receiver],
                'trend_x': trend_x,
                'trend_y': trend_y
            }
    
    # Set y-axis limit based on data with some padding, but cap at 150
    data_max_y = global_max_y
    calculated_y_max = max(data_max_y * 1.1, 1.2)  # Add 10% padding, minimum 1.2
    global_y_max = min(calculated_y_max, 150)  # Cap at 150
    outlier_threshold = global_y_max  # Values above y_max are considered outliers and will be marked
    
    # Plot data for all LIB receivers
    legend_elements = []
    outliers = []  # Track outliers for annotation
    
    for lib_receiver, data in all_plot_data.items():
        # Plot scatter points with transparency (alpha=0.4 from original code)
        scatter = ax.scatter(data['years'], data['normalized_values'], 
                           c=[data['color']], s=60, alpha=0.4,
                           edgecolors='black', linewidth=0.8,
                           label=data['label'])
        
        # Check for outliers and collect them
        for i, (year, normalized_value) in enumerate(zip(data['years'], data['normalized_values'])):
            if normalized_value > outlier_threshold:
                outliers.append({
                    'year': year,
                    'normalized_value': normalized_value,
                    'receiver': data['label'],
                    'color': data['color']
                })
        
        # Add linear trend line
        if len(data['years']) >= 2:
            ax.plot(data['trend_x'], data['trend_y'], '--', 
                   color=data['color'], alpha=1.0, linewidth=2)
        
        # Create legend element
        legend_elements.append(scatter)
        
        print(f"  {data['label']}: {len(data['years'])} data points plotted")
    
    # Configure plot
    ax.set_xlim(year_min, year_max)
    ax.set_ylim(0, global_y_max)
    
    # Annotate outliers
    outlier_y_position = global_y_max * 0.97  # Position outliers at 97% of y_max
    for outlier in outliers:
        # Position outlier point at 97% of y_max
        ax.scatter(outlier['year'], outlier_y_position, c=outlier['color'], s=80, alpha=0.8,
                  marker='^', edgecolors='black', linewidth=1.2)
        
        # Add annotation with actual value below the outlier marker
        ax.annotate(f'{outlier["normalized_value"]:.2f}', 
                   xy=(outlier['year'], outlier_y_position), 
                   xytext=(0, -15), textcoords='offset points',
                   ha='center', va='top', fontsize=10, fontweight='bold',
                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor='gray'))
        
        print(f"  Outlier marked: {outlier['receiver']} in {outlier['year']} with {outlier['normalized_value']:.2f}")
    
    # Add horizontal reference line at y=1.0 (represents 2022 level) - very light gray and dashed
    ax.axhline(y=1.0, color='lightgray', linestyle='--', linewidth=1.0, alpha=0.4)
    
    # Set labels with updated y-axis description
    ax.set_xlabel('Year of knowledge flow', fontsize=16, fontweight='bold', labelpad=8)
    ax.set_ylabel(f'Normalized knowledge flow\n(relative to latest available year ≤ 2022)', 
                  fontsize=16, fontweight='bold', labelpad=8)
    
    # Set ticks
    ax.set_xticks(year_ticks)
    ax.set_xticklabels([str(year) for year in year_ticks], rotation=45, fontsize=12)
    
    # Set y-axis ticks based on actual y_max with better scaling
    if global_y_max <= 2.0:
        y_tick_interval = 0.2
    elif global_y_max <= 5.0:
        y_tick_interval = 0.5
    elif global_y_max <= 10.0:
        y_tick_interval = 1.0
    elif global_y_max <= 20.0:
        y_tick_interval = 2.0
    elif global_y_max <= 50.0:
        y_tick_interval = 5.0
    elif global_y_max <= 100.0:
        y_tick_interval = 10.0
    else:  # up to 150
        y_tick_interval = 10.0
    
    y_ticks = np.arange(0, global_y_max + y_tick_interval, y_tick_interval)
    ax.set_yticks(y_ticks)
    ax.set_yticklabels([f'{y:.1f}' if y_tick_interval < 1.0 else f'{y:.0f}' for y in y_ticks], fontsize=12)
    
    # Add grid
    ax.grid(True, alpha=0.3, color='lightgray')
    for year in year_ticks:
        ax.axvline(x=year, color='lightgray', alpha=0.2, linewidth=0.5)
    
    # LEGEND MOVED TO BOTTOM RIGHT - outside the plot area (3 columns for 5 items)
    legend = ax.legend(handles=legend_elements, title='LIB knowledge receiver:', 
                      loc='center', bbox_to_anchor=(0.71, -0.29), ncol=3,
                      fontsize=10, title_fontsize=11,
                      framealpha=0.9, edgecolor='gray',
                      columnspacing=1.0, handletextpad=0.5, handlelength=1.5)
    
    # Add subtitle explaining normalization and aggregation - positioned at y-axis label level and legend height
    fig.text(-0.12, -0.29, 
            f'LIB knowledge providers (excluding receiver):\n'
            'Layered Oxides (A)), Polyanionic (B)), Spinel (C)),\n'
            'Graphite-based (G)) and LTO (H))',
            ha='left', va='center', fontsize=11,
            style='italic', color='black', transform=ax.transAxes)
    
    # Adjust layout to accommodate bottom legend and text
    plt.tight_layout()
    plt.subplots_adjust(bottom=0.28)  # More room for legend and annotation
    
    # Save the figure
    filename = f'LIB_Knowledge_Receivers_Normalized_Flexible_Ref_2000-2022.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"\nVisualization saved as '{filename}'")
    
    # Create DataFrame with all plotted data for verification
    plotted_data_df = pd.DataFrame(plotted_data_list)
    
    # Save plotted data to CSV for inspection
    plotted_data_filename = f'plotted_data_LIB_normalized_flexible_ref_2000-2022.csv'
    plotted_data_df.to_csv(plotted_data_filename, index=False)
    print(f"Plotted data saved to '{plotted_data_filename}' for verification")
    
    # Print verification statistics
    print("\n" + "="*60)
    print(f"NORMALIZED LIB DATA VERIFICATION (2000-2022, flexible ref year)")
    print("="*60)
    
    if not plotted_data_df.empty:
        print(f"Total plotted data points: {len(plotted_data_df)}")
        print(f"Year range: {plotted_data_df['year'].min()} - {plotted_data_df['year'].max()}")
        print(f"Normalized value range: {plotted_data_df['normalized_value'].min():.2f} - {plotted_data_df['normalized_value'].max():.2f}")
        
        # Show reference years used for each receiver
        print(f"\nReference years used:")
        ref_years_summary = plotted_data_df.groupby('receiver')['reference_year'].first()
        for receiver, ref_year in ref_years_summary.items():
            print(f"  {receiver}: {ref_year}")
        
        # Show summary by receiver
        print(f"\nSummary by receiver:")
        receiver_summary = plotted_data_df.groupby('receiver').agg({
            'normalized_value': ['count', 'min', 'max', 'mean'],
            'reference_count': 'first'
        }).round(2)
        print(receiver_summary)
        
        # Show interpretation guide
        print(f"\nInterpretation:")
        print(f"- Value = 1.0: Same citation count as in receiver's reference year")
        print(f"- Value > 1.0: Higher citation count than in receiver's reference year")
        print(f"- Value < 1.0: Lower citation count than in receiver's reference year")
    
    plt.show()
    
    return fig, plotted_data_df


# Execute the analysis with new normalization for LIB
print("Creating LIB Knowledge Receiver Analysis with new normalization (2000-2022)...")
print("Normalization: citation_count_year_t / citation_count_2022")
fig_lib, plotted_data_df_lib = create_lib_knowledge_receiver_single_plot_new_normalization(structure_citations_df, structure_family_colors, combined_df)

print("\n" + "="*80)
print("LIB KNOWLEDGE RECEIVER ANALYSIS COMPLETE (NEW NORMALIZATION, 2000-2022)")
print("="*80)
print("Created single plot showing normalized knowledge flow from other LIB chemistries to LIB structure families:")
print("- Knowledge Providers: Other LIB chemistries (aggregated, excluding respective receiver)")
print("- Knowledge Receivers: LIB Layered Oxides, LIB Polyanionic, LIB Spinel, LIB Graphite-based, LIB LTO")
print("- Normalization: citation_count_year_t / citation_count_2022")
print("\nVisualization features:")
print("- X-axis: Years of knowledge transfer (2000-2022)")
print("- Y-axis: Normalized knowledge flow (relative to 2022)")
print("- Reference line: y=1.0 represents 2022 citation level")
print("- Point colors: Different LIB knowledge receivers")
print("- Dashed lines: Linear trend lines")
print("- Values > 1.0: Higher activity than in 2022")
print("- Values < 1.0: Lower activity than in 2022")
print("="*80)

print("\n" + "="*80)
print("Creating SIB to SIB Knowledge Receiver Analysis with new normalization (2012-2022)...")
print("Normalization: citation_count_year_t / citation_count_2022")
fig_sib, plotted_data_df_sib = create_sib_to_sib_knowledge_receiver_single_plot_new_normalization(structure_citations_df, structure_family_colors, combined_df)

print("\n" + "="*80)
print("SIB TO SIB KNOWLEDGE RECEIVER ANALYSIS COMPLETE (NEW NORMALIZATION, 2012-2022)")
print("="*80)
print("Created single plot showing normalized knowledge flow from other SIB chemistries to SIB structure families:")
print("- Knowledge Providers: Other SIB chemistries (aggregated, excluding respective receiver)")
print("- Knowledge Receivers: SIB Layered Oxides, SIB Polyanionic, SIB PBA, SIB Hard Carbon")
print("- Normalization: citation_count_year_t / citation_count_2022")
print("\nVisualization features:")
print("- X-axis: Years of knowledge transfer (2012-2022)")
print("- Y-axis: Normalized knowledge flow (relative to 2022)")
print("- Reference line: y=1.0 represents 2022 citation level")
print("- Point colors: Different SIB knowledge receivers")
print("- Dashed lines: Linear trend lines")
print("- Values > 1.0: Higher activity than in 2022")
print("- Values < 1.0: Lower activity than in 2022")
print("="*80)


#%%


#==============================================================
# LIB Knowledge Receiver Analysis - Share of LIB Citations
# Neue Normierung: LIB citation count / Total backward citations of LIB receiver
# Zeigt den Anteil der LIB-Zitationen an allen Zitationen der LIB-Technologie
# X-axis truncated from 2000 to 2022
#==============================================================

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from matplotlib.lines import Line2D
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import warnings
warnings.filterwarnings('ignore')

def determine_linear_fit(x_data, y_data):
    """
    Simple linear fit for guidance lines.
    """
    if len(x_data) < 2:
        return np.array(x_data), np.array(y_data)
    
    x_array = np.array(x_data).reshape(-1, 1)
    y_array = np.array(y_data)
    
    # Generate smooth x values for trend line (within data range only)
    x_smooth = np.linspace(min(x_data), max(x_data), 100).reshape(-1, 1)
    
    try:
        # Linear fit only
        linear_reg = LinearRegression()
        linear_reg.fit(x_array, y_array)
        y_smooth_linear = linear_reg.predict(x_smooth)
        
        return x_smooth.flatten(), y_smooth_linear
                
    except Exception as e:
        # Fallback to simple linear interpolation
        return np.array(x_data), np.array(y_data)


def create_lib_to_lib_knowledge_receiver_share_analysis(structure_citations_df, structure_family_colors, combined_df):
    """
    Create visualization showing LIB citation share of total backward citations for each LIB receiver.
    Normalization: LIB_citation_count / total_backward_citations_of_LIB_receiver_per_year
    X-axis truncated from 2000 to 2022.
    """
    
    # Define all LIB structure families
    all_lib_families = [
        'LIB_Layered_Oxides',
        'LIB_Polyanionic', 
        'LIB_Spinel',
        'LIB_Anode_Graphite_based',
        'LIB_Anode_LTO'
    ]
    
    # Display names for better readability
    display_names = {
        'LIB_Layered_Oxides': 'LIB Layered Oxides (1)',
        'LIB_Polyanionic': 'LIB Polyanionic (2)',
        'LIB_Spinel': 'LIB Spinel (3)',
        'LIB_Anode_Graphite_based': 'LIB Graphite-based (7)',
        'LIB_Anode_LTO': 'LIB LTO (8)'
    }
    
    # Define colors for LIB receivers (specified color scheme)
    lib_receiver_colors_rgb = {
        'LIB_Layered_Oxides': (255/255, 0/255, 0/255),        # Red
        'LIB_Polyanionic': (0/255, 0/255, 255/255),           # Blue  
        'LIB_Spinel': (0/255, 255/255, 255/255),              # Cyan
        'LIB_Anode_Graphite_based': (102/255, 102/255, 102/255), # Gray
        'LIB_Anode_LTO': (255/255, 165/255, 0/255)            # Orange
    }
    
    # Get ALL citations for LIB receivers to calculate total backward citations
    all_lib_citations = structure_citations_df[
        structure_citations_df['citing_structure_family'].isin(all_lib_families)
    ]
    
    if all_lib_citations.empty:
        print("No LIB citations found in the dataset")
        return
    
    print(f"Found {len(all_lib_citations)} total LIB citations for analysis")
    
    # Year range - TRUNCATED FROM 2000 TO 2022 with extended limits
    year_min, year_max = 1999.5, 2022.5  # Plot limits extended for visibility
    year_range = list(range(2000, 2023))  # Data range (2000-2022 inclusive)
    year_ticks = list(range(2000, 2023, 2))  # Tick labels every 2 years for better readability
    
    # Create list to store all plotted data for verification
    plotted_data_list = []
    
    # Create figure with single plot - REDUCED SIZE
    fig, ax = plt.subplots(1, 1, figsize=(10, 6.67))  # Reduced from 15x10 to 10x6.67 (same ratio)
    
    # Collect all data for each LIB receiver
    all_plot_data = {}
    global_max_y = 0
    
    for lib_receiver in all_lib_families:
        print(f"\nProcessing {display_names[lib_receiver]}...")
        
        # Define providers for this receiver (all other LIB families)
        lib_providers_for_this_receiver = [fam for fam in all_lib_families if fam != lib_receiver]
        
        # Filter LIB→LIB citations for this specific LIB receiver
        lib_receiver_citations = structure_citations_df[
            (structure_citations_df['cited_structure_family'].isin(lib_providers_for_this_receiver)) &
            (structure_citations_df['citing_structure_family'] == lib_receiver)
        ]
        
        # Filter ALL citations for this specific LIB receiver (for total backward citations)
        all_receiver_citations = all_lib_citations[
            all_lib_citations['citing_structure_family'] == lib_receiver
        ]
        
        if lib_receiver_citations.empty:
            print(f"No LIB citations found for {lib_receiver}")
            continue
            
        if all_receiver_citations.empty:
            print(f"No total citations found for {lib_receiver}")
            continue
        
        # Calculate citation counts and backward citations for each year
        year_data = {}
        for year in year_range:
            # Count UNIQUE patent-to-patent citations for this year (avoid double counting)
            lib_year_citations = lib_receiver_citations[
                lib_receiver_citations['citing_priority_year'] == year
            ]
            # Count unique citing_patent -> cited_patent pairs
            unique_patent_citations = lib_year_citations[['citing_patent', 'cited_patent']].drop_duplicates()
            lib_citation_count = len(unique_patent_citations)
            
            # Calculate total backward citations for this LIB receiver in this year
            # Sum of citing_patent_backward_citations for UNIQUE citing patents
            all_year_citations = all_receiver_citations[
                all_receiver_citations['citing_priority_year'] == year
            ]
            
            if len(all_year_citations) == 0:
                total_backward_citations = 0
            else:
                # Get unique citing patents and sum their backward citations
                unique_patents_data = all_year_citations.drop_duplicates(subset=['citing_patent'])
                total_backward_citations = unique_patents_data['citing_patent_backward_citations'].sum()
            
            year_data[year] = {
                'lib_citation_count': lib_citation_count,
                'total_backward_citations': total_backward_citations
            }
        
        # Calculate share values for each year
        years_to_plot = []
        share_values = []
        
        for year in year_range:
            lib_count = year_data[year]['lib_citation_count']
            total_backward = year_data[year]['total_backward_citations']
            
            # Only plot years where we have both LIB citations AND total backward citations
            if lib_count > 0 and total_backward > 0:
                # NEW NORMALIZATION: lib_citation_count / total_backward_citations
                share_value = lib_count / total_backward
                
                years_to_plot.append(year)
                share_values.append(share_value)
                
                # Store data for verification
                plotted_data_list.append({
                    'receiver': lib_receiver,
                    'year': year,
                    'lib_citation_count': lib_count,
                    'total_backward_citations': total_backward,
                    'share_value': share_value
                })
        
        # Store data if we have any
        if years_to_plot:
            print(f"  Found {len(years_to_plot)} data points")
            print(f"  Share range: {min(share_values):.3f} - {max(share_values):.3f}")
            
            # Calculate linear fit
            if len(years_to_plot) >= 2:
                trend_x, trend_y = determine_linear_fit(years_to_plot, share_values)
                max_trend_y = max(trend_y) if len(trend_y) > 0 else 0
                global_max_y = max(global_max_y, max(share_values), max_trend_y)
            else:
                trend_x, trend_y = years_to_plot, share_values
                global_max_y = max(global_max_y, max(share_values))
            
            all_plot_data[lib_receiver] = {
                'years': years_to_plot,
                'share_values': share_values,
                'color': lib_receiver_colors_rgb[lib_receiver],
                'label': display_names[lib_receiver],
                'trend_x': trend_x,
                'trend_y': trend_y
            }
    
    # Set global y-axis limit to 30%
    global_y_max = 0.3  # 30% limit
    outlier_threshold = 0.3  # Values above 30% will be marked as outliers
    
    # Plot data for all LIB receivers
    legend_elements = []
    outliers = []  # Track outliers for annotation
    
    for lib_receiver, data in all_plot_data.items():
        # Plot scatter points with transparency
        scatter = ax.scatter(data['years'], data['share_values'], 
                            c=[data['color']], s=60, alpha=0.4,
                            edgecolors='black', linewidth=0.8,
                            label=data['label'])
        
        # Check for outliers and collect them
        for i, (year, share_value) in enumerate(zip(data['years'], data['share_values'])):
            if share_value > outlier_threshold:
                outliers.append({
                    'year': year,
                    'share_value': share_value,
                    'receiver': data['label'],
                    'color': data['color']
                })
        
        # Add linear trend line
        if len(data['years']) >= 2:
            ax.plot(data['trend_x'], data['trend_y'], '--', 
                    color=data['color'], alpha=1.0, linewidth=2)
        
        # Create legend element
        legend_elements.append(scatter)
        
        print(f"  {data['label']}: {len(data['years'])} data points plotted")
    
    # Configure plot
    ax.set_xlim(year_min, year_max)
    ax.set_ylim(0, global_y_max)
    
    # Annotate high-dependency outliers if any
    if outliers:
        outlier_y_position = global_y_max * 0.95  # Position near top
        for outlier in outliers:
            # Position outlier point
            ax.scatter(outlier['year'], outlier_y_position, c=outlier['color'], s=80, alpha=0.8,
                      marker='^', edgecolors='black', linewidth=1.2)
            
            # Add annotation with actual value
            ax.annotate(f'{outlier["share_value"]*100:.1f}%', 
                        xy=(outlier['year'], outlier_y_position), 
                        xytext=(0, -15), textcoords='offset points',
                        ha='center', va='top', fontsize=10, fontweight='bold',
                        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor='gray'))
            
            print(f"  High LIB dependency: {outlier['receiver']} in {outlier['year']} with {outlier['share_value']*100:.1f}%")
    
    # Set labels with updated y-axis description
    ax.set_xlabel('Year of knowledge flow', fontsize=16, fontweight='bold', labelpad=8)
    ax.set_ylabel("Percentage of LIB knowledge flows\nwithin the total knowledge base (%)", 
                  fontsize=16, fontweight='bold', labelpad=8)

    # Set ticks
    ax.set_xticks(year_ticks)
    ax.set_xticklabels([str(year) for year in year_ticks], rotation=45, fontsize=12)
    
    # Set y-axis ticks as percentages (0% to 30% in 5% steps)
    y_ticks = np.arange(0, global_y_max + 0.05, 0.05)  # 5% Schritte: 0%, 5%, 10%, 15%, 20%, 25%, 30%
    ax.set_yticks(y_ticks)
    ax.set_yticklabels([f'{y*100:.0f}%' for y in y_ticks], fontsize=12)  # Konvertiert zu Prozent
    
    # Add grid
    ax.grid(True, alpha=0.3, color='lightgray')
    for year in year_ticks:
        ax.axvline(x=year, color='lightgray', alpha=0.2, linewidth=0.5)
    
    # LEGEND MOVED TO BOTTOM RIGHT - outside the plot area (3 columns for 5 items)
    legend = ax.legend(handles=legend_elements, title='LIB knowledge receiver:', 
                      loc='center', bbox_to_anchor=(0.71, -0.29), ncol=3,
                      fontsize=10, title_fontsize=11,
                      framealpha=0.9, edgecolor='gray',
                      columnspacing=1.0, handletextpad=0.5, handlelength=1.5)
    
    # Add subtitle explaining normalization
    fig.text(-0.12, -0.26, 
            'Normalization:\nLIB to LIB knowledge flows / Total knowledge base of\nLIB receiver\n'
            'LIB knowledge providers (excluding receiver):\n'
            'Layered Oxides (A)), Polyanionic (B)), Spinel (C)),\nGraphite-based (G)) and LTO (H))',
            ha='left', va='center', fontsize=11,
            style='italic', color='black', transform=ax.transAxes)
    
    # Adjust layout to accommodate bottom legend and text
    plt.tight_layout()
    plt.subplots_adjust(bottom=0.28)  # More room for legend and annotation
    
    # Save the figure
    filename = 'LIB_to_LIB_Knowledge_Receivers_Share_2000-2022.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"\nVisualization saved as '{filename}'")
    
    # Create DataFrame with all plotted data for verification
    plotted_data_df = pd.DataFrame(plotted_data_list)
    
    # Save plotted data to CSV for inspection
    plotted_data_filename = 'plotted_data_LIB_to_LIB_share_2000-2022.csv'
    plotted_data_df.to_csv(plotted_data_filename, index=False)
    print(f"Plotted data saved to '{plotted_data_filename}' for verification")
    
    # Print verification statistics
    print("\n" + "="*60)
    print("LIB TO LIB SHARE ANALYSIS VERIFICATION (2000-2022)")
    print("="*60)
    
    if not plotted_data_df.empty:
        print(f"Total plotted data points: {len(plotted_data_df)}")
        print(f"Year range: {plotted_data_df['year'].min()} - {plotted_data_df['year'].max()}")
        print(f"Share value range: {plotted_data_df['share_value'].min():.3f} - {plotted_data_df['share_value'].max():.3f}")
        
        # Show summary by receiver
        print(f"\nSummary by receiver:")
        receiver_summary = plotted_data_df.groupby('receiver').agg({
            'share_value': ['count', 'min', 'max', 'mean'],
            'lib_citation_count': 'sum',
            'total_backward_citations': 'sum'
        }).round(3)
        print(receiver_summary)
        
        # Show interpretation guide
        print(f"\nInterpretation:")
        print(f"- Value = 0%: No LIB citations (complete independence)")
        print(f"- Value = 15%: 15% of citations come from LIB")
        print(f"- Value = 30%: 30% of citations come from LIB (high dependence)")
        print(f"- Higher values: Stronger dependence on LIB knowledge")
        print(f"- Lower values: More diverse knowledge sources")
    
    plt.show()
    
    return fig, plotted_data_df

fig, plotted_data_df = create_lib_to_lib_knowledge_receiver_share_analysis(structure_citations_df, structure_family_colors, combined_df)



#%%

#==============================================================
# SIB Knowledge Receiver Analysis - Single Visualization
# Aggregates other SIB providers (excluding receiver) to "other SIB chemistries"
# Shows all SIB receivers in one plot with different colors
# X-axis truncated from 2012 to 2022
#==============================================================

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from matplotlib.lines import Line2D
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import warnings
warnings.filterwarnings('ignore')

def determine_linear_fit(x_data, y_data):
    """
    Simple linear fit for guidance lines.
    """
    if len(x_data) < 2:
        return np.array(x_data), np.array(y_data)
    
    x_array = np.array(x_data).reshape(-1, 1)
    y_array = np.array(y_data)
    
    # Generate smooth x values for trend line (within data range only)
    x_smooth = np.linspace(min(x_data), max(x_data), 100).reshape(-1, 1)
    
    try:
        # Linear fit only
        linear_reg = LinearRegression()
        linear_reg.fit(x_array, y_array)
        y_smooth_linear = linear_reg.predict(x_smooth)
        
        return x_smooth.flatten(), y_smooth_linear
                
    except Exception as e:
        # Fallback to simple linear interpolation
        return np.array(x_data), np.array(y_data)


def create_sib_to_sib_knowledge_receiver_single_plot(structure_citations_df, structure_family_colors, combined_df):
    """
    Create single visualization showing all SIB knowledge receivers with aggregated other SIB knowledge providers.
    X-axis truncated from 2012 to 2022.
    """
    
    # Define all SIB structure families
    all_sib_families = [
        'SIB_Layered_Oxides',
        'SIB_Polyanionic',
        'SIB_PBA', 
        'SIB_Anode_Hard_Carbon'
    ]
    
    # Display names for better readability
    display_names = {
        'SIB_Layered_Oxides': 'SIB Layered Oxides (4)',
        'SIB_Polyanionic': 'SIB Polyanionic (5)',
        'SIB_PBA': 'SIB PBA (6)',
        'SIB_Anode_Hard_Carbon': 'SIB Hard Carbon (9)'
    }
    
    # Define colors for SIB receivers (matching the first code's color scheme)
    sib_receiver_colors_rgb = {
        'SIB_Layered_Oxides': (255/255, 102/255, 102/255),    # Light Red
        'SIB_Polyanionic': (0/255, 102/255, 255/255),         # Medium Blue
        'SIB_PBA': (0/255, 204/255, 0/255),                   # Green
        'SIB_Anode_Hard_Carbon': (153/255, 153/255, 153/255)  # Light Gray
    }
    
    # Create structure family aggregation for backward citations
    family_aggregated_df = create_structure_family_aggregated_df(combined_df)
    
    # Year range - TRUNCATED FROM 2012 TO 2022 with extended limits
    year_min, year_max = 2011.5, 2022.5  # Plot limits extended for visibility
    year_range = list(range(2012, 2023))  # Data range (2012-2022 inclusive)
    year_ticks = list(range(2012, 2023))  # Tick labels
    
    # Create list to store all plotted data for verification
    plotted_data_list = []
    
    # Create figure with single plot - REDUCED SIZE
    fig, ax = plt.subplots(1, 1, figsize=(10, 6.67))  # Reduced from 15x10 to 10x6.67 (same ratio)
    # fig.suptitle('Knowledge Flow from Other SIB Chemistries to SIB Structure Families\n(Linear trend lines show trends)', 
                 # fontsize=14, fontweight='bold', y=0.98)  # Reduced font size
    
    # Collect all data for each SIB receiver
    all_plot_data = {}
    global_max_y = 0
    
    for sib_receiver in all_sib_families:
        print(f"\nProcessing {display_names[sib_receiver]}...")
        
        # Define providers for this receiver (all other SIB families)
        sib_providers_for_this_receiver = [fam for fam in all_sib_families if fam != sib_receiver]
        
        print(f"  Providers: {[display_names[prov] for prov in sib_providers_for_this_receiver]}")
        
        # Filter to SIB→SIB flows for this specific receiver
        sib_to_sib_citations = structure_citations_df[
            (structure_citations_df['cited_structure_family'].isin(sib_providers_for_this_receiver)) &
            (structure_citations_df['citing_structure_family'] == sib_receiver)
        ]
        
        if sib_to_sib_citations.empty:
            print(f"No citations found for {sib_receiver}")
            continue
        
        # Calculate total backward citations for this receiver using aggregated data
        receiver_data = family_aggregated_df[
            (family_aggregated_df['structure_family'] == sib_receiver) &
            (family_aggregated_df['battery_type'] == 'SIB')
        ]
        
        if receiver_data.empty:
            print(f"Warning: No aggregated data found for {sib_receiver}")
            continue
        
        total_receiver_backward_citations = receiver_data['backward_citations'].sum()
        
        if total_receiver_backward_citations == 0:
            print(f"Warning: Zero backward citations for {sib_receiver}")
            continue
        
        # Calculate aggregated knowledge transfer for each year
        years_to_plot = []
        percentages_to_plot = []
        
        for year in year_range:
            # Filter citations for this year (from all other SIB providers combined)
            year_citations = sib_to_sib_citations[
                sib_to_sib_citations['citing_priority_year'] == year
            ]
            
            if year_citations.empty:
                continue
            
            # Total citation count from all other SIB providers
            total_count = len(year_citations)
            
            if total_count > 0:
                # Calculate knowledge percentage
                knowledge_percentage = (total_count / total_receiver_backward_citations) * 100
                
                years_to_plot.append(year)
                percentages_to_plot.append(knowledge_percentage)
                
                # Store data for verification
                plotted_data_list.append({
                    'receiver': sib_receiver,
                    'provider': 'other SIB chemistries',
                    'year': year,
                    'total_count': total_count,
                    'total_backward_citations': total_receiver_backward_citations,
                    'knowledge_percentage': knowledge_percentage
                })
        
        # Store data if we have any
        if years_to_plot:
            print(f"  Found {len(years_to_plot)} data points")
            
            # Calculate linear fit
            if len(years_to_plot) >= 2:
                trend_x, trend_y = determine_linear_fit(years_to_plot, percentages_to_plot)
                max_trend_y = max(trend_y) if len(trend_y) > 0 else 0
                global_max_y = max(global_max_y, max(percentages_to_plot), max_trend_y)
            else:
                trend_x, trend_y = years_to_plot, percentages_to_plot
                global_max_y = max(global_max_y, max(percentages_to_plot))
            
            all_plot_data[sib_receiver] = {
                'years': years_to_plot,
                'percentages': percentages_to_plot,
                'color': sib_receiver_colors_rgb[sib_receiver],
                'label': display_names[sib_receiver],
                'trend_x': trend_x,
                'trend_y': trend_y
            }
    
    # Set global y-axis limit to 1.4% with outlier handling
    global_y_max = 1.4
    outlier_threshold = 1.4  # Values above 1.4% are considered outliers and will be marked
    
    # Plot data for all SIB receivers
    legend_elements = []
    outliers = []  # Track outliers for annotation
    
    for sib_receiver, data in all_plot_data.items():
        # Plot scatter points with transparency (alpha=0.4 from original code)
        scatter = ax.scatter(data['years'], data['percentages'], 
                           c=[data['color']], s=60, alpha=0.4,
                           edgecolors='black', linewidth=0.8,
                           label=data['label'])
        
        # Check for outliers and collect them
        for i, (year, percentage) in enumerate(zip(data['years'], data['percentages'])):
            if percentage > outlier_threshold:
                outliers.append({
                    'year': year,
                    'percentage': percentage,
                    'receiver': data['label'],
                    'color': data['color']
                })
        
        # Add linear trend line
        if len(data['years']) >= 2:
            ax.plot(data['trend_x'], data['trend_y'], '--', 
                   color=data['color'], alpha=1.0, linewidth=2)
        
        # Create legend element
        legend_elements.append(scatter)
        
        print(f"  {data['label']}: {len(data['years'])} data points plotted")
    
    # Configure plot
    ax.set_xlim(year_min, year_max)
    ax.set_ylim(0, global_y_max)
    
    # Annotate outliers
    outlier_y_position = 1.36  # Position outliers at 1.36%
    for outlier in outliers:
        # Position outlier point at 1.36%
        ax.scatter(outlier['year'], outlier_y_position, c=outlier['color'], s=80, alpha=0.8,
                  marker='^', edgecolors='black', linewidth=1.2)
        
        # Add annotation with actual value below the outlier marker
        ax.annotate(f'{outlier["percentage"]:.2f}%', 
                   xy=(outlier['year'], outlier_y_position), 
                   xytext=(0, -15), textcoords='offset points',
                   ha='center', va='top', fontsize=10, fontweight='bold',
                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor='gray'))
        
        print(f"  Outlier marked: {outlier['receiver']} in {outlier['year']} with {outlier['percentage']:.2f}%")
    
    # Set labels
    ax.set_xlabel('Year of knowledge flow', fontsize=16, fontweight='bold', labelpad=8)  # Reduced font size
    ax.set_ylabel('Percentage knowledge received', 
                  fontsize=16, fontweight='bold', labelpad=8)  # Reduced font size
    
    # Set ticks
    ax.set_xticks(year_ticks)
    ax.set_xticklabels([str(year) for year in year_ticks], rotation=45, fontsize=12)  # Reduced font size
    
    # Set y-axis ticks for fixed 1.4% limit
    y_ticks = np.arange(0, global_y_max + 0.2, 0.2)
    ax.set_yticks(y_ticks)
    ax.set_yticklabels([f'{y:.1f}%' for y in y_ticks], fontsize=12)  # Reduced font size
    
    # Add grid
    ax.grid(True, alpha=0.3, color='lightgray')
    for year in year_ticks:
        ax.axvline(x=year, color='lightgray', alpha=0.2, linewidth=0.5)
    
    # LEGEND MOVED TO BOTTOM RIGHT - outside the plot area
    legend = ax.legend(handles=legend_elements, title='SIB knowledge receiver:', 
                      loc='center', bbox_to_anchor=(0.785, -0.27), ncol=2,
                      fontsize=10, title_fontsize=11,  # Reduced font sizes
                      framealpha=0.9, edgecolor='gray')
    
    # Add subtitle explaining aggregation below the plot, further left
    fig.text(0.1, 0.02, 'SIB knowledge providers (excluding receiver):\nLayered Oxides (D)), Polyanionic (E)), PBA (F))\nand Hard Carbon (I)',
            ha='left', va='bottom', fontsize=12,  # Reduced font size
            style='italic', color='black')
    
    # Adjust layout to accommodate bottom legend and text
    plt.tight_layout()
    plt.subplots_adjust(bottom=0.25)  # More room for legend and annotation
    
    # Save the figure
    filename = 'SIB_to_SIB_Knowledge_Receivers_Single_Plot_Aggregated_2012-2022.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"\nVisualization saved as '{filename}'")
    
    # Create DataFrame with all plotted data for verification
    plotted_data_df = pd.DataFrame(plotted_data_list)
    
    # Save plotted data to CSV for inspection
    plotted_data_filename = 'plotted_data_SIB_to_SIB_single_plot_aggregated_2012-2022.csv'
    plotted_data_df.to_csv(plotted_data_filename, index=False)
    print(f"Plotted data saved to '{plotted_data_filename}' for verification")
    
    # Print verification statistics
    print("\n" + "="*60)
    print("AGGREGATED SIB TO SIB DATA VERIFICATION (2012-2022)")
    print("="*60)
    
    if not plotted_data_df.empty:
        print(f"Total plotted data points: {len(plotted_data_df)}")
        print(f"Year range: {plotted_data_df['year'].min()} - {plotted_data_df['year'].max()}")
        print(f"Knowledge percentage range: {plotted_data_df['knowledge_percentage'].min():.2f}% - {plotted_data_df['knowledge_percentage'].max():.2f}%")
        
        # Show summary by receiver
        print(f"\nSummary by receiver:")
        receiver_summary = plotted_data_df.groupby('receiver').agg({
            'knowledge_percentage': ['count', 'min', 'max', 'mean'],
            'total_backward_citations': 'first'
        }).round(2)
        print(receiver_summary)
    
    plt.show()
    
    return fig, plotted_data_df


# Execute the analysis with aggregated SIB providers
print("Creating SIB to SIB Knowledge Receiver Analysis with aggregated other SIB providers (2012-2022)...")
fig, plotted_data_df = create_sib_to_sib_knowledge_receiver_single_plot(structure_citations_df, structure_family_colors, combined_df)

print("\n" + "="*80)
print("SIB TO SIB KNOWLEDGE RECEIVER ANALYSIS COMPLETE (AGGREGATED, 2012-2022)")
print("="*80)
print("Created single plot showing knowledge flow from other SIB chemistries to SIB structure families:")
print("- Knowledge Providers: Other SIB chemistries (aggregated, excluding respective receiver)")
print("- Knowledge Receivers: SIB Layered Oxides, SIB Polyanionic, SIB PBA, SIB Hard Carbon")
print("\nVisualization features:")
print("- X-axis: Years of knowledge transfer (2012-2022) - TRUNCATED with extended limits")
print("- Y-axis: Percentage of knowledge received from other SIB chemistries (limited to 1.4%)")
print("- Point colors: Different SIB knowledge receivers")
print("- Dashed lines: Linear trend lines")
print("- Outlier markers: Triangular markers at y=1.36% with actual values annotated below")
print("- Data aggregation: Other SIB providers combined (excluding respective receiver)")
print("="*80)


#%%


#==============================================================
# SIB Knowledge Receiver Analysis - Share of SIB Citations
# Neue Normierung: SIB citation count / Total backward citations of SIB receiver
# Zeigt den Anteil der SIB-Zitationen an allen Zitationen der SIB-Technologie
# X-axis truncated from 2012 to 2022
#==============================================================

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from matplotlib.lines import Line2D
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import warnings
warnings.filterwarnings('ignore')

def determine_linear_fit(x_data, y_data):
    """
    Simple linear fit for guidance lines.
    """
    if len(x_data) < 2:
        return np.array(x_data), np.array(y_data)
    
    x_array = np.array(x_data).reshape(-1, 1)
    y_array = np.array(y_data)
    
    # Generate smooth x values for trend line (within data range only)
    x_smooth = np.linspace(min(x_data), max(x_data), 100).reshape(-1, 1)
    
    try:
        # Linear fit only
        linear_reg = LinearRegression()
        linear_reg.fit(x_array, y_array)
        y_smooth_linear = linear_reg.predict(x_smooth)
        
        return x_smooth.flatten(), y_smooth_linear
                
    except Exception as e:
        # Fallback to simple linear interpolation
        return np.array(x_data), np.array(y_data)


def create_sib_to_sib_knowledge_receiver_share_analysis(structure_citations_df, structure_family_colors, combined_df):
    """
    Create visualization showing SIB citation share of total backward citations for each SIB receiver.
    Normalization: SIB_citation_count / total_backward_citations_of_SIB_receiver_per_year
    X-axis truncated from 2012 to 2022.
    """
    
    # Define all SIB structure families
    all_sib_families = [
        'SIB_Layered_Oxides',
        'SIB_Polyanionic',
        'SIB_PBA', 
        'SIB_Anode_Hard_Carbon'
    ]
    
    # Display names for better readability
    display_names = {
        'SIB_Layered_Oxides': 'SIB Layered Oxides (4)',
        'SIB_Polyanionic': 'SIB Polyanionic (5)',
        'SIB_PBA': 'SIB PBA (6)',
        'SIB_Anode_Hard_Carbon': 'SIB Hard Carbon (9)'
    }
    
    # Convert RGBA to RGB for matplotlib (since we'll handle alpha separately)
    sib_receiver_colors_rgb = {
        'SIB_Layered_Oxides': (255/255, 102/255, 102/255),    # Light Red
        'SIB_Polyanionic': (0/255, 102/255, 255/255),         # Medium Blue
        'SIB_PBA': (0/255, 204/255, 0/255),                   # Green
        'SIB_Anode_Hard_Carbon': (153/255, 153/255, 153/255)  # Light Gray
    }
    
    # Get ALL citations for SIB receivers to calculate total backward citations
    all_sib_citations = structure_citations_df[
        structure_citations_df['citing_structure_family'].isin(all_sib_families)
    ]
    
    if all_sib_citations.empty:
        print("No SIB citations found in the dataset")
        return
    
    print(f"Found {len(all_sib_citations)} total SIB citations for analysis")
    
    # Year range - TRUNCATED FROM 2012 TO 2022 with extended limits
    year_min, year_max = 2011.5, 2022.5  # Plot limits extended for visibility
    year_range = list(range(2012, 2023))  # Data range (2012-2022 inclusive)
    year_ticks = list(range(2012, 2023))  # Tick labels
    
    # Create list to store all plotted data for verification
    plotted_data_list = []
    
    # Create figure with single plot - REDUCED SIZE
    fig, ax = plt.subplots(1, 1, figsize=(10, 6.67))  # Reduced from 15x10 to 10x6.67 (same ratio)
    
    # Collect all data for each SIB receiver
    all_plot_data = {}
    global_max_y = 0
    
    for sib_receiver in all_sib_families:
        print(f"\nProcessing {display_names[sib_receiver]}...")
        
        # Define providers for this receiver (all other SIB families)
        sib_providers_for_this_receiver = [fam for fam in all_sib_families if fam != sib_receiver]
        
        # Filter SIB→SIB citations for this specific SIB receiver
        sib_receiver_citations = structure_citations_df[
            (structure_citations_df['cited_structure_family'].isin(sib_providers_for_this_receiver)) &
            (structure_citations_df['citing_structure_family'] == sib_receiver)
        ]
        
        # Filter ALL citations for this specific SIB receiver (for total backward citations)
        all_receiver_citations = all_sib_citations[
            all_sib_citations['citing_structure_family'] == sib_receiver
        ]
        
        if sib_receiver_citations.empty:
            print(f"No SIB citations found for {sib_receiver}")
            continue
            
        if all_receiver_citations.empty:
            print(f"No total citations found for {sib_receiver}")
            continue
        
        # Calculate citation counts and backward citations for each year
        year_data = {}
        for year in year_range:
            # Count SIB→SIB citations for this year
            sib_year_citations = sib_receiver_citations[
                sib_receiver_citations['citing_priority_year'] == year
            ]
            # Count unique citing_patent -> cited_patent pairs to avoid double counting
            unique_patent_citations = sib_year_citations[['citing_patent', 'cited_patent']].drop_duplicates()
            sib_citation_count = len(unique_patent_citations)
            
            # Calculate total backward citations for this SIB receiver in this year
            # Sum of citing_patent_backward_citations for UNIQUE citing patents
            all_year_citations = all_receiver_citations[
                all_receiver_citations['citing_priority_year'] == year
            ]
            
            if len(all_year_citations) == 0:
                total_backward_citations = 0
            else:
                # Get unique citing patents and sum their backward citations
                unique_patents_data = all_year_citations.drop_duplicates(subset=['citing_patent'])
                total_backward_citations = unique_patents_data['citing_patent_backward_citations'].sum()
            
            year_data[year] = {
                'sib_citation_count': sib_citation_count,
                'total_backward_citations': total_backward_citations
            }
        
        # Calculate share values for each year
        years_to_plot = []
        share_values = []
        
        for year in year_range:
            sib_count = year_data[year]['sib_citation_count']
            total_backward = year_data[year]['total_backward_citations']
            
            # Only plot years where we have both SIB citations AND total backward citations
            if sib_count > 0 and total_backward > 0:
                # NEW NORMALIZATION: sib_citation_count / total_backward_citations
                share_value = sib_count / total_backward
                
                years_to_plot.append(year)
                share_values.append(share_value)
                
                # Store data for verification
                plotted_data_list.append({
                    'receiver': sib_receiver,
                    'year': year,
                    'sib_citation_count': sib_count,
                    'total_backward_citations': total_backward,
                    'share_value': share_value
                })
        
        # Store data if we have any
        if years_to_plot:
            print(f"  Found {len(years_to_plot)} data points")
            print(f"  Share range: {min(share_values):.3f} - {max(share_values):.3f}")
            
            # Calculate linear fit
            if len(years_to_plot) >= 2:
                trend_x, trend_y = determine_linear_fit(years_to_plot, share_values)
                max_trend_y = max(trend_y) if len(trend_y) > 0 else 0
                global_max_y = max(global_max_y, max(share_values), max_trend_y)
            else:
                trend_x, trend_y = years_to_plot, share_values
                global_max_y = max(global_max_y, max(share_values))
            
            all_plot_data[sib_receiver] = {
                'years': years_to_plot,
                'share_values': share_values,
                'color': sib_receiver_colors_rgb[sib_receiver],
                'label': display_names[sib_receiver],
                'trend_x': trend_x,
                'trend_y': trend_y
            }
    
    # Set global y-axis limit to 30%
    global_y_max = 0.3  # 30% limit
    outlier_threshold = 0.3  # Values above 30% will be marked as outliers
    
    # Plot data for all SIB receivers
    legend_elements = []
    outliers = []  # Track outliers for annotation
    
    for sib_receiver, data in all_plot_data.items():
        # Plot scatter points with transparency
        scatter = ax.scatter(data['years'], data['share_values'], 
                            c=[data['color']], s=60, alpha=0.6,
                            edgecolors='black', linewidth=0.8,
                            label=data['label'])
        
        # Check for outliers and collect them
        for i, (year, share_value) in enumerate(zip(data['years'], data['share_values'])):
            if share_value > outlier_threshold:
                outliers.append({
                    'year': year,
                    'share_value': share_value,
                    'receiver': data['label'],
                    'color': data['color']
                })
        
        # Add linear trend line
        if len(data['years']) >= 2:
            ax.plot(data['trend_x'], data['trend_y'], '--', 
                    color=data['color'], alpha=1.0, linewidth=2)
        
        # Create legend element
        legend_elements.append(scatter)
        
        print(f"  {data['label']}: {len(data['years'])} data points plotted")
    
    # Configure plot
    ax.set_xlim(year_min, year_max)
    ax.set_ylim(0, global_y_max)
    
    # Annotate high-dependency outliers if any
    if outliers:
        outlier_y_position = global_y_max * 0.95  # Position near top
        for outlier in outliers:
            # Position outlier point
            ax.scatter(outlier['year'], outlier_y_position, c=outlier['color'], s=80, alpha=0.8,
                      marker='^', edgecolors='black', linewidth=1.2)
            
            # Add annotation with actual value
            ax.annotate(f'{outlier["share_value"]*100:.1f}%', 
                        xy=(outlier['year'], outlier_y_position), 
                        xytext=(0, -15), textcoords='offset points',
                        ha='center', va='top', fontsize=10, fontweight='bold',
                        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor='gray'))
            
            print(f"  High SIB dependency: {outlier['receiver']} in {outlier['year']} with {outlier['share_value']*100:.1f}%")
    
    # Set labels with updated y-axis description
    ax.set_xlabel('Year of knowledge flow', fontsize=16, fontweight='bold', labelpad=8)
    ax.set_ylabel('Percentage of SIB knowledge flows\nwithin the total knowledge base (%)', 
                  fontsize=16, fontweight='bold', labelpad=8)
    
    # Set ticks
    ax.set_xticks(year_ticks)
    ax.set_xticklabels([str(year) for year in year_ticks], rotation=45, fontsize=12)
    
    # Set y-axis ticks as percentages (0% to 30% in 5% steps)
    y_ticks = np.arange(0, global_y_max + 0.05, 0.05)  # 5% Schritte: 0%, 5%, 10%, 15%, 20%, 25%, 30%
    ax.set_yticks(y_ticks)
    ax.set_yticklabels([f'{y*100:.0f}%' for y in y_ticks], fontsize=12)  # Konvertiert zu Prozent
    
    # Add grid
    ax.grid(True, alpha=0.3, color='lightgray')
    for year in year_ticks:
        ax.axvline(x=year, color='lightgray', alpha=0.2, linewidth=0.5)
    
    # LEGEND MOVED TO BOTTOM RIGHT - outside the plot area
    legend = ax.legend(handles=legend_elements, title='SIB knowledge receiver:', 
                      loc='center', bbox_to_anchor=(0.785, -0.29), ncol=2,
                      fontsize=10, title_fontsize=11,
                      framealpha=0.9, edgecolor='gray')
    
    # Add subtitle explaining normalization
    fig.text(-0.11, -0.29, 
            'Normalization:\nSIB to SIB knowledge flows / Total knowledge base of SIB receiver\n'
            'Shows the share of SIB knowledge in total knowledge absorption\n'
            'Higher values indicate stronger dependence on SIB knowledge',
            ha='left', va='center', fontsize=11,
            style='italic', color='black', transform=ax.transAxes)
    
    # Adjust layout to accommodate bottom legend and text
    plt.tight_layout()
    plt.subplots_adjust(bottom=0.28)  # More room for legend and annotation
    
    # Save the figure
    filename = 'SIB_to_SIB_Knowledge_Receivers_Share_2012-2022.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"\nVisualization saved as '{filename}'")
    
    # Create DataFrame with all plotted data for verification
    plotted_data_df = pd.DataFrame(plotted_data_list)
    
    # Save plotted data to CSV for inspection
    plotted_data_filename = 'plotted_data_SIB_to_SIB_share_2012-2022.csv'
    plotted_data_df.to_csv(plotted_data_filename, index=False)
    print(f"Plotted data saved to '{plotted_data_filename}' for verification")
    
    # Print verification statistics
    print("\n" + "="*60)
    print("SIB TO SIB SHARE ANALYSIS VERIFICATION (2012-2022)")
    print("="*60)
    
    if not plotted_data_df.empty:
        print(f"Total plotted data points: {len(plotted_data_df)}")
        print(f"Year range: {plotted_data_df['year'].min()} - {plotted_data_df['year'].max()}")
        print(f"Share value range: {plotted_data_df['share_value'].min():.3f} - {plotted_data_df['share_value'].max():.3f}")
        
        # Show summary by receiver
        print(f"\nSummary by receiver:")
        receiver_summary = plotted_data_df.groupby('receiver').agg({
            'share_value': ['count', 'min', 'max', 'mean'],
            'sib_citation_count': 'sum',
            'total_backward_citations': 'sum'
        }).round(3)
        print(receiver_summary)
        
        # Show interpretation guide
        print(f"\nInterpretation:")
        print(f"- Value = 0%: No SIB citations (complete independence)")
        print(f"- Value = 15%: 15% of citations come from SIB")
        print(f"- Value = 30%: 30% of citations come from SIB (high dependence)")
        print(f"- Higher values: Stronger dependence on SIB knowledge")
        print(f"- Lower values: More diverse knowledge sources")
    
    plt.show()
    
    return fig, plotted_data_df


# Execute the analysis with new share-based normalization
print("Creating SIB to SIB Knowledge Receiver Analysis with SIB share normalization (2012-2022)...")
print("Normalization: SIB_citation_count / total_backward_citations_of_SIB_receiver")
fig, plotted_data_df = create_sib_to_sib_knowledge_receiver_share_analysis(structure_citations_df, structure_family_colors, combined_df)

print("\n" + "="*80)
print("SIB TO SIB KNOWLEDGE RECEIVER ANALYSIS COMPLETE (SIB SHARE NORMALIZATION, 2012-2022)")
print("="*80)
print("Created plot showing SIB citation share of total backward citations for SIB structure families:")
print("- Knowledge Provider: Material-specific SIB chemistries")
print("- Knowledge Receivers: SIB Layered Oxides, SIB Polyanionic, SIB PBA, SIB Hard Carbon")
print("- Normalization: SIB_citation_count / total_backward_citations_of_SIB_receiver")
print("\nVisualization features:")
print("- X-axis: Years of knowledge transfer (2012-2022)")
print("- Y-axis: Share of SIB citations (0% to 30%)")
print("- Point colors: Different SIB knowledge receivers")
print("- Dashed lines: Linear trend lines")
print("- Higher values: Stronger dependence on SIB knowledge")
print("- Lower values: More diverse knowledge sources beyond SIB")
print("="*80)


#%%

#==============================================================
# Battery Knowledge Flow Analysis - Panel Figure
# Panel (a): LIB to LIB knowledge flows
# Panel (b): SIB to SIB knowledge flows  
# Panel (c): LIB to SIB knowledge flows
# Vertical layout with bottom legend
#==============================================================

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from matplotlib.lines import Line2D
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import warnings
warnings.filterwarnings('ignore')

def determine_linear_fit(x_data, y_data):
    """
    Simple linear fit for guidance lines.
    """
    if len(x_data) < 2:
        return np.array(x_data), np.array(y_data)
    
    x_array = np.array(x_data).reshape(-1, 1)
    y_array = np.array(y_data)
    
    # Generate smooth x values for trend line (within data range only)
    x_smooth = np.linspace(min(x_data), max(x_data), 100).reshape(-1, 1)
    
    try:
        # Linear fit only
        linear_reg = LinearRegression()
        linear_reg.fit(x_array, y_array)
        y_smooth_linear = linear_reg.predict(x_smooth)
        
        return x_smooth.flatten(), y_smooth_linear
                
    except Exception as e:
        # Fallback to simple linear interpolation
        return np.array(x_data), np.array(y_data)


def create_knowledge_flow_summary_table(structure_citations_df, combined_df):
    """
    Create a comprehensive summary table of knowledge flows by provider, receiver, and year.
    Includes patent counts and backward citations for context.
    Saves to Excel with separate sheets for each flow type.
    """
    import pandas as pd
    from openpyxl import Workbook
    from openpyxl.styles import Font, PatternFill, Alignment
    
    print("Creating knowledge flow summary table...")
    
    # Define flow types and their configurations
    flow_configs = {
        'LIB_to_LIB': {
            'providers': ['LIB_Layered_Oxides', 'LIB_Polyanionic', 'LIB_Spinel', 'LIB_Anode_Graphite_based', 'LIB_Anode_LTO'],
            'receivers': ['LIB_Layered_Oxides', 'LIB_Polyanionic', 'LIB_Spinel', 'LIB_Anode_Graphite_based', 'LIB_Anode_LTO'],
            'year_range': range(2000, 2023),
            'exclude_self': True
        },
        'SIB_to_SIB': {
            'providers': ['SIB_Layered_Oxides', 'SIB_Polyanionic', 'SIB_PBA', 'SIB_Anode_Hard_Carbon'],
            'receivers': ['SIB_Layered_Oxides', 'SIB_Polyanionic', 'SIB_PBA', 'SIB_Anode_Hard_Carbon'],
            'year_range': range(2010, 2023),
            'exclude_self': True
        },
        'LIB_to_SIB': {
            'providers': ['LIB_Layered_Oxides', 'LIB_Polyanionic', 'LIB_Spinel', 'LIB_Anode_Graphite_based', 'LIB_Anode_LTO'],
            'receivers': ['SIB_Layered_Oxides', 'SIB_Polyanionic', 'SIB_PBA', 'SIB_Anode_Hard_Carbon'],
            'year_range': range(2012, 2023),
            'exclude_self': False
        }
    }
    
    # Create Excel writer
    filename = 'Knowledge_Flow_Extended_Summary_Table.xlsx'
    with pd.ExcelWriter(filename, engine='openpyxl') as writer:
        
        for flow_type, config in flow_configs.items():
            print(f"Processing {flow_type}...")
            
            # Create summary data
            summary_data = []
            
            for receiver in config['receivers']:
                # Define providers for this receiver
                if config['exclude_self']:
                    providers = [p for p in config['providers'] if p != receiver]
                else:
                    providers = config['providers']
                
                for year in config['year_range']:
                    # Get receiver context data for this year
                    receiver_context = get_receiver_context_data(receiver, year, structure_citations_df, combined_df)
                    
                    for provider in providers:
                        # Filter citations for this specific combination
                        citations = structure_citations_df[
                            (structure_citations_df['cited_structure_family'] == provider) &
                            (structure_citations_df['citing_structure_family'] == receiver) &
                            (structure_citations_df['citing_priority_year'] == year)
                        ]
                        
                        # Count weighted citations
                        citation_count = citations['individual_citation_count'].sum() if len(citations) > 0 else 0
                        unique_patent_pairs = len(citations[['citing_patent', 'cited_patent']].drop_duplicates()) if len(citations) > 0 else 0
                        
                        # Only add row if there's data or context data exists
                        if citation_count > 0 or receiver_context['patent_count'] > 0:
                            summary_data.append({
                                'Year': year,
                                'Knowledge_Provider': provider,
                                'Knowledge_Receiver': receiver,
                                'Weighted_Citation_Count': citation_count,
                                'Unique_Patent_Pairs': unique_patent_pairs,
                                'Receiver_Patent_Count': receiver_context['patent_count'],
                                'Receiver_Total_Backward_Citations': receiver_context['total_backward_citations'],
                                'Citation_Share_Percent': round(
                                    (citation_count / receiver_context['total_backward_citations'] * 100) 
                                    if receiver_context['total_backward_citations'] > 0 else 0, 2
                                ),
                                'Flow_Type': flow_type
                            })
            
            # Create DataFrame
            if summary_data:
                summary_df = pd.DataFrame(summary_data)
                
                # Create pivot tables for different metrics
                
                # 1. Citation Count Pivot
                pivot_citations = summary_df.pivot_table(
                    index=['Knowledge_Provider', 'Knowledge_Receiver'],
                    columns='Year',
                    values='Weighted_Citation_Count',
                    fill_value=0,
                    aggfunc='sum'
                )
                pivot_citations['Total'] = pivot_citations.sum(axis=1)
                pivot_citations.loc['TOTAL'] = pivot_citations.sum()
                
                # 2. Citation Share Pivot
                pivot_shares = summary_df.pivot_table(
                    index=['Knowledge_Provider', 'Knowledge_Receiver'],
                    columns='Year',
                    values='Citation_Share_Percent',
                    fill_value=0,
                    aggfunc='mean'
                )
                
                # 3. Context data (unique per receiver-year)
                context_df = summary_df[['Year', 'Knowledge_Receiver', 'Receiver_Patent_Count', 'Receiver_Total_Backward_Citations']].drop_duplicates()
                context_pivot_patents = context_df.pivot_table(
                    index='Knowledge_Receiver',
                    columns='Year',
                    values='Receiver_Patent_Count',
                    fill_value=0,
                    aggfunc='first'
                )
                context_pivot_citations = context_df.pivot_table(
                    index='Knowledge_Receiver',
                    columns='Year',
                    values='Receiver_Total_Backward_Citations',
                    fill_value=0,
                    aggfunc='first'
                )
                
                # Save to Excel sheets
                summary_df.to_excel(writer, sheet_name=f'{flow_type}_Raw', index=False)
                pivot_citations.to_excel(writer, sheet_name=f'{flow_type}_Citations')
                pivot_shares.to_excel(writer, sheet_name=f'{flow_type}_Shares')
                context_pivot_patents.to_excel(writer, sheet_name=f'{flow_type}_PatentCounts')
                context_pivot_citations.to_excel(writer, sheet_name=f'{flow_type}_BackwardCitations')
                
                print(f"  {flow_type}: {len(summary_data)} records created")
            else:
                print(f"  {flow_type}: No data found")
        
        # Create an overview sheet
        create_overview_sheet_extended(writer, flow_configs, structure_citations_df, combined_df)
    
    print(f"Excel file saved as '{filename}'")
    return filename

def get_receiver_context_data(receiver, year, structure_citations_df, combined_df):
    """
    Get context data for a knowledge receiver in a specific year:
    - Number of patents published
    - Total backward citations
    """
    
    # Get all citations where this receiver is the citing family in this year
    receiver_citations = structure_citations_df[
        (structure_citations_df['citing_structure_family'] == receiver) &
        (structure_citations_df['citing_priority_year'] == year)
    ]
    
    if len(receiver_citations) == 0:
        return {
            'patent_count': 0,
            'total_backward_citations': 0
        }
    
    # Count unique patents (citing_patent)
    unique_patents = receiver_citations['citing_patent'].nunique()
    
    # Sum total backward citations (avoiding double counting)
    unique_patent_data = receiver_citations.drop_duplicates(subset=['citing_patent'])
    total_backward_citations = unique_patent_data['citing_patent_backward_citations'].sum()
    
    return {
        'patent_count': unique_patents,
        'total_backward_citations': total_backward_citations
    }

def create_overview_sheet_extended(writer, flow_configs, structure_citations_df, combined_df):
    """Create an overview sheet with extended summary statistics."""
    
    overview_data = []
    
    for flow_type, config in flow_configs.items():
        for receiver in config['receivers']:
            # Define providers for this receiver
            if config['exclude_self']:
                providers = [p for p in config['providers'] if p != receiver]
            else:
                providers = config['providers']
            
            # Get total context for this receiver across all years
            total_receiver_context = {
                'total_patents': 0,
                'total_backward_citations': 0,
                'years_active': 0
            }
            
            for year in config['year_range']:
                context = get_receiver_context_data(receiver, year, structure_citations_df, combined_df)
                if context['patent_count'] > 0:
                    total_receiver_context['total_patents'] += context['patent_count']
                    total_receiver_context['total_backward_citations'] += context['total_backward_citations']
                    total_receiver_context['years_active'] += 1
            
            for provider in providers:
                # Get all citations for this provider-receiver pair
                citations = structure_citations_df[
                    (structure_citations_df['cited_structure_family'] == provider) &
                    (structure_citations_df['citing_structure_family'] == receiver) &
                    (structure_citations_df['citing_priority_year'].isin(config['year_range']))
                ]
                
                if len(citations) > 0:
                    total_weighted = citations['individual_citation_count'].sum()
                    total_pairs = len(citations[['citing_patent', 'cited_patent']].drop_duplicates())
                    years_active = citations['citing_priority_year'].nunique()
                    first_year = citations['citing_priority_year'].min()
                    last_year = citations['citing_priority_year'].max()
                    
                    # Calculate share of total receiver citations
                    total_share = round(
                        (total_weighted / total_receiver_context['total_backward_citations'] * 100) 
                        if total_receiver_context['total_backward_citations'] > 0 else 0, 2
                    )
                    
                    overview_data.append({
                        'Flow_Type': flow_type,
                        'Knowledge_Provider': provider,
                        'Knowledge_Receiver': receiver,
                        'Total_Weighted_Citations': total_weighted,
                        'Total_Unique_Patent_Pairs': total_pairs,
                        'Years_Active': years_active,
                        'First_Year': first_year,
                        'Last_Year': last_year,
                        'Average_Citations_Per_Year': round(total_weighted / years_active, 2) if years_active > 0 else 0,
                        'Receiver_Total_Patents': total_receiver_context['total_patents'],
                        'Receiver_Total_Backward_Citations': total_receiver_context['total_backward_citations'],
                        'Provider_Share_of_Receiver_Citations_Percent': total_share
                    })
    
    if overview_data:
        overview_df = pd.DataFrame(overview_data)
        overview_df = overview_df.sort_values(['Flow_Type', 'Total_Weighted_Citations'], ascending=[True, False])
        overview_df.to_excel(writer, sheet_name='Overview', index=False)



def create_battery_knowledge_flow_panel_figure(structure_citations_df, structure_family_colors, combined_df):
    """
    Create panel figure with three knowledge flow analyses:
    (a) LIB to LIB, (b) SIB to SIB, (c) LIB to SIB
    Vertical layout with bottom legend
    """
    
    # Define all structure families and their colors
    all_families = {
        'LIB_Layered_Oxides': {'name': 'LIB Layered Oxides (1)', 'color': (255/255, 0/255, 0/255)},        # Red
        'LIB_Polyanionic': {'name': 'LIB Polyanionic (2)', 'color': (0/255, 0/255, 255/255)},             # Blue  
        'LIB_Spinel': {'name': 'LIB Spinel (3)', 'color': (0/255, 255/255, 255/255)},                    # Cyan
        'SIB_Layered_Oxides': {'name': 'SIB Layered Oxides (4)', 'color': (255/255, 102/255, 102/255)},  # Light Red
        'SIB_Polyanionic': {'name': 'SIB Polyanionic (5)', 'color': (0/255, 102/255, 255/255)},          # Medium Blue
        'SIB_PBA': {'name': 'SIB PBA (6)', 'color': (0/255, 204/255, 0/255)},                           # Green
        'LIB_Anode_Graphite_based': {'name': 'LIB Graphite-based (7)', 'color': (102/255, 102/255, 102/255)}, # Gray
        'LIB_Anode_LTO': {'name': 'LIB LTO (8)', 'color': (255/255, 165/255, 0/255)},                   # Orange
        'SIB_Anode_Hard_Carbon': {'name': 'SIB Hard Carbon (9)', 'color': (153/255, 153/255, 153/255)}   # Light Gray
    }
    
    # Create figure with vertical layout: 3 rows, single column
    fig, axes = plt.subplots(3, 1, figsize=(10, 14))
    
    # Panel (a): LIB to LIB - top
    ax_a = axes[0]
    # Add panel title in top-left corner, moved down
    ax_a.text(0.02, 0.92, '(a) LIB to LIB knowledge flows', transform=ax_a.transAxes, 
              fontsize=14, fontweight='bold',
              bbox=dict(boxstyle='round,pad=0.4', facecolor='white', alpha=0.3, edgecolor='gray'))
    plot_data_a = analyze_lib_to_lib_flows(structure_citations_df, combined_df, ax_a, all_families)
    
    # Panel (b): SIB to SIB - middle
    ax_b = axes[1]
    ax_b.text(0.02, 0.92, '(b) SIB to SIB knowledge flows', transform=ax_b.transAxes, 
              fontsize=14, fontweight='bold',
              bbox=dict(boxstyle='round,pad=0.4', facecolor='white', alpha=0.5, edgecolor='gray'))
    plot_data_b = analyze_sib_to_sib_flows(structure_citations_df, combined_df, ax_b, all_families)
    
    # Panel (c): LIB to SIB - bottom
    ax_c = axes[2]
    ax_c.text(0.02, 0.92, '(c) LIB to SIB knowledge flows', transform=ax_c.transAxes, 
              fontsize=14, fontweight='bold',
              bbox=dict(boxstyle='round,pad=0.4', facecolor='white', alpha=0.5, edgecolor='gray'))
    plot_data_c = analyze_lib_to_sib_flows(structure_citations_df, combined_df, ax_c, all_families)
    
    # Create horizontal legend at the bottom
    legend_elements = []
    family_order = ['LIB_Layered_Oxides', 'LIB_Polyanionic', 'LIB_Spinel', 'SIB_Layered_Oxides', 
                    'SIB_Polyanionic', 'SIB_PBA', 'LIB_Anode_Graphite_based', 'LIB_Anode_LTO', 
                    'SIB_Anode_Hard_Carbon']
    
    for family_key in family_order:
        family_info = all_families[family_key]
        legend_elements.append(
            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=family_info['color'], 
                      markersize=8, alpha=0.7, markeredgecolor='black', markeredgewidth=0.6)
        )
    
    # Extract just the names for labels
    legend_labels = [all_families[fam]['name'] for fam in family_order]
    
    # Add horizontal legend below all subplots
    fig.legend(handles=legend_elements, labels=legend_labels,
              title='Battery chemistries:', 
              loc='lower center', bbox_to_anchor=(0.5, -0.02), ncol=5,
              fontsize=10, title_fontsize=12, framealpha=0.9, edgecolor='gray',
              columnspacing=1.0, handletextpad=0.5)
    
    # Adjust layout (no main title anymore)
    plt.tight_layout()
    plt.subplots_adjust(bottom=0.12, hspace=0.3)  # Room for legend
    
    # Save figure
    filename = 'Battery_Knowledge_Flow_Panel_Figure_Final.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"Panel figure saved as '{filename}'")
    
    plt.show()
    
    return fig, plot_data_a, plot_data_b, plot_data_c


def analyze_lib_to_lib_flows(structure_citations_df, combined_df, ax, all_families):
    """
    Analyze LIB to LIB knowledge flows for panel (a)
    """
    lib_families = ['LIB_Layered_Oxides', 'LIB_Polyanionic', 'LIB_Spinel', 'LIB_Anode_Graphite_based', 'LIB_Anode_LTO']
    
    # Get ALL citations for LIB receivers
    all_lib_citations = structure_citations_df[
        structure_citations_df['citing_structure_family'].isin(lib_families)
    ]
    
    year_range = list(range(2002, 2023))
    year_ticks = list(range(2002, 2023, 2))  # Every 2 years for better spacing
    plotted_data_list = []
    
    for lib_receiver in lib_families:
        lib_providers = [fam for fam in lib_families if fam != lib_receiver]
        
        # Filter LIB→LIB citations
        lib_receiver_citations = structure_citations_df[
            (structure_citations_df['cited_structure_family'].isin(lib_providers)) &
            (structure_citations_df['citing_structure_family'] == lib_receiver)
        ]
        
        all_receiver_citations = all_lib_citations[
            all_lib_citations['citing_structure_family'] == lib_receiver
        ]
        
        if lib_receiver_citations.empty or all_receiver_citations.empty:
            continue
        
        years_to_plot = []
        share_values = []
        
        for year in year_range:
            # Count UNIQUE patent-to-patent citations
            lib_year_citations = lib_receiver_citations[
                lib_receiver_citations['citing_priority_year'] == year
            ]
            # Count WEIGHTED individual citations (sum of individual_citation_count)
            lib_citation_count = lib_year_citations['individual_citation_count'].sum()
            
            # Total backward citations
            all_year_citations = all_receiver_citations[
                all_receiver_citations['citing_priority_year'] == year
            ]
            
            if len(all_year_citations) == 0:
                total_backward_citations = 0
            else:
                unique_patents_data = all_year_citations.drop_duplicates(subset=['citing_patent'])
                total_backward_citations = unique_patents_data['citing_patent_backward_citations'].sum()
            
            if lib_citation_count > 0 and total_backward_citations > 0:
                share_value = lib_citation_count / total_backward_citations
                years_to_plot.append(year)
                share_values.append(share_value)
                
                plotted_data_list.append({
                    'receiver': lib_receiver,
                    'year': year,
                    'lib_citation_count': lib_citation_count,
                    'total_backward_citations': total_backward_citations,
                    'share_value': share_value
                })
        
        if years_to_plot:
            family_info = all_families[lib_receiver]
            
            # Plot scatter points - larger size for better visibility
            ax.scatter(years_to_plot, share_values, 
                      c=[family_info['color']], s=60, alpha=0.7,
                      edgecolors='black', linewidth=0.8)
            
            # Add trend line
            if len(years_to_plot) >= 2:
                trend_x, trend_y = determine_linear_fit(years_to_plot, share_values)
                ax.plot(trend_x, trend_y, '--', color=family_info['color'], alpha=0.9, linewidth=2)
    
    # Configure panel (a)
    ax.set_xlim(2001.5, 2022.5)
    ax.set_ylim(0, 0.3)
    ax.set_xlabel('Year', fontsize=13, fontweight='bold')
    ax.set_ylabel('Percentage of LIB knowledge flows\nwithin the total knowledge base (%)', 
                  fontsize=13, fontweight='bold')
    
    ax.set_xticks(year_ticks)
    ax.set_xticklabels([str(year) for year in year_ticks], rotation=45, fontsize=11)
    
    y_ticks = np.arange(0, 0.35, 0.05)
    ax.set_yticks(y_ticks)
    ax.set_yticklabels([f'{y*100:.0f}%' for y in y_ticks], fontsize=11)
    
    ax.grid(True, alpha=0.3, color='lightgray')
    
    return plotted_data_list


def analyze_sib_to_sib_flows(structure_citations_df, combined_df, ax, all_families):
    """
    Analyze SIB to SIB knowledge flows for panel (b)
    """
    sib_families = ['SIB_Layered_Oxides', 'SIB_Polyanionic', 'SIB_PBA', 'SIB_Anode_Hard_Carbon']
    
    # Get ALL citations for SIB receivers
    all_sib_citations = structure_citations_df[
        structure_citations_df['citing_structure_family'].isin(sib_families)
    ]
    
    year_range = list(range(2017, 2023))
    year_ticks = list(range(2017, 2023, 1))  # Every year
    plotted_data_list = []
    
    for sib_receiver in sib_families:
        sib_providers = [fam for fam in sib_families if fam != sib_receiver]
        
        # Filter SIB→SIB citations
        sib_receiver_citations = structure_citations_df[
            (structure_citations_df['cited_structure_family'].isin(sib_providers)) &
            (structure_citations_df['citing_structure_family'] == sib_receiver)
        ]
        
        all_receiver_citations = all_sib_citations[
            all_sib_citations['citing_structure_family'] == sib_receiver
        ]
        
        if sib_receiver_citations.empty or all_receiver_citations.empty:
            continue
        
        years_to_plot = []
        share_values = []
        
        for year in year_range:
            # Count UNIQUE patent-to-patent citations
            sib_year_citations = sib_receiver_citations[
                sib_receiver_citations['citing_priority_year'] == year
            ]
          
            
            # Count WEIGHTED individual citations (sum of individual_citation_count)
            sib_citation_count = sib_year_citations['individual_citation_count'].sum()
            
            # Total backward citations
            all_year_citations = all_receiver_citations[
                all_receiver_citations['citing_priority_year'] == year
            ]
            
            if len(all_year_citations) == 0:
                total_backward_citations = 0
            else:
                unique_patents_data = all_year_citations.drop_duplicates(subset=['citing_patent'])
                total_backward_citations = unique_patents_data['citing_patent_backward_citations'].sum()
            
            if sib_citation_count > 0 and total_backward_citations > 0:
                share_value = sib_citation_count / total_backward_citations
                years_to_plot.append(year)
                share_values.append(share_value)
                
                plotted_data_list.append({
                    'receiver': sib_receiver,
                    'year': year,
                    'sib_citation_count': sib_citation_count,
                    'total_backward_citations': total_backward_citations,
                    'share_value': share_value
                })
        
        if years_to_plot:
            family_info = all_families[sib_receiver]
            
            # Plot scatter points
            ax.scatter(years_to_plot, share_values, 
                      c=[family_info['color']], s=60, alpha=0.7,
                      edgecolors='black', linewidth=0.8)
            
            # Add trend line
            if len(years_to_plot) >= 2:
                trend_x, trend_y = determine_linear_fit(years_to_plot, share_values)
                ax.plot(trend_x, trend_y, '--', color=family_info['color'], alpha=0.9, linewidth=2)
    
    # Configure panel (b)
    ax.set_xlim(2016.8, 2022.2)
    ax.set_ylim(0, 0.3)
    ax.set_xlabel('Year', fontsize=13, fontweight='bold')
    ax.set_ylabel('Percentage of SIB knowledge flows\nwithin the total knowledge base (%)', 
                  fontsize=13, fontweight='bold')
    
    ax.set_xticks(year_ticks)
    ax.set_xticklabels([str(year) for year in year_ticks], rotation=45, fontsize=11)
    
    y_ticks = np.arange(0, 0.35, 0.05)
    ax.set_yticks(y_ticks)
    ax.set_yticklabels([f'{y*100:.0f}%' for y in y_ticks], fontsize=11)
    
    ax.grid(True, alpha=0.3, color='lightgray')
    
    return plotted_data_list


def analyze_lib_to_sib_flows(structure_citations_df, combined_df, ax, all_families):
    """
    Analyze LIB to SIB knowledge flows for panel (c)
    """
    lib_providers = ['LIB_Layered_Oxides', 'LIB_Polyanionic', 'LIB_Spinel', 'LIB_Anode_Graphite_based', 'LIB_Anode_LTO']
    sib_receivers = ['SIB_Layered_Oxides', 'SIB_Polyanionic', 'SIB_PBA', 'SIB_Anode_Hard_Carbon']
    
    # Get ALL citations for SIB receivers
    all_sib_citations = structure_citations_df[
        structure_citations_df['citing_structure_family'].isin(sib_receivers)
    ]
    
    year_range = list(range(2017, 2023))
    year_ticks = list(range(2017, 2023, 1))  # Every year
    plotted_data_list = []
    
    for sib_receiver in sib_receivers:
        # Filter LIB→SIB citations
        lib_receiver_citations = structure_citations_df[
            (structure_citations_df['cited_structure_family'].isin(lib_providers)) &
            (structure_citations_df['citing_structure_family'] == sib_receiver)
        ]
        
        all_receiver_citations = all_sib_citations[
            all_sib_citations['citing_structure_family'] == sib_receiver
        ]
        
        if lib_receiver_citations.empty or all_receiver_citations.empty:
            continue
        
        years_to_plot = []
        share_values = []
        
        for year in year_range:
            # Count UNIQUE patent-to-patent citations
            lib_year_citations = lib_receiver_citations[
                lib_receiver_citations['citing_priority_year'] == year
            ]
            
            # Count WEIGHTED individual citations (sum of individual_citation_count)
            lib_citation_count = lib_year_citations['individual_citation_count'].sum()
            
            # Total backward citations
            all_year_citations = all_receiver_citations[
                all_receiver_citations['citing_priority_year'] == year
            ]
            
            if len(all_year_citations) == 0:
                total_backward_citations = 0
            else:
                unique_patents_data = all_year_citations.drop_duplicates(subset=['citing_patent'])
                total_backward_citations = unique_patents_data['citing_patent_backward_citations'].sum()
            
            if lib_citation_count > 0 and total_backward_citations > 0:
                share_value = lib_citation_count / total_backward_citations
                years_to_plot.append(year)
                share_values.append(share_value)
                
                plotted_data_list.append({
                    'receiver': sib_receiver,
                    'year': year,
                    'lib_citation_count': lib_citation_count,
                    'total_backward_citations': total_backward_citations,
                    'share_value': share_value
                })
        
        if years_to_plot:
            family_info = all_families[sib_receiver]
            
            # Plot scatter points
            ax.scatter(years_to_plot, share_values, 
                      c=[family_info['color']], s=60, alpha=0.7,
                      edgecolors='black', linewidth=0.8)
            
            # Add trend line
            if len(years_to_plot) >= 2:
                trend_x, trend_y = determine_linear_fit(years_to_plot, share_values)
                ax.plot(trend_x, trend_y, '--', color=family_info['color'], alpha=0.9, linewidth=2)
    
    # Configure panel (c)
    ax.set_xlim(2016.8, 2022.2)
    ax.set_ylim(0, 0.3)
    ax.set_xlabel('Year', fontsize=13, fontweight='bold')
    ax.set_ylabel('Percentage of LIB knowledge flows\nwithin the total knowledge base (%)', 
                  fontsize=13, fontweight='bold')
    
    ax.set_xticks(year_ticks)
    ax.set_xticklabels([str(year) for year in year_ticks], rotation=45, fontsize=11)
    
    y_ticks = np.arange(0, 0.35, 0.05)
    ax.set_yticks(y_ticks)
    ax.set_yticklabels([f'{y*100:.0f}%' for y in y_ticks], fontsize=11)
    
    ax.grid(True, alpha=0.3, color='lightgray')
    
    return plotted_data_list




# Execute the panel analysis
print("Creating Battery Knowledge Flow Panel Figure (Final Layout)...")
fig, data_a, data_b, data_c = create_battery_knowledge_flow_panel_figure(structure_citations_df, structure_family_colors, combined_df)

print("\n" + "="*80)
print("BATTERY KNOWLEDGE FLOW PANEL FIGURE COMPLETE (FINAL LAYOUT)")
print("="*80)
print("Created panel figure with optimized layout:")
print("- Full panel titles in top-left corners (moved down)")
print("- Horizontal legend below all panels")
print("- Original y-axis labels restored")
print("- No main title")
print("- Unique patent-to-patent citation counting")
print("="*80)

# Create knowledge flow summary table
excel_filename = create_knowledge_flow_summary_table(structure_citations_df, combined_df)
print(f"Knowledge flow summary saved to: {excel_filename}")


#%%


#==============================================================
# Self-Referencing Knowledge Flow Analysis
# Analysiert wie viel Wissen von der gleichen Strukturfamilie kommt
# (z.B. SIB Polyanionic → SIB Polyanionic)
#==============================================================

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LinearRegression

def analyze_self_referencing_knowledge_flows(structure_citations_df, combined_df):
    """
    Analyze how much knowledge comes from the same structure family (self-referencing).
    """
    
    # Define all structure families
    all_families = {
        'LIB_Layered_Oxides': {'name': 'LIB Layered Oxides (1)', 'color': (255/255, 0/255, 0/255), 'type': 'LIB'},
        'LIB_Polyanionic': {'name': 'LIB Polyanionic (2)', 'color': (0/255, 0/255, 255/255), 'type': 'LIB'},
        'LIB_Spinel': {'name': 'LIB Spinel (3)', 'color': (0/255, 255/255, 255/255), 'type': 'LIB'},
        'SIB_Layered_Oxides': {'name': 'SIB Layered Oxides (4)', 'color': (255/255, 102/255, 102/255), 'type': 'SIB'},
        'SIB_Polyanionic': {'name': 'SIB Polyanionic (5)', 'color': (0/255, 102/255, 255/255), 'type': 'SIB'},
        'SIB_PBA': {'name': 'SIB PBA (6)', 'color': (0/255, 204/255, 0/255), 'type': 'SIB'},
        'LIB_Anode_Graphite_based': {'name': 'LIB Graphite-based (7)', 'color': (102/255, 102/255, 102/255), 'type': 'LIB'},
        'LIB_Anode_LTO': {'name': 'LIB LTO (8)', 'color': (255/255, 165/255, 0/255), 'type': 'LIB'},
        'SIB_Anode_Hard_Carbon': {'name': 'SIB Hard Carbon (9)', 'color': (153/255, 153/255, 153/255), 'type': 'SIB'}
    }
    
    # Determine year ranges for each battery type
    lib_families = [fam for fam, info in all_families.items() if info['type'] == 'LIB']
    sib_families = [fam for fam, info in all_families.items() if info['type'] == 'SIB']
    
    year_ranges = {
        'LIB': range(2002, 2023),
        'SIB': range(2017, 2023)
    }
    
    # Collect all self-referencing data
    all_self_ref_data = []
    
    print("Analyzing self-referencing knowledge flows...")
    
    for family, family_info in all_families.items():
        battery_type = family_info['type']
        year_range = year_ranges[battery_type]
        
        print(f"\nProcessing {family_info['name']}...")
        
        # Get ALL citations for this family as receiver
        all_family_citations = structure_citations_df[
            structure_citations_df['citing_structure_family'] == family
        ]
        
        if all_family_citations.empty:
            print(f"  No citations found for {family}")
            continue
        
        years_to_plot = []
        self_ref_shares = []
        
        for year in year_range:
            # Get all citations for this family in this year
            year_citations = all_family_citations[
                all_family_citations['citing_priority_year'] == year
            ]
            
            if len(year_citations) == 0:
                continue
            
            # Count self-referencing citations (same family → same family)
            self_ref_citations = year_citations[
                year_citations['cited_structure_family'] == family
            ]
            self_ref_count = self_ref_citations['individual_citation_count'].sum()
            
            # Count total backward citations for this family in this year
            unique_patents_data = year_citations.drop_duplicates(subset=['citing_patent'])
            total_backward_citations = unique_patents_data['citing_patent_backward_citations'].sum()
            
            if total_backward_citations > 0:
                self_ref_share = self_ref_count / total_backward_citations
                years_to_plot.append(year)
                self_ref_shares.append(self_ref_share)
                
                all_self_ref_data.append({
                    'family': family,
                    'family_name': family_info['name'],
                    'battery_type': battery_type,
                    'year': year,
                    'self_ref_count': self_ref_count,
                    'total_backward_citations': total_backward_citations,
                    'self_ref_share': self_ref_share,
                    'color': family_info['color']
                })
        
        if years_to_plot:
            print(f"  Found {len(years_to_plot)} data points")
            print(f"  Self-reference range: {min(self_ref_shares)*100:.1f}% - {max(self_ref_shares)*100:.1f}%")
    
    # Create visualization
    if all_self_ref_data:
        create_self_referencing_visualization(all_self_ref_data, all_families)
        create_self_referencing_excel_report(all_self_ref_data)
    
    return all_self_ref_data

def create_self_referencing_visualization(data, all_families):
    """
    Create visualization of self-referencing knowledge flows.
    """
    
    # Create figure with two subplots (LIB and SIB)
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))
    
    # Convert data to DataFrame for easier handling
    df = pd.DataFrame(data)
    
    # Plot LIB families
    lib_data = df[df['battery_type'] == 'LIB']
    plot_families_self_ref(lib_data, ax1, 'LIB', range(2002, 2023, 2))
    
    # Plot SIB families  
    sib_data = df[df['battery_type'] == 'SIB']
    plot_families_self_ref(sib_data, ax2, 'SIB', range(2017, 2023, 1))
    
    # Add panel labels in top-left corners
    ax1.text(0.02, 0.92, '(a) LIB Self-Referencing Knowledge Flows', transform=ax1.transAxes, 
             fontsize=12, fontweight='bold',
             bbox=dict(boxstyle='round,pad=0.4', facecolor='white', alpha=0.3, edgecolor='gray'))
    
    ax2.text(0.02, 0.92, '(b) SIB Self-Referencing Knowledge Flows', transform=ax2.transAxes, 
             fontsize=12, fontweight='bold',
             bbox=dict(boxstyle='round,pad=0.4', facecolor='white', alpha=0.3, edgecolor='gray'))
    
    # Create legend (Haupttitel entfernt)
    legend_elements = []
    for family, family_info in all_families.items():
        legend_elements.append(
            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=family_info['color'], 
                      markersize=8, alpha=0.7, markeredgecolor='black', markeredgewidth=0.6,
                      label=family_info['name'])
        )
    
    # Add legend below subplots
    fig.legend(handles=legend_elements, title='Battery chemistries:', 
              loc='lower center', bbox_to_anchor=(0.5, -0.02), ncol=5,
              fontsize=9, title_fontsize=10, framealpha=0.9, edgecolor='gray',
              columnspacing=1.0, handletextpad=0.5)
    
    plt.tight_layout()
    plt.subplots_adjust(bottom=0.15)  # Weniger Platz oben nötig ohne Haupttitel
    
    # Save figure
    filename = 'Self_Referencing_Knowledge_Flows.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"Self-referencing visualization saved as '{filename}'")
    
    plt.show()

def plot_families_self_ref(data, ax, battery_type, year_ticks):
    """
    Plot self-referencing data for families of one battery type.
    """
    
    # Plot each family
    for family in data['family'].unique():
        family_data = data[data['family'] == family]
        
        if len(family_data) == 0:
            continue
        
        years = family_data['year'].values
        shares = family_data['self_ref_share'].values
        color = family_data['color'].iloc[0]
        
        # Plot scatter points
        ax.scatter(years, shares, c=[color], s=60, alpha=0.7,
                  edgecolors='black', linewidth=0.8)
        
        # Add trend line if enough points
        if len(years) >= 2:
            # Linear fit
            X = years.reshape(-1, 1)
            y = shares
            
            model = LinearRegression()
            model.fit(X, y)
            
            # Create smooth trend line
            trend_years = np.linspace(min(years), max(years), 100)
            trend_shares = model.predict(trend_years.reshape(-1, 1))
            
            ax.plot(trend_years, trend_shares, '--', color=color, alpha=0.8, linewidth=2)
    
    # Configure subplot (TITEL ENTFERNT)
    if battery_type == 'LIB':
        ax.set_xlim(2001.5, 2022.5)
        # ax.set_title entfernt
    else:
        ax.set_xlim(2016.8, 2022.2)
        # ax.set_title entfernt
    
    ax.set_ylim(0, 0.3)
    ax.set_xlabel('Year', fontsize=12, fontweight='bold')
    ax.set_ylabel('Self-Referencing Share (%)', fontsize=12, fontweight='bold')
    
    # Set ticks
    ax.set_xticks(year_ticks)
    ax.set_xticklabels([str(year) for year in year_ticks], rotation=45, fontsize=10)
    
    y_ticks = np.arange(0, 0.35, 0.05)
    ax.set_yticks(y_ticks)
    ax.set_yticklabels([f'{y*100:.0f}%' for y in y_ticks], fontsize=10)
    
    ax.grid(True, alpha=0.3, color='lightgray')

def create_self_referencing_excel_report(data):
    """
    Create Excel report with self-referencing data.
    """
    
    df = pd.DataFrame(data)
    
    # Create pivot table
    pivot_df = df.pivot_table(
        index=['family_name', 'battery_type'],
        columns='year',
        values='self_ref_share',
        fill_value=0
    )
    
    # Convert to percentages
    pivot_df = pivot_df * 100
    
    # Add statistics
    pivot_df['Average_%'] = pivot_df.mean(axis=1).round(1)
    pivot_df['Min_%'] = pivot_df.min(axis=1).round(1)
    pivot_df['Max_%'] = pivot_df.max(axis=1).round(1)
    
    # Save to Excel
    filename = 'Self_Referencing_Knowledge_Analysis.xlsx'
    with pd.ExcelWriter(filename, engine='openpyxl') as writer:
        # Raw data
        df.to_excel(writer, sheet_name='Raw_Data', index=False)
        
        # Pivot table with percentages
        pivot_df.to_excel(writer, sheet_name='Self_Ref_Percentages')
        
        # Summary statistics
        summary_df = df.groupby(['family_name', 'battery_type']).agg({
            'self_ref_share': ['count', 'mean', 'min', 'max', 'std'],
            'self_ref_count': 'sum',
            'total_backward_citations': 'sum'
        }).round(3)
        summary_df.to_excel(writer, sheet_name='Summary_Statistics')
    
    print(f"Self-referencing Excel report saved as '{filename}'")

# Execute the self-referencing analysis
print("="*80)
print("SELF-REFERENCING KNOWLEDGE FLOW ANALYSIS")
print("="*80)

self_ref_data = analyze_self_referencing_knowledge_flows(structure_citations_df, combined_df)

print("\n" + "="*80)
print("SELF-REFERENCING ANALYSIS COMPLETE")
print("="*80)
print("Analysis shows how much knowledge each structure family receives from itself")
print("- Higher percentages = more reliance on own knowledge")
print("- Lower percentages = more diverse knowledge sources")
print("- Two visualizations: LIB families (2000-2022) and SIB families (2010-2022)")
print("="*80)





#%%


#==============================================================
# Combined SIB and LIB Knowledge Receiver Analysis
# Two side-by-side plots with shared y-axis
# Optimized for 16.5 cm width with larger font sizes
#==============================================================

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from matplotlib.lines import Line2D
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import warnings
warnings.filterwarnings('ignore')

def determine_linear_fit(x_data, y_data):
    """
    Simple linear fit for guidance lines.
    """
    if len(x_data) < 2:
        return np.array(x_data), np.array(y_data)
    
    x_array = np.array(x_data).reshape(-1, 1)
    y_array = np.array(y_data)
    
    # Generate smooth x values for trend line (within data range only)
    x_smooth = np.linspace(min(x_data), max(x_data), 100).reshape(-1, 1)
    
    try:
        # Linear fit only
        linear_reg = LinearRegression()
        linear_reg.fit(x_array, y_array)
        y_smooth_linear = linear_reg.predict(x_smooth)
        
        return x_smooth.flatten(), y_smooth_linear
                
    except Exception as e:
        # Fallback to simple linear interpolation
        return np.array(x_data), np.array(y_data)


def create_combined_knowledge_receiver_plots(structure_citations_df, structure_family_colors, combined_df):
    """
    Create combined visualization showing both SIB and LIB knowledge receivers side by side.
    """
    
    # Define structure families
    lib_families = [
        'LIB_Layered_Oxides',
        'LIB_Polyanionic', 
        'LIB_Spinel',
        'LIB_Anode_Graphite_based',
        'LIB_Anode_LTO'
    ]
    
    sib_receivers = [
        'SIB_Layered_Oxides',
        'SIB_Polyanionic',
        'SIB_PBA', 
        'SIB_Anode_Hard_Carbon'
    ]
    
    # Display names for better readability
    lib_display_names = {
        'LIB_Layered_Oxides': 'LIB Layered Oxides',
        'LIB_Polyanionic': 'LIB Polyanionic',
        'LIB_Spinel': 'LIB Spinel',
        'LIB_Anode_Graphite_based': 'LIB Graphite-based',
        'LIB_Anode_LTO': 'LIB LTO'
    }
    
    sib_display_names = {
        'SIB_Layered_Oxides': 'SIB Layered Oxides',
        'SIB_Polyanionic': 'SIB Polyanionic',
        'SIB_PBA': 'SIB PBA',
        'SIB_Anode_Hard_Carbon': 'SIB Hard Carbon'
    }
    
    # Define colors
    lib_receiver_colors_rgb = {
        'LIB_Layered_Oxides': (255/255, 0/255, 0/255),        # Red
        'LIB_Polyanionic': (0/255, 0/255, 255/255),           # Blue  
        'LIB_Spinel': (0/255, 255/255, 255/255),              # Cyan
        'LIB_Anode_Graphite_based': (102/255, 102/255, 102/255), # Gray
        'LIB_Anode_LTO': (255/255, 165/255, 0/255)            # Orange
    }
    
    sib_receiver_colors_rgb = {
        'SIB_Layered_Oxides': (255/255, 102/255, 102/255),    # Light Red
        'SIB_Polyanionic': (0/255, 102/255, 255/255),         # Medium Blue
        'SIB_PBA': (0/255, 204/255, 0/255),                   # Green
        'SIB_Anode_Hard_Carbon': (153/255, 153/255, 153/255)  # Light Gray
    }
    
    # Create structure family aggregation for backward citations
    family_aggregated_df = create_structure_family_aggregated_df(combined_df)
    
    # Year ranges for different analyses
    sib_year_min, sib_year_max = 2011.5, 2022.5
    sib_year_range = list(range(2012, 2023))
    sib_year_ticks = list(range(2012, 2023))
    
    lib_year_min, lib_year_max = 1999.5, 2022.5
    lib_year_range = list(range(2000, 2023))
    lib_year_ticks = list(range(2000, 2023, 2))
    
    # Create figure with two side-by-side subplots - optimized for 16.5 cm width
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6.5, 4.5), sharey=True)  # 16.5 cm ≈ 6.5 inches
    fig.suptitle('Knowledge Flow Between Battery Chemistries', 
                 fontsize=9, fontweight='bold', y=0.96)
    
    # Set global y-axis limit
    global_y_max = 1.4
    outlier_threshold = 1.4
    outlier_y_position = 1.36
    
    #==============================================================
    # LEFT PLOT: SIB Knowledge Receivers
    #==============================================================
    
    print("Processing SIB Knowledge Receivers...")
    
    # LIB providers for SIB analysis
    lib_providers = [
        'LIB_Layered_Oxides',
        'LIB_Polyanionic', 
        'LIB_Spinel',
        'LIB_Anode_Graphite_based',
        'LIB_Anode_LTO'
    ]
    
    # Filter to LIB→SIB flows
    lib_to_sib_citations = structure_citations_df[
        (structure_citations_df['cited_structure_family'].isin(lib_providers)) &
        (structure_citations_df['citing_structure_family'].isin(sib_receivers))
    ]
    
    sib_outliers = []
    sib_legend_elements = []
    
    for sib_receiver in sib_receivers:
        receiver_citations = lib_to_sib_citations[
            lib_to_sib_citations['citing_structure_family'] == sib_receiver
        ]
        
        if receiver_citations.empty:
            continue
        
        receiver_data = family_aggregated_df[
            (family_aggregated_df['structure_family'] == sib_receiver) &
            (family_aggregated_df['battery_type'] == 'SIB')
        ]
        
        if receiver_data.empty:
            continue
        
        total_receiver_backward_citations = receiver_data['backward_citations'].sum()
        
        if total_receiver_backward_citations == 0:
            continue
        
        years_to_plot = []
        percentages_to_plot = []
        
        for year in sib_year_range:
            year_citations = receiver_citations[
                receiver_citations['citing_priority_year'] == year
            ]
            
            if year_citations.empty:
                continue
            
            total_count = len(year_citations)
            if total_count > 0:
                knowledge_percentage = (total_count / total_receiver_backward_citations) * 100
                years_to_plot.append(year)
                percentages_to_plot.append(knowledge_percentage)
        
        if years_to_plot:
            color = sib_receiver_colors_rgb[sib_receiver]
            
            # Plot scatter points
            scatter = ax1.scatter(years_to_plot, percentages_to_plot, 
                               c=[color], s=35, alpha=0.4,
                               edgecolors='black', linewidth=0.5,
                               label=sib_display_names[sib_receiver])
            
            # Check for outliers
            for year, percentage in zip(years_to_plot, percentages_to_plot):
                if percentage > outlier_threshold:
                    sib_outliers.append({
                        'year': year,
                        'percentage': percentage,
                        'receiver': sib_display_names[sib_receiver],
                        'color': color
                    })
            
            # Add trend line
            if len(years_to_plot) >= 2:
                trend_x, trend_y = determine_linear_fit(years_to_plot, percentages_to_plot)
                ax1.plot(trend_x, trend_y, '--', color=color, alpha=1.0, linewidth=1.2)
            
            sib_legend_elements.append(scatter)
    
    # Annotate SIB outliers
    for outlier in sib_outliers:
        ax1.scatter(outlier['year'], outlier_y_position, c=outlier['color'], s=45, alpha=0.8,
                  marker='^', edgecolors='black', linewidth=0.8)
        ax1.annotate(f'{outlier["percentage"]:.2f}%', 
                   xy=(outlier['year'], outlier_y_position), 
                   xytext=(0, -10), textcoords='offset points',
                   ha='center', va='top', fontsize=5, fontweight='bold',
                   bbox=dict(boxstyle='round,pad=0.15', facecolor='white', alpha=0.8, edgecolor='gray'))
    
    #==============================================================
    # RIGHT PLOT: LIB Knowledge Receivers
    #==============================================================
    
    print("Processing LIB Knowledge Receivers...")
    
    lib_outliers = []
    lib_legend_elements = []
    
    for lib_receiver in lib_families:
        # Define providers for this receiver (all other LIB families)
        lib_providers_for_this_receiver = [fam for fam in lib_families if fam != lib_receiver]
        
        # Filter to LIB→LIB flows for this specific receiver
        lib_to_lib_citations = structure_citations_df[
            (structure_citations_df['cited_structure_family'].isin(lib_providers_for_this_receiver)) &
            (structure_citations_df['citing_structure_family'] == lib_receiver)
        ]
        
        if lib_to_lib_citations.empty:
            continue
        
        receiver_data = family_aggregated_df[
            (family_aggregated_df['structure_family'] == lib_receiver) &
            (family_aggregated_df['battery_type'] == 'LIB')
        ]
        
        if receiver_data.empty:
            continue
        
        total_receiver_backward_citations = receiver_data['backward_citations'].sum()
        
        if total_receiver_backward_citations == 0:
            continue
        
        years_to_plot = []
        percentages_to_plot = []
        
        for year in lib_year_range:
            year_citations = lib_to_lib_citations[
                lib_to_lib_citations['citing_priority_year'] == year
            ]
            
            if year_citations.empty:
                continue
            
            total_count = len(year_citations)
            if total_count > 0:
                knowledge_percentage = (total_count / total_receiver_backward_citations) * 100
                years_to_plot.append(year)
                percentages_to_plot.append(knowledge_percentage)
        
        if years_to_plot:
            color = lib_receiver_colors_rgb[lib_receiver]
            
            # Plot scatter points
            scatter = ax2.scatter(years_to_plot, percentages_to_plot, 
                               c=[color], s=35, alpha=0.4,
                               edgecolors='black', linewidth=0.5,
                               label=lib_display_names[lib_receiver])
            
            # Check for outliers
            for year, percentage in zip(years_to_plot, percentages_to_plot):
                if percentage > outlier_threshold:
                    lib_outliers.append({
                        'year': year,
                        'percentage': percentage,
                        'receiver': lib_display_names[lib_receiver],
                        'color': color
                    })
            
            # Add trend line
            if len(years_to_plot) >= 2:
                trend_x, trend_y = determine_linear_fit(years_to_plot, percentages_to_plot)
                ax2.plot(trend_x, trend_y, '--', color=color, alpha=1.0, linewidth=1.2)
            
            lib_legend_elements.append(scatter)
    
    # Annotate LIB outliers
    for outlier in lib_outliers:
        ax2.scatter(outlier['year'], outlier_y_position, c=outlier['color'], s=45, alpha=0.8,
                  marker='^', edgecolors='black', linewidth=0.8)
        ax2.annotate(f'{outlier["percentage"]:.2f}%', 
                   xy=(outlier['year'], outlier_y_position), 
                   xytext=(0, -10), textcoords='offset points',
                   ha='center', va='top', fontsize=5, fontweight='bold',
                   bbox=dict(boxstyle='round,pad=0.15', facecolor='white', alpha=0.8, edgecolor='gray'))
    
    #==============================================================
    # Configure both plots
    #==============================================================
    
    # Left plot (SIB)
    ax1.set_xlim(sib_year_min, sib_year_max)
    ax1.set_ylim(0, global_y_max)
    ax1.set_title('Material-specific LIB → SIB', fontsize=10, fontweight='bold', pad=8)
    ax1.set_xlabel('Years of Knowledge Transfer', fontsize=9, fontweight='bold')
    ax1.set_ylabel('Knowledge received (%)', fontsize=9, fontweight='bold')
    
    ax1.set_xticks(sib_year_ticks)
    ax1.set_xticklabels([str(year) for year in sib_year_ticks], rotation=45, fontsize=8)
    
    # Right plot (LIB)
    ax2.set_xlim(lib_year_min, lib_year_max)
    ax2.set_ylim(0, global_y_max)
    ax2.set_title('Other LIB → LIB', fontsize=10, fontweight='bold', pad=8)
    ax2.set_xlabel('Years of Knowledge Transfer', fontsize=9, fontweight='bold')
    
    ax2.set_xticks(lib_year_ticks)
    ax2.set_xticklabels([str(year) for year in lib_year_ticks], rotation=45, fontsize=8)
    
    # Set y-axis ticks only for left plot
    y_ticks = np.arange(0, global_y_max + 0.2, 0.2)
    ax1.set_yticks(y_ticks)
    ax1.set_yticklabels([f'{y:.1f}' for y in y_ticks], fontsize=8)
    
    # Add grids
    ax1.grid(True, alpha=0.3, color='lightgray')
    ax2.grid(True, alpha=0.3, color='lightgray')
    
    # Add legends below plots
    # Create legend entries with smaller markers for better fit
    sib_legend_handles = []
    sib_legend_labels = []
    for element in sib_legend_elements:
        color = element.get_facecolors()[0]
        label = element.get_label()
        sib_legend_handles.append(plt.Line2D([0], [0], marker='o', color='w', 
                                            markerfacecolor=color, markersize=4, 
                                            markeredgecolor='black', markeredgewidth=0.3))
        sib_legend_labels.append(label)
    
    lib_legend_handles = []
    lib_legend_labels = []
    for element in lib_legend_elements:
        color = element.get_facecolors()[0]
        label = element.get_label()
        lib_legend_handles.append(plt.Line2D([0], [0], marker='o', color='w', 
                                            markerfacecolor=color, markersize=4, 
                                            markeredgecolor='black', markeredgewidth=0.3))
        lib_legend_labels.append(label)
    
    # Position legends below the subplots
    legend1 = fig.legend(handles=sib_legend_handles, labels=sib_legend_labels, 
                        title='SIB Receiver:', loc='lower left', 
                        bbox_to_anchor=(0.05, 0.1), fontsize=5, title_fontsize=5,
                        framealpha=0.9, edgecolor='gray', ncol=2)
    
    legend2 = fig.legend(handles=lib_legend_handles, labels=lib_legend_labels, 
                        title='LIB Receiver:', loc='lower right', 
                        bbox_to_anchor=(0.95, 0.1), fontsize=5, title_fontsize=5,
                        framealpha=0.9, edgecolor='gray', ncol=3)
    
    # Add bottom annotations
    fig.text(0.25, 0.05, 'LIB chemistries: Layered Oxides,\nPolyanionic, Spinel, Graphite-based, LTO',
            ha='center', va='bottom', fontsize=5, 
            style='italic', color='gray')
    
    fig.text(0.75, 0.05, 'Other LIB chemistries exclude\nthe respective receiver',
            ha='center', va='bottom', fontsize=5, 
            style='italic', color='gray')
    
    # Adjust layout
    plt.tight_layout()
    plt.subplots_adjust(bottom=0.32, top=0.88)  # More space for legends and annotations
    
    # Save the figure
    filename = 'Combined_Knowledge_Receivers_Analysis.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"\nVisualization saved as '{filename}'")
    
    plt.show()
    
    return fig


# Execute the combined analysis
print("Creating Combined Knowledge Receiver Analysis...")
fig = create_combined_knowledge_receiver_plots(structure_citations_df, structure_family_colors, combined_df)

print("\n" + "="*80)
print("COMBINED KNOWLEDGE RECEIVER ANALYSIS COMPLETE")
print("="*80)
print("Created side-by-side visualization showing:")
print("- Left: Knowledge flow from material-specific LIB chemistries to SIB structure families")
print("- Right: Knowledge flow from other LIB chemistries to LIB structure families")
print("\nVisualization features:")
print("- Optimized for 16.5 cm width with larger, readable fonts")
print("- Shared y-axis to save space")
print("- Different time ranges: SIB (2012-2022), LIB (2000-2022)")
print("- Outlier markers with values at y=1.36%")
print("- Linear trend lines for all receivers")
print("="*80)

#%%

#==============================================================
# Create comprehensive heatmaps using Percentage of Citations
# Both Material Level and Chemical Structure Family Level
# Supports both "incoming" and "outgoing" citation normalization
#==============================================================

# CONFIGURATION: Choose normalization type
# Set to "incoming" for percentage of incoming citations (columns sum to 100%)
# Set to "outgoing" for percentage of outgoing citations (rows sum to 100%)
NORMALIZATION_TYPE = "incoming"  # Change this to "incoming" or "outgoing"

# Set to "linear" for normal percentage scale (0-100%)
# Set to "logarithmic" for log10 scale (better for wide value ranges)
SCALE_TYPE = "linear"  # Change this to "linear" or "logarithmic"


#==============================================================
# STRUCTURE FAMILY CITATION DATASET CREATION
#==============================================================

def get_structure_families_for_materials(materials_list):
    """
    Map individual materials to their corresponding structure families.
    This function needs to be customized based on your material classification.
    """
    
    # Define material to structure family mappings - UPDATED ANODE MAPPING
    material_to_family = {
        # LIB Cathode Materials -> Structure Families
        'LCO': 'LIB_Layered_Oxides',
        'NMC': 'LIB_Layered_Oxides', 
        'NCA': 'LIB_Layered_Oxides',
        'LFP': 'LIB_Polyanionic',
        'LMO': 'LIB_Spinel',
        
        # LIB Anode Materials -> Structure Families (UPDATED)
        'Graphite': 'LIB_Anode_Graphite_based',
        'Silicon/Carbon': 'LIB_Anode_Graphite_based',
        'LTO': 'LIB_Anode_LTO',
        
        # SIB Cathode Materials -> Structure Families
        'NFM': 'SIB_Layered_Oxides',
        'CFM': 'SIB_Layered_Oxides',
        'NMO': 'SIB_Layered_Oxides',
        'NVPF': 'SIB_Polyanionic',
        'NFPP/NFP': 'SIB_Polyanionic',
        'Fe-PBA': 'SIB_PBA',
        'Mn-PBA': 'SIB_PBA',
        
        # SIB Anode Materials -> Structure Families (UPDATED)
        'Hard Carbon': 'SIB_Anode_Hard_Carbon'
    }
    
    # Extract unique structure families for the given materials
    families = set()
    
    if isinstance(materials_list, list):
        for material in materials_list:
            if material in material_to_family:
                families.add(material_to_family[material])
    else:
        # Handle case where materials_list might be a single string or other type
        print(f"Warning: Expected list but got {type(materials_list)}: {materials_list}")
    
    return list(families)

def classify_citation_lag(citing_year, cited_year):
    """
    Classify the time lag between citing and cited patents.
    Only distinguishes between Short (<5 years) and Long (≥5 years).
    """
    if pd.isna(citing_year) or pd.isna(cited_year):
        return 'Unknown'
    
    lag = citing_year - cited_year
    
    if lag < 0:
        return 'Negative'  # Citing patent is older (shouldn't happen normally)
    elif lag < 5:
        return 'Short (<5 years)'
    else:
        return 'Long (≥5 years)'

# Define structure family metadata - UPDATED
structure_family_types = {
    'LIB_Layered_Oxides': 'cathode',
    'LIB_Polyanionic': 'cathode',
    'LIB_Spinel': 'cathode',
    'LIB_Anode_Graphite_based': 'anode',  # UPDATED
    'LIB_Anode_LTO': 'anode',
    'SIB_Layered_Oxides': 'cathode',
    'SIB_Polyanionic': 'cathode',
    'SIB_PBA': 'cathode',
    'SIB_Anode_Hard_Carbon': 'anode'  # UPDATED
}

structure_family_battery_types = {
    'LIB_Layered_Oxides': 'LIB',
    'LIB_Polyanionic': 'LIB',
    'LIB_Spinel': 'LIB',
    'LIB_Anode_Graphite_based': 'LIB',  # UPDATED
    'LIB_Anode_LTO': 'LIB',
    'SIB_Layered_Oxides': 'SIB',
    'SIB_Polyanionic': 'SIB',
    'SIB_PBA': 'SIB',
    'SIB_Anode_Hard_Carbon': 'SIB'  # UPDATED
}

def create_structure_family_citation_dataset(df):
    """Create a dataset with all citations between different chemical structure families."""
    
    print("Creating structure family citation dataset...")
    citation_data = []
    
    # Create lookup dictionaries
    cathode_materials_dict = df.set_index('node_num')['Cathode Material'].to_dict()
    anode_materials_dict = df.set_index('node_num')['Anode Material'].to_dict()
    priority_date_dict = df.set_index('node_num')['earliest_priority_date'].to_dict()
    priority_year_dict = df.set_index('node_num')['priority_year'].to_dict()
    battery_type_dict = df.set_index('node_num')['battery_type'].to_dict()
    
    print(f"Processing {len(df)} patents for structure family citations...")
    
    # Iterate through all rows
    for idx, row in df.iterrows():
        if idx % 1000 == 0:  # Progress indicator
            print(f"Processing patent {idx+1}/{len(df)}")
            
        cited_patent = row['node_num']
        cited_cathode_materials = row['Cathode Material']
        cited_anode_materials = row['Anode Material']
        cited_priority_date = row['earliest_priority_date']
        cited_priority_year = row['priority_year']
        cited_battery_type = row['battery_type']
        
        # Combine cathode and anode materials
        cited_all_materials = []
        if isinstance(cited_cathode_materials, list):
            cited_all_materials.extend(cited_cathode_materials)
        if isinstance(cited_anode_materials, list):
            cited_all_materials.extend(cited_anode_materials)
        
        # Get structure families for cited patent
        cited_families = get_structure_families_for_materials(cited_all_materials)
        
        # Skip if no valid structure families found
        if not cited_families:
            continue
        
        # Check if 'Citing_node_numbers' exists and is not empty
        citing_patents = row.get('Citing_node_numbers', [])
        if not isinstance(citing_patents, list):
            citing_patents = []
        
        # Iterate through all citing patents
        for citing_patent in citing_patents:
            # Check if the citing patent exists in the dataset
            if citing_patent in cathode_materials_dict and citing_patent in anode_materials_dict:
                citing_cathode_materials = cathode_materials_dict[citing_patent]
                citing_anode_materials = anode_materials_dict[citing_patent]
                citing_priority_date = priority_date_dict[citing_patent]
                citing_priority_year = priority_year_dict[citing_patent]
                citing_battery_type = battery_type_dict[citing_patent]
                
                # Skip self-citations
                if cited_patent == citing_patent:
                    continue
                
                # Combine citing materials
                citing_all_materials = []
                if isinstance(citing_cathode_materials, list):
                    citing_all_materials.extend(citing_cathode_materials)
                if isinstance(citing_anode_materials, list):
                    citing_all_materials.extend(citing_anode_materials)
                
                # Get structure families for citing patent
                citing_families = get_structure_families_for_materials(citing_all_materials)
                
                # Skip if no valid structure families found
                if not citing_families:
                    continue
                
                # Calculate citation lag
                citation_lag = classify_citation_lag(citing_priority_year, cited_priority_year)
                
                # Create citations between all combinations of structure families
                for cited_family in cited_families:
                    for citing_family in citing_families:
                        citation_data.append({
                            'cited_patent': cited_patent,
                            'cited_structure_family': cited_family,
                            'cited_family_type': structure_family_types[cited_family],
                            'cited_battery_type': structure_family_battery_types[cited_family],
                            'cited_priority_date': cited_priority_date,
                            'cited_priority_year': cited_priority_year,
                            'citing_patent': citing_patent,
                            'citing_structure_family': citing_family,
                            'citing_family_type': structure_family_types[citing_family],
                            'citing_battery_type': structure_family_battery_types[citing_family],
                            'citing_priority_date': citing_priority_date,
                            'citing_priority_year': citing_priority_year,
                            'citation_lag': citation_lag
                        })
    
    # Create DataFrame from collected data
    citations_df = pd.DataFrame(citation_data)
    
    print(f"Structure family citation dataset created with {len(citations_df)} citations")
    
    # Print summary statistics
    if len(citations_df) > 0:
        print("\nStructure Family Citation Summary:")
        print(f"Unique cited families: {citations_df['cited_structure_family'].nunique()}")
        print(f"Unique citing families: {citations_df['citing_structure_family'].nunique()}")
        print(f"Citation lag distribution:")
        print(citations_df['citation_lag'].value_counts())
    
    return citations_df

def create_structure_family_patent_counts(structure_citations_df):
    """Create patent counts for each structure family."""
    
    print("Creating structure family patent counts...")
    
    # Count unique patents per cited structure family
    cited_counts = structure_citations_df.groupby('cited_structure_family')['cited_patent'].nunique()
    
    # Count unique patents per citing structure family  
    citing_counts = structure_citations_df.groupby('citing_structure_family')['citing_patent'].nunique()
    
    # Combine both counts (a family can be both cited and citing)
    all_families = set(cited_counts.index) | set(citing_counts.index)
    
    family_counts = {}
    for family in all_families:
        cited_count = cited_counts.get(family, 0)
        citing_count = citing_counts.get(family, 0)
        # Use the maximum count (since a patent can both cite and be cited)
        family_counts[family] = max(cited_count, citing_count)
    
    print(f"Structure family patent counts created for {len(family_counts)} families")
    print("Family patent counts:")
    for family, count in sorted(family_counts.items()):
        print(f"  {family}: {count} patents")
    
    return family_counts

#==============================================================
# CREATE STRUCTURE FAMILY CITATION DATASET AND COUNTS
#==============================================================

print("="*80)
print("CREATING STRUCTURE FAMILY CITATION DATASET")
print("="*80)

# Create structure family citation dataset
structure_citations_df = create_structure_family_citation_dataset(combined_df)

# Create structure family patent counts
structure_family_patent_counts = create_structure_family_patent_counts(structure_citations_df)

print(f"\n✅ Structure family citation dataset ready!")
print(f"   - {len(structure_citations_df)} total citations")
print(f"   - {len(structure_family_patent_counts)} structure families")
print(f"   - Ready for heatmap creation")

print("="*80)


def create_material_aggregated_df(combined_df):
    """Create aggregated DataFrame at material level with backward citation counts."""
    material_data = []
    
    for _, row in combined_df.iterrows():
        battery_type = row['battery_type']
        
        # Process cathode materials
        for material in row['Cathode Material']:
            if material != 'none':
                material_data.append({
                    'material': material,
                    'electrode_type': 'cathode',
                    'battery_type': battery_type,
                    'backward_citations': row['family_level_cited_patent_counts']
                })
        
        # Process anode materials  
        for material in row['Anode Material']:
            if material != 'none':
                material_data.append({
                    'material': material,
                    'electrode_type': 'anode',
                    'battery_type': battery_type,
                    'backward_citations': row['family_level_cited_patent_counts']
                })
    
    # Create DataFrame and aggregate by material
    material_df = pd.DataFrame(material_data)
    
    # Sum backward citations for each unique material-battery combination
    material_aggregated = material_df.groupby(['material', 'electrode_type', 'battery_type']).agg({
        'backward_citations': 'sum'
    }).reset_index()
    
    return material_aggregated

def create_structure_family_aggregated_df(combined_df):
    """Create aggregated DataFrame at structure family level with backward citation counts."""
    family_data = []
    
    for _, row in combined_df.iterrows():
        # Get all materials for this patent
        all_materials = []
        
        # Add cathode materials
        for material in row['Cathode Material']:
            if material != 'none':
                all_materials.append(material)
        
        # Add anode materials  
        for material in row['Anode Material']:
            if material != 'none':
                all_materials.append(material)
        
        # Get structure families for these materials
        families = get_structure_families_for_materials(all_materials)
        
        # Add entry for each family
        for family in families:
            family_data.append({
                'structure_family': family,
                'family_type': structure_family_types[family],
                'battery_type': structure_family_battery_types[family],
                'backward_citations': row['family_level_cited_patent_counts']  # CORRECTED COLUMN NAME
            })
    
    # Create DataFrame and aggregate by family
    family_df = pd.DataFrame(family_data)
    
    # Sum backward citations for each unique family
    family_aggregated = family_df.groupby(['structure_family', 'family_type', 'battery_type']).agg({
        'backward_citations': 'sum'
    }).reset_index()
    
    return family_aggregated

def create_material_level_percentage_heatmap(citations_df, material_patent_counts, normalization_type="incoming", scale_type="linear"):
    """
    Create heatmap showing percentage of citation flows between materials.
    CORRECTED: Uses total backward citations as normalization base.
    
    Args:
        citations_df: DataFrame with citation data
        material_patent_counts: Dictionary with patent counts per material (not used in corrected version)
        normalization_type: "incoming" or "outgoing"
        scale_type: "linear" or "logarithmic"
    """
    print(f"Creating CORRECTED material-level {normalization_type} percentage heatmap with {scale_type} scale...")
    
    # Create material aggregated data for total backward citations
    material_aggregated_df = create_material_aggregated_df(combined_df)
    
    # Define material ordering by battery type and electrode type
    def get_comprehensive_material_order():
        """Get ordered list of all materials grouped by battery type and electrode type."""
        
        # LIB materials
        lib_cathode_ordered = ['LCO', 'NMC', 'NCA', 'LFP', 'LMO']
        lib_anode_ordered = ['LTO', 'Silicon/Carbon', 'Graphite']
        
        # SIB materials  
        sib_cathode_ordered = ['NFM', 'CFM', 'NMO', 'NVPF', 'NFPP/NFP', 'Fe-PBA', 'Mn-PBA']
        sib_anode_ordered = ['Hard Carbon']
        
        # Combine in the specified order
        ordered_materials = []
        
        # LIB Cathodes
        for material in lib_cathode_ordered:
            if material in lib_cathode_materials:
                ordered_materials.append((material, 'cathode', 'LIB'))
        
        # LIB Anodes
        for material in lib_anode_ordered:
            if material in lib_anode_materials:
                ordered_materials.append((material, 'anode', 'LIB'))
        
        # SIB Cathodes
        for material in sib_cathode_ordered:
            if material in sib_cathode_materials:
                ordered_materials.append((material, 'cathode', 'SIB'))
        
        # SIB Anodes
        for material in sib_anode_ordered:
            if material in sib_anode_materials:
                ordered_materials.append((material, 'anode', 'SIB'))
        
        return ordered_materials
    
    # Get ordered materials
    ordered_materials = get_comprehensive_material_order()
    material_names = [f"{battery}_{material}" for material, electrode, battery in ordered_materials]
    
    print(f"Creating CORRECTED material-level {normalization_type} percentage heatmap for {len(material_names)} materials")
    
    # Create lookup dictionaries for material indices
    material_to_idx = {name: idx for idx, name in enumerate(material_names)}
    
    # Get total backward citations for each material-battery combination
    total_backward_citations = {}
    for _, row in material_aggregated_df.iterrows():
        key = f"{row['battery_type']}_{row['material']}"
        total_backward_citations[key] = row['backward_citations']
    
    # Group citations by material pairs to accumulate all citations
    citation_pairs = {}
    
    for _, row in citations_df.iterrows():
        cited_key = f"{row['cited_battery_type']}_{row['cited_material']}"
        citing_key = f"{row['citing_battery_type']}_{row['citing_material']}"
        
        # Check if both materials are in our ordered list
        if cited_key in material_to_idx and citing_key in material_to_idx:
            pair_key = (cited_key, citing_key)
            if pair_key not in citation_pairs:
                citation_pairs[pair_key] = 0
            citation_pairs[pair_key] += 1
    
    # Populate raw citation count matrix
    raw_citation_matrix = np.zeros((len(material_names), len(material_names)))
    for (cited_key, citing_key), citation_count in citation_pairs.items():
        cited_idx = material_to_idx[cited_key]
        citing_idx = material_to_idx[citing_key]
        raw_citation_matrix[cited_idx, citing_idx] = citation_count
    
    # Calculate percentage matrix based on normalization type using TOTAL backward citations
    percentage_matrix = np.zeros((len(material_names), len(material_names)))
    
    if normalization_type == "incoming":
        # Each column normalized by total backward citations of that receiver
        for j in range(len(material_names)):
            receiver_key = material_names[j]
            total_backward = total_backward_citations.get(receiver_key, 0)
            
            if total_backward > 0:
                # Normalize by TOTAL backward citations (not just LIB/SIB citations)
                percentage_matrix[:, j] = (raw_citation_matrix[:, j] / total_backward) * 100
        
        title_suffix = "Percentage of total knowledge received"
        interpretation = "Each column shows % of receiver's TOTAL knowledge from each provider (columns do NOT sum to 100%)"
        
    elif normalization_type == "outgoing":
        # Each row normalized by total backward citations of that provider
        for i in range(len(material_names)):
            provider_key = material_names[i]
            total_backward = total_backward_citations.get(provider_key, 0)
            
            if total_backward > 0:
                # For outgoing, we still use the raw citation sum for that provider
                row_sum = np.sum(raw_citation_matrix[i, :])
                if row_sum > 0:
                    percentage_matrix[i, :] = (raw_citation_matrix[i, :] / row_sum) * 100
        
        title_suffix = "Percentage of Outgoing Citations"
        interpretation = "Each row sums to 100% - shows knowledge distribution targets for each provider"
    
    else:
        raise ValueError("normalization_type must be 'incoming' or 'outgoing'")
    
    # Create labels for the heatmap
    heatmap_labels = []
    for material, electrode, battery in ordered_materials:
        heatmap_labels.append(f"{battery}\n{material}")
    
    # Calculate column sums for total knowledge display (only for incoming)
    if normalization_type == "incoming":
        # Column sums show % of total knowledge that comes from LIB/SIB technologies
        column_sums_percentage = np.sum(percentage_matrix, axis=0)
        # Also show absolute backward citations
        column_total_citations = [total_backward_citations.get(material_names[j], 0) for j in range(len(material_names))]
    else:
        column_sums_percentage = None
        column_total_citations = None
    
    # Create the heatmap visualization
    create_single_percentage_heatmap_corrected(
        percentage_matrix,
        heatmap_labels,
        ordered_materials,
        f'Knowledge Flow Heatmap - Material Level (CORRECTED)\n{title_suffix} ({scale_type.capitalize()} Scale)',
        f'Material_Level_{normalization_type.capitalize()}_{scale_type.capitalize()}_Citation_Heatmap_CORRECTED.png',
        f'{title_suffix} (%)',
        normalization_type,
        interpretation,
        scale_type,
        raw_citation_matrix,  # Pass raw matrix for reference
        column_sums_percentage,  # Pass column sums for total knowledge display
        column_total_citations   # Pass total citations for display
    )
    
    # Save matrices as CSV
    percentage_df = pd.DataFrame(percentage_matrix, index=heatmap_labels, columns=heatmap_labels)
    percentage_df.to_csv(f'material_level_{normalization_type}_{scale_type}_citation_matrix_CORRECTED.csv')
    
    raw_citation_df = pd.DataFrame(raw_citation_matrix, index=heatmap_labels, columns=heatmap_labels)
    raw_citation_df.to_csv(f'material_level_raw_citation_matrix_{normalization_type}_{scale_type}_CORRECTED.csv')
    
    print(f"CORRECTED Material-level {normalization_type} percentage heatmap with {scale_type} scale completed and saved.")

def create_structure_family_level_percentage_heatmap(structure_citations_df, structure_family_patent_counts, normalization_type="incoming", scale_type="linear"):
    """
    Create heatmap showing percentage of citation flows between structure families.
    CORRECTED: Uses total backward citations as normalization base.
    
    Args:
        structure_citations_df: DataFrame with structure family citation data
        structure_family_patent_counts: Dictionary with patent counts per family (not used in corrected version)
        normalization_type: "incoming" or "outgoing"
        scale_type: "linear" or "logarithmic"
    """
    print(f"Creating CORRECTED structure family-level {normalization_type} percentage heatmap with {scale_type} scale...")
    
    # Create structure family aggregated data for total backward citations
    family_aggregated_df = create_structure_family_aggregated_df(combined_df)
    
    # Define structure family ordering
    def get_structure_family_order():
        """Get ordered list of structure families in new specified order."""
        
        ordered_families = [
            ('LIB_Layered_Oxides', 'cathode', 'LIB'),
            ('LIB_Polyanionic', 'cathode', 'LIB'),
            ('LIB_Spinel', 'cathode', 'LIB'),
            ('SIB_Layered_Oxides', 'cathode', 'SIB'),
            ('SIB_Polyanionic', 'cathode', 'SIB'),
            ('SIB_PBA', 'cathode', 'SIB'),
            ('LIB_Anode_Graphite_based', 'anode', 'LIB'),
            ('LIB_Anode_LTO', 'anode', 'LIB'),
            ('SIB_Anode_Hard_Carbon', 'anode', 'SIB')
        ]
        
        return ordered_families
    
    # Get ordered structure families
    ordered_families = get_structure_family_order()
    family_names = [family for family, electrode, battery in ordered_families]
    
    print(f"Creating CORRECTED structure family-level {normalization_type} percentage heatmap for {len(family_names)} families")
    
    # Create lookup dictionaries for family indices
    family_to_idx = {name: idx for idx, name in enumerate(family_names)}
    
    # Get total backward citations for each structure family
    total_backward_citations = {}
    for _, row in family_aggregated_df.iterrows():
        family = row['structure_family']
        total_backward_citations[family] = row['backward_citations']
    
    # Group citations by structure family pairs
    citation_pairs = {}
    
    for _, row in structure_citations_df.iterrows():
        cited_family = row['cited_structure_family']
        citing_family = row['citing_structure_family']
        
        if cited_family in family_to_idx and citing_family in family_to_idx:
            pair_key = (cited_family, citing_family)
            if pair_key not in citation_pairs:
                citation_pairs[pair_key] = 0
            citation_pairs[pair_key] += 1
    
    # Populate raw citation count matrix
    raw_citation_matrix = np.zeros((len(family_names), len(family_names)))
    for (cited_family, citing_family), citation_count in citation_pairs.items():
        cited_idx = family_to_idx[cited_family]
        citing_idx = family_to_idx[citing_family]
        raw_citation_matrix[cited_idx, citing_idx] = citation_count
    
    # Calculate percentage matrix using TOTAL backward citations
    percentage_matrix = np.zeros((len(family_names), len(family_names)))
    
    if normalization_type == "incoming":
        # Each column normalized by total backward citations of that receiver
        for j in range(len(family_names)):
            receiver_family = family_names[j]
            total_backward = total_backward_citations.get(receiver_family, 0)
            
            if total_backward > 0:
                # Normalize by TOTAL backward citations (not just LIB/SIB citations)
                percentage_matrix[:, j] = (raw_citation_matrix[:, j] / total_backward) * 100
        
        title_suffix = "Percentage of total knowledge received"
        interpretation = "Each column shows % of receiver's TOTAL knowledge from each provider (columns do NOT sum to 100%)"
        
    elif normalization_type == "outgoing":
        # Each row normalized by sum of that row (traditional outgoing normalization)
        for i in range(len(family_names)):
            row_sum = np.sum(raw_citation_matrix[i, :])
            if row_sum > 0:
                percentage_matrix[i, :] = (raw_citation_matrix[i, :] / row_sum) * 100
        
        title_suffix = "Percentage of Outgoing Citations"
        interpretation = "Each row sums to 100% - shows knowledge distribution targets for each provider"
    
    else:
        raise ValueError("normalization_type must be 'incoming' or 'outgoing'")
    
    # Create labels for the heatmap
    heatmap_labels = []
    for family, electrode, battery in ordered_families:
        if family == 'LIB_Anode_Graphite_based':
            family_short = 'Graphite-based'
        elif family == 'SIB_Anode_Hard_Carbon':
            family_short = 'Hard Carbon'
        else:
            family_short = family.replace('_', ' ').replace('LIB ', '').replace('SIB ', '')
        heatmap_labels.append(f"{battery}\n{family_short}")
    
    # Calculate column sums for total knowledge display (only for incoming)
    if normalization_type == "incoming":
        # Column sums show % of total knowledge that comes from LIB/SIB technologies
        column_sums_percentage = np.sum(percentage_matrix, axis=0)
        # Also show absolute backward citations
        column_total_citations = [total_backward_citations.get(family_names[j], 0) for j in range(len(family_names))]
    else:
        column_sums_percentage = None
        column_total_citations = None
    
    # Create the heatmap visualization
    create_single_percentage_heatmap_corrected(
        percentage_matrix,
        heatmap_labels,
        ordered_families,
        f'Knowledge Flow Heatmap - Structure Family Level\n{title_suffix} ({scale_type.capitalize()} Scale)',
        f'Structure_Family_Level_{normalization_type.capitalize()}_{scale_type.capitalize()}_Citation_Heatmap_CORRECTED.png',
        f'{title_suffix} (%)',
        normalization_type,
        interpretation,
        scale_type,
        raw_citation_matrix,  # Pass raw matrix for reference
        column_sums_percentage,  # Pass column sums for total knowledge display
        column_total_citations   # Pass total citations for display
    )
    
    # Save matrices as CSV
    percentage_df = pd.DataFrame(percentage_matrix, index=heatmap_labels, columns=heatmap_labels)
    percentage_df.to_csv(f'structure_family_level_{normalization_type}_{scale_type}_citation_matrix_CORRECTED.csv')
    
    raw_citation_df = pd.DataFrame(raw_citation_matrix, index=heatmap_labels, columns=heatmap_labels)
    raw_citation_df.to_csv(f'structure_family_level_raw_citation_matrix_{normalization_type}_{scale_type}_CORRECTED.csv')
    
    print(f"CORRECTED Structure family-level {normalization_type} percentage heatmap with {scale_type} scale completed and saved.")
    return percentage_matrix, raw_citation_matrix



def create_single_percentage_heatmap_corrected(matrix, heatmap_labels, ordered_items, title, filename, colorbar_label, normalization_type, interpretation, scale_type="linear", raw_citation_matrix=None, column_sums_percentage=None, column_total_citations=None):
    """Create a single percentage-based heatmap visualization with CORRECTED normalization and updated color scheme."""
    
    # Set up the figure with larger size for better readability
    fig, ax = plt.subplots(figsize=(24, 22))
    
    # Prepare data based on scale type
    if scale_type == "logarithmic":
        # For logarithmic scale, add small epsilon to avoid log(0)
        epsilon = 0.01  # 0.01% minimum value
        log_matrix = np.where(matrix > 0, matrix, epsilon)
        log_matrix = np.log10(log_matrix)
        display_matrix = log_matrix
        
        # Set appropriate range for log scale
        vmin = np.log10(epsilon)  # log10(0.01) = -2
        vmax = np.log10(100)      # log10(100) = 2
        
        # Create custom colormap for log scale
        from matplotlib.colors import LinearSegmentedColormap
        import matplotlib.colors as mcolors
        
        # Use a different colormap that works well for log scale
        colors = ['white', '#FFF5F0', '#FEE0D2', '#FCBBA1', '#FC9272', '#FB6A4A', '#EF3B2C', '#CB181D', '#99000D', '#67000D']
        custom_cmap = mcolors.LinearSegmentedColormap.from_list("log_scale", colors, N=256)
        
        # Set NaN for original zero values
        display_matrix = np.where(matrix > 0, log_matrix, np.nan)
        
    else:  # linear scale with BLUE-YELLOW-RED COLOR SCHEME (0-12%)
        from matplotlib.colors import LinearSegmentedColormap
        import matplotlib.colors as mcolors
        
        # MONOCHROMATISCHER GRADIENT: Weiß (0%) → Dunkelblau (12%)
        colors_monochrome_blue = ['#FFFFFF', '#E6F3FF', '#CCE7FF', '#B3DBFF', '#99CFFF', '#80C3FF', '#66B7FF', '#4DABFF', '#339FFF', '#1A93FF', '#0087FF', '#0066CC', '#004499']
        
        # Create smooth monochromatic colormap from white to dark blue
        custom_cmap = mcolors.LinearSegmentedColormap.from_list(
            "monochrome_blue", 
            colors_monochrome_blue,
            N=256
        )
        
        # Set NaN for zero values, use 0-12% scale
        display_matrix = np.where(matrix > 0, matrix, np.nan)
        vmin = 0
        vmax = 12  # Limit to 12% instead of 100%
    
    # Create heatmap
    im = ax.imshow(display_matrix, cmap=custom_cmap, aspect='equal', interpolation='nearest', vmin=vmin, vmax=vmax)
    
    # Add text annotations with percentage values
    for i in range(len(heatmap_labels)):
        for j in range(len(heatmap_labels)):
            value = matrix[i, j]
            if value > 0:  # Only show text for non-zero values
                # Choose text color based on value and scale type
                if scale_type == "logarithmic":
                    # For log scale, use log value for color decision
                    log_value = np.log10(value)
                    threshold_log = (vmin + vmax) / 2
                    text_color = 'white' if log_value > threshold_log else 'black'
                else:
                    # Black text for values 0-8%, white text for values > 8%
                    text_color = 'white' if value > 8 else 'black'
                
                # Format percentage value
                if value >= 10:
                    text = f'{value:.0f}%'
                elif value >= 1:
                    text = f'{value:.1f}%'
                else:
                    text = f'{value:.2f}%'
                
                base_fontsize = max(8, min(14, int(120 / len(heatmap_labels))))
                ax.text(j, i, text, ha='center', va='center', 
                        color=text_color, fontsize=base_fontsize, fontweight='bold')
    
    # Add CORRECTED total knowledge row (only for incoming normalization) with UPDATED FORMAT
    if normalization_type == "incoming" and column_sums_percentage is not None and column_total_citations is not None:
        # Calculate fontsize to match heatmap values
        base_fontsize = max(8, min(14, int(120 / len(heatmap_labels))))
        
        # Add text annotations for total knowledge above the heatmap
        for j in range(len(heatmap_labels)):
            if column_total_citations[j] > 0:  # Only show for non-zero columns
                # Show absolute total citations and percentage from LIB/SIB (NO DECIMAL, NO "from LIB/SIB")
                lib_sib_percentage = int(round(column_sums_percentage[j]))  # Round to nearest integer
                total_citations = int(column_total_citations[j])
                
                total_text = f'{total_citations}\n({lib_sib_percentage}%)'
                # Position text above the heatmap with REDUCED fontsize
                reduced_fontsize = max(6, int(base_fontsize * 1))  # 70% of base fontsize
                ax.text(j, -1, total_text, 
                       ha='center', va='center', 
                       color='darkblue', fontsize=reduced_fontsize, fontweight='bold',
                       bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.7))
        
        # Add label for the total knowledge row (MOVED FURTHER LEFT)
        ax.text(-1.8, -1, 'Total knowledge flows\n(patent citations)\nto knowledge receiver\n(% share from LIB & SIB):', 
               ha='center', va='center', 
               color='darkblue', fontsize=reduced_fontsize, fontweight='bold',
               bbox=dict(boxstyle="round,pad=0.3", facecolor="lightcyan", alpha=0.8))
    
    # Set the background color for NaN values to light gray
    ax.set_facecolor('lightgray')
    
    # Set ticks and labels
    ax.set_xticks(range(len(heatmap_labels)))
    ax.set_yticks(range(len(heatmap_labels)))
    ax.set_xticklabels(heatmap_labels, rotation=45, ha='right', fontsize=12)
    ax.set_yticklabels(heatmap_labels, fontsize=12)
    
    # Add labels with increased padding
    ax.set_xlabel('Knowledge receivers', fontsize=24, fontweight='bold', labelpad=60)
    ax.set_ylabel('Knowledge providers', fontsize=24, fontweight='bold', labelpad=60)
    
    # Add horizontal colorbar at bottom with 0-12% scale (positioned below x-axis label)
    cbar = plt.colorbar(im, ax=ax, orientation='horizontal', shrink=0.6, pad=0.18, aspect=30)
    cbar.set_label(f'{colorbar_label}', fontsize=16, fontweight='bold', labelpad=15)
    
    if scale_type == "logarithmic":
        # Set logarithmic colorbar ticks
        log_ticks = [-2, -1, 0, 1, 2]  # log10(0.01), log10(0.1), log10(1), log10(10), log10(100)
        tick_labels = ['0.01%', '0.1%', '1%', '10%', '100%']
        cbar.set_ticks(log_ticks)
        cbar.set_ticklabels(tick_labels, fontsize=12)
        
        # Add annotation for log scale
        scale_info = 'Log₁₀ scale: Better visualization of wide value ranges'
    else:
        # Set 0-12% colorbar ticks with blue-yellow-red scheme
        cbar.set_ticks([0, 2, 4, 6, 8, 10, 12])
        cbar.set_ticklabels(['0%', '2%', '4%', '6%', '8%', '10%', '12%'], fontsize=16)
        
        # Add annotation for monochromatic blue scale
        scale_info = 'Linear scale 0-12%: White (low) → Dark Blue (high)'
    
    # Add grid lines - FIXED positions for new order with THREE grid lines
    grid_positions = []
    
    # Add grid lines at specified positions:
    # Between LIB Spinel and SIB Layered Oxides (after position 2)
    grid_positions.append(2.5)
    # Between SIB PBA and LIB Graphite-based (after position 5) 
    grid_positions.append(5.5)
    # Between LIB LTO and SIB Hard Carbon (after position 7)
    grid_positions.append(7.5)
    
    # Draw grid lines
    for pos in grid_positions:
        ax.axhline(y=pos, color='black', linewidth=2.5, alpha=0.8)
        ax.axvline(x=pos, color='black', linewidth=2.5, alpha=0.8)
    
    # Add section labels with NEW order - FIXED positions
    # LIB Cathodes: positions 0,1,2 (LIB Layered Oxides, LIB Polyanionic, LIB Spinel)
    lib_cathode_positions = [0, 1, 2]
    # SIB Cathodes: positions 3,4,5 (SIB Layered Oxides, SIB Polyanionic, SIB PBA)  
    sib_cathode_positions = [3, 4, 5]
    # LIB Anodes: positions 6,7 (LIB Graphite-based, LIB LTO)
    lib_anode_positions = [6, 7]
    # SIB Anodes: position 8 (SIB Hard Carbon)
    sib_anode_positions = [8]
    
    def add_section_label_with_spacing(positions, label):
        if positions:
            center_pos = (min(positions) + max(positions)) / 2
            
            # SPEZIELLE ANPASSUNG für überlappende Anode-Labels
            if 'LIB' in label and 'negative' in label:
                # LIB Negative Electrode etwas nach links verschieben
                adjusted_center_pos = center_pos - 0.2
            elif 'SIB' in label and 'negative' in label:
                # SIB Negative Electrode etwas nach rechts verschieben  
                adjusted_center_pos = center_pos + 0.1
            else:
                adjusted_center_pos = center_pos
            
            # Top labels - closer to x-axis labels
            top_position = len(heatmap_labels) + 1.2 if normalization_type == "incoming" and column_sums_percentage is not None else len(heatmap_labels) + 1.0
            ax.text(adjusted_center_pos, top_position, label, ha='center', va='bottom', 
                    fontsize=12, fontweight='bold', rotation=0,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgrey", alpha=0.7))
            
            # Left labels - closer to y-axis labels with same adjustment
            ax.text(-2.2, adjusted_center_pos, label, ha='center', va='center', 
                    fontsize=12, fontweight='bold', rotation=90,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgrey", alpha=0.7))
    
    # Add section labels with SWAPPED positions for LIB/SIB electrodes
    add_section_label_with_spacing(lib_cathode_positions, 'LIB\npositive electrode')
    add_section_label_with_spacing(sib_cathode_positions, 'SIB\npositive electrode')  
    add_section_label_with_spacing(lib_anode_positions, 'LIB\nnegative electrode')
    add_section_label_with_spacing(sib_anode_positions, 'SIB\nnegative electrode')
    
    # Add coordinate labels (A1, A2, B1, etc.) in top-right corner of each cell
    for i in range(len(heatmap_labels)):
        for j in range(len(heatmap_labels)):
            # Generate coordinate label: row letter + column number
            row_letter = chr(65 + i)  # A=65, B=66, C=67, etc.
            col_number = j + 1
            coordinate_label = f"{row_letter}{col_number}"
            
            # Position in top-right corner of cell
            # Offset by 0.45 units from center towards top-right
            ax.text(j + 0.45, i - 0.45, coordinate_label, 
                   ha='right', va='top', 
                   color='black', fontsize=12, fontweight='bold',
                   bbox=dict(boxstyle="round,pad=0.1", facecolor="white", alpha=0.8, edgecolor='none'))
    
    # Set title
    # plt.title(title, fontsize=16, fontweight='bold', pad=100)
    
    # Adjust layout to accommodate new elements and horizontal colorbar
    plt.tight_layout()
    if normalization_type == "incoming" and column_sums_percentage is not None:
        plt.subplots_adjust(left=0.20, bottom=0.20, top=0.85, right=0.95)  # Optimized space for larger heatmap
    else:
        plt.subplots_adjust(left=0.20, bottom=0.20, top=0.90, right=0.95)  # Optimized space for larger heatmap
    
    # Save the figure
    plt.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white', transparent=True)
    print(f"CORRECTED Heatmap ({scale_type} scale) saved to {filename}")
    
    # Close figure to free memory
    plt.close(fig)


def print_percentage_heatmap_summary_statistics(material_percentage_matrix, structure_percentage_matrix, 
                                                material_raw_matrix, structure_raw_matrix, normalization_type, scale_type):
    """Print comprehensive summary statistics for both percentage heatmaps."""
    
    print("\n" + "="*80)
    print(f"PERCENTAGE HEATMAP SUMMARY STATISTICS ({scale_type.upper()} SCALE)")
    print("="*80)
    
    print(f"\n📊 MATERIAL LEVEL PERCENTAGE HEATMAP:")
    print(f"   Non-zero entries: {np.count_nonzero(material_percentage_matrix):4d}")
    print(f"   Total possible connections: {material_percentage_matrix.size:4d}")
    print(f"   Connection density: {np.count_nonzero(material_percentage_matrix)/material_percentage_matrix.size*100:6.2f}%")
    print(f"   Maximum percentage: {np.max(material_percentage_matrix):6.2f}%")
    print(f"   Mean percentage (non-zero only): {np.mean(material_percentage_matrix[material_percentage_matrix > 0]):6.2f}%")
    print(f"   Median percentage (non-zero only): {np.median(material_percentage_matrix[material_percentage_matrix > 0]):6.2f}%")
    
    # Verify column sums equal 100% (for incoming) or row sums equal 100% (for outgoing)
    if normalization_type == "incoming":
        col_sums = np.sum(material_percentage_matrix, axis=0)
        non_zero_cols = col_sums > 0
        if np.any(non_zero_cols):
            avg_col_sum = np.mean(col_sums[non_zero_cols])
            print(f"   Average column sum (should be ~100%): {avg_col_sum:.2f}%")
            print(f"   Number of receivers with knowledge sources: {np.sum(non_zero_cols)}")
    else:  # outgoing
        row_sums = np.sum(material_percentage_matrix, axis=1)
        non_zero_rows = row_sums > 0
        if np.any(non_zero_rows):
            avg_row_sum = np.mean(row_sums[non_zero_rows])
            print(f"   Average row sum (should be ~100%): {avg_row_sum:.2f}%")
            print(f"   Number of providers with knowledge targets: {np.sum(non_zero_rows)}")
    
    print(f"\n📊 STRUCTURE FAMILY LEVEL PERCENTAGE HEATMAP:")
    print(f"   Non-zero entries: {np.count_nonzero(structure_percentage_matrix):4d}")
    print(f"   Total possible connections: {structure_percentage_matrix.size:4d}")
    print(f"   Connection density: {np.count_nonzero(structure_percentage_matrix)/structure_percentage_matrix.size*100:6.2f}%")
    print(f"   Maximum percentage: {np.max(structure_percentage_matrix):6.2f}%")
    print(f"   Mean percentage (non-zero only): {np.mean(structure_percentage_matrix[structure_percentage_matrix > 0]):6.2f}%")
    print(f"   Median percentage (non-zero only): {np.median(structure_percentage_matrix[structure_percentage_matrix > 0]):6.2f}%")
    
    # Verify sums equal 100% based on normalization type
    if normalization_type == "incoming":
        col_sums = np.sum(structure_percentage_matrix, axis=0)
        non_zero_cols = col_sums > 0
        if np.any(non_zero_cols):
            avg_col_sum = np.mean(col_sums[non_zero_cols])
            print(f"   Average column sum (should be ~100%): {avg_col_sum:.2f}%")
            print(f"   Number of receivers with knowledge sources: {np.sum(non_zero_cols)}")
    else:  # outgoing
        row_sums = np.sum(structure_percentage_matrix, axis=1)
        non_zero_rows = row_sums > 0
        if np.any(non_zero_rows):
            avg_row_sum = np.mean(row_sums[non_zero_rows])
            print(f"   Average row sum (should be ~100%): {avg_row_sum:.2f}%")
            print(f"   Number of providers with knowledge targets: {np.sum(non_zero_rows)}")
    
    print(f"\n📈 KNOWLEDGE FLOW INSIGHTS ({scale_type.upper()} SCALE):")
    
    # Identify most self-reliant materials/families (high diagonal values)
    if normalization_type == "incoming":
        print(f"\nMost self-reliant receivers (high internal knowledge percentage):")
        material_diagonal = np.diag(material_percentage_matrix)
        top_self_reliant_materials = np.argsort(material_diagonal)[-5:][::-1]
        for idx in top_self_reliant_materials:
            if material_diagonal[idx] > 0:
                print(f"   Material receiver {idx}: {material_diagonal[idx]:.1f}% self-sourced knowledge")
        
        print(f"\nMost self-reliant structure family receivers (high internal knowledge percentage):")
        structure_diagonal = np.diag(structure_percentage_matrix)
        top_self_reliant_families = np.argsort(structure_diagonal)[-5:][::-1]
        for idx in top_self_reliant_families:
            if structure_diagonal[idx] > 0:
                print(f"   Family receiver {idx}: {structure_diagonal[idx]:.1f}% self-sourced knowledge")
    
    else:  # outgoing
        print(f"\nMost self-focused providers (high internal knowledge distribution):")
        material_diagonal = np.diag(material_percentage_matrix)
        top_self_focused_materials = np.argsort(material_diagonal)[-5:][::-1]
        for idx in top_self_focused_materials:
            if material_diagonal[idx] > 0:
                print(f"   Material provider {idx}: {material_diagonal[idx]:.1f}% knowledge to self")
        
        print(f"\nMost self-focused structure family providers (high internal knowledge distribution):")
        structure_diagonal = np.diag(structure_percentage_matrix)
        top_self_focused_families = np.argsort(structure_diagonal)[-5:][::-1]
        for idx in top_self_focused_families:
            if structure_diagonal[idx] > 0:
                print(f"   Family provider {idx}: {structure_diagonal[idx]:.1f}% knowledge to self")
    
    # Additional statistics for both normalization types
    print(f"\n📊 CROSS-FLOW ANALYSIS:")
    
    # Identify strongest cross-technology flows
    print(f"\nStrongest cross-technology knowledge flows:")
    
    # For material level
    max_off_diagonal_value = 0
    max_off_diagonal_pos = None
    for i in range(material_percentage_matrix.shape[0]):
        for j in range(material_percentage_matrix.shape[1]):
            if i != j and material_percentage_matrix[i, j] > max_off_diagonal_value:
                max_off_diagonal_value = material_percentage_matrix[i, j]
                max_off_diagonal_pos = (i, j)
    
    if max_off_diagonal_pos:
        i, j = max_off_diagonal_pos
        if normalization_type == "incoming":
            print(f"   Material: Provider {i} → Receiver {j}: {max_off_diagonal_value:.1f}% of receiver's knowledge")
        else:
            print(f"   Material: Provider {i} → Receiver {j}: {max_off_diagonal_value:.1f}% of provider's knowledge")
    
    # For structure family level
    max_off_diagonal_value = 0
    max_off_diagonal_pos = None
    for i in range(structure_percentage_matrix.shape[0]):
        for j in range(structure_percentage_matrix.shape[1]):
            if i != j and structure_percentage_matrix[i, j] > max_off_diagonal_value:
                max_off_diagonal_value = structure_percentage_matrix[i, j]
                max_off_diagonal_pos = (i, j)
    
    if max_off_diagonal_pos:
        i, j = max_off_diagonal_pos
        if normalization_type == "incoming":
            print(f"   Family: Provider {i} → Receiver {j}: {max_off_diagonal_value:.1f}% of receiver's knowledge")
        else:
            print(f"   Family: Provider {i} → Receiver {j}: {max_off_diagonal_value:.1f}% of provider's knowledge")
    
    print(f"\n🎯 NORMALIZATION VERIFICATION:")
    print(f"   Analysis type: {normalization_type.upper()} citations")
    print(f"   Scale type: {scale_type.upper()}")
    if normalization_type == "incoming":
        print(f"   ✓ Each column should sum to 100% (represents complete knowledge sources for each receiver)")
    else:
        print(f"   ✓ Each row should sum to 100% (represents complete knowledge targets for each provider)")
    
    print("="*80)

def print_percentage_interpretation_guide_corrected(normalization_type):
    """Print comprehensive interpretation guide for CORRECTED percentage-based citation heatmaps."""
    
    print("\n" + "="*80)
    print(f"CORRECTED INTERPRETATION GUIDE: {normalization_type.upper()} PERCENTAGE CITATION HEATMAPS")
    print("="*80)

    if normalization_type == "incoming":
        print("""
🎯 CORRECTED PERCENTAGE OF INCOMING CITATIONS:
    Formula: Citations(Provider→Receiver) / TOTAL_Backward_Citations(Receiver) × 100%
   
    Where:
    • Provider = Cited Material/Family (gives knowledge)
    • Receiver = Citing Material/Family (receives knowledge)
    • TOTAL_Backward_Citations = ALL citations received from ALL technologies (not just LIB/SIB)
    • Citation Direction: Receiver cites Provider (B → A)
    • Knowledge Direction: Provider → Receiver (A → B)
   
    🔄 KEY DIFFERENCE FROM ORIGINAL:
    • OLD: Normalized by LIB/SIB citations only → columns summed to 100%
    • NEW: Normalized by ALL backward citations → columns do NOT sum to 100%
    • Column sum = % of receiver's total knowledge that comes from LIB/SIB technologies

📊 CORRECTED HEATMAP READING GUIDE:

AXES INTERPRETATION:
• Y-axis (Rows): Knowledge providers (cited materials/patent families)
• X-axis (Columns): Knowledge receivers (citing materials/patent families)  
• Cell value: Percentage of receiver's TOTAL knowledge (from ALL technologies) from this specific provider
• Total Knowledge Row: Shows absolute backward citations & % from LIB/SIB technologies

COLUMN INTERPRETATION:
• Each column = Knowledge receiver (citing material/family)
• Column shows: "What % of receiver's TOTAL knowledge comes from each LIB/SIB provider?"
• Columns do NOT sum to 100% (remaining % comes from other technologies)
• Column sum = Total % of knowledge from LIB/SIB technologies combined
• High values = Important LIB/SIB knowledge dependency
• Low column sums = Receiver relies heavily on non-LIB/SIB technologies

🎯 STRATEGIC QUESTIONS ANSWERED:
• What % of receiver's TOTAL knowledge comes from specific LIB/SIB providers?
• How dependent is each receiver on LIB/SIB knowledge vs. other technologies?
• Which LIB/SIB providers have the strongest influence on each receiver?
• What is the total knowledge volume and LIB/SIB share for each receiver?
• Which receivers are most vs. least integrated into LIB/SIB knowledge networks?

💡 BUSINESS INTERPRETATION:
• High cell values (>5%): Strong technological dependency on specific LIB/SIB knowledge
• High column sums (>50%): Receiver heavily embedded in LIB/SIB ecosystem
• Low column sums (<20%): Receiver draws mainly from other technology domains
• Pattern analysis: Identifies cross-technology learning and knowledge isolation
""")
    
    else:  # outgoing
        print("""
🎯 PERCENTAGE OF OUTGOING CITATIONS (unchanged logic):
    Formula: Citations(Provider→Receiver) / Total_Outgoing_Citations(Provider) × 100%
   
    This normalization remains unchanged - still shows relative distribution of 
    knowledge targets for each provider material/family.

📊 HEATMAP READING GUIDE:

AXES INTERPRETATION:
• Y-axis (Rows): Knowledge providers (cited materials/patent families)
• X-axis (Columns): Knowledge receivers (citing materials/patent families)  
• Cell value: Percentage of provider's total outgoing knowledge to this specific receiver

ROW INTERPRETATION:
• Each row = Knowledge provider (cited material/family)
• Row shows: "Where does this provider send its knowledge?" (distribution across receivers)
• Each row sums to 100% = All outgoing knowledge targets for that provider
• High values in a row = Important knowledge receivers for that provider
• Diagonal values = Self-reinforcing knowledge percentage
• Off-diagonal values = External knowledge distribution percentages

🎯 STRATEGIC QUESTIONS ANSWERED:
• What % of provider's knowledge goes within chemistry vs. to other chemistries?
• Which receivers are most important targets for each provider's knowledge?
• How focused vs. broadly-distributed are different providers' knowledge outputs?
• What are the knowledge influence patterns in the innovation network?
""")

    print("""
📈 CORRECTED PERCENTAGE VALUE RANGES:

FOR INCOMING NORMALIZATION (corrected):
HIGH PERCENTAGES (>5%):
    ✓ Major dependency on specific LIB/SIB knowledge source
    ✓ Core technological relationship
    ✓ High integration into LIB/SIB innovation ecosystem

MODERATE PERCENTAGES (1-5%):
    ✓ Significant but not dominant LIB/SIB knowledge relationship
    ✓ Secondary technological connection within battery ecosystem

LOW PERCENTAGES (<1%):
    ✓ Minor LIB/SIB knowledge relationship
    ✓ Receiver primarily builds on non-battery technologies
    ✓ Emerging or weak connection to battery innovation network

COLUMN SUM INTERPRETATION:
HIGH SUM (>50%): Receiver heavily embedded in LIB/SIB knowledge ecosystem
MODERATE SUM (20-50%): Balanced between battery and other technologies  
LOW SUM (<20%): Primarily builds on non-battery technological foundations

🔍 STRATEGIC PATTERNS:

DIAGONAL DOMINANCE:
• High self-referencing within LIB/SIB ecosystem
• Strong internal knowledge development within battery technologies
• Mature, established battery technology domains

CROSS-TECHNOLOGY PATTERNS:
• LIB→SIB flows: Knowledge transfer from mature to emerging battery tech
• Low percentages with diverse sources: Interdisciplinary innovation
• High percentages concentrated: Core battery technology dependencies

ECOSYSTEM INTEGRATION:
• High column sums: Technologies central to battery innovation ecosystem
• Low column sums: Technologies on periphery or bridging to other domains
• Mixed patterns: Technologies at intersection of multiple innovation systems

💡 BUSINESS APPLICATIONS:

TECHNOLOGY STRATEGY:
• Identify critical LIB/SIB knowledge dependencies and their intensity
• Assess integration level into battery innovation ecosystem
• Plan technology development based on ecosystem positioning

COMPETITIVE POSITIONING:
• Monitor competitors' integration into LIB/SIB knowledge networks
• Identify opportunities in under-served knowledge spaces
• Leverage strong ecosystem relationships for competitive advantage

INNOVATION STRATEGY:
• Target high-percentage relationships for deeper collaboration
• Explore low-percentage areas for breakthrough innovation opportunities
• Balance ecosystem integration with external knowledge diversity

This corrected analysis reveals the TRUE relative importance of LIB/SIB knowledge
in the broader technological landscape for each receiver.
""")

    print("="*80)

def print_scale_comparison_guide():
    """Print guide explaining when to use linear vs logarithmic scale."""
    
    print("\n" + "="*80)
    print("SCALE TYPE SELECTION GUIDE")
    print("="*80)
    
    print("""
🔢 LINEAR SCALE (Default):
    ✓ Best for: Moderate value ranges (0-100%)
    ✓ Advantages: Intuitive interpretation, preserves absolute differences
    ✓ Use when: Most values are in similar ranges (e.g., 1-50%)
    ✓ Color scheme: Nuanced 0-50%, dark red 50-100%

📊 LOGARITHMIC SCALE (Log₁₀):
    ✓ Best for: Wide value ranges with many small values
    ✓ Advantages: Better visualization of small percentages, compresses high values
    ✓ Use when: Values span multiple orders of magnitude (0.01% to 100%)
    ✓ Color scheme: Continuous gradient across log range
   
🎯 DECISION CRITERIA:

Use LINEAR scale when:
• Most values are between 1-100%
• You want to emphasize absolute differences
• Audience prefers intuitive percentage interpretation
• Values are reasonably distributed

Use LOGARITHMIC scale when:
• Many values are <1% but some are >10%
• You have very sparse matrices with few high values
• You want to better visualize small but meaningful relationships
• Wide value distribution makes linear scale hard to read

⚖️ TRADE-OFFS:

Linear Scale:
+ Intuitive percentage interpretation
+ Clear absolute relationships
- May obscure small values
- Poor for wide value ranges

Logarithmic Scale:
+ Better visualization of small values
+ Handles wide ranges well
+ Reveals hidden patterns
- Less intuitive interpretation
- May overemphasize tiny values

💡 RECOMMENDATION:
Start with LINEAR scale for initial analysis. Switch to LOGARITHMIC if:
- Many cells have values <1%
- High values dominate the visualization
- You need to identify weak but existing relationships
""")

#==============================================================
# Main execution with both normalization and scale options
#==============================================================

print(f"Creating {NORMALIZATION_TYPE} Percentage Citation heatmaps with {SCALE_TYPE} scale...")
print("This will generate heatmaps showing relative knowledge flow distributions.")

# Print scale selection guide
print_scale_comparison_guide()

# Check if required variables exist
required_vars = {
    'citations_df': 'citations_df' in locals(),
    'material_patent_counts': 'material_patent_counts' in locals(),
    'structure_citations_df': 'structure_citations_df' in locals(),
    'structure_family_patent_counts': 'structure_family_patent_counts' in locals()
}

print("\n🔍 Checking required variables:")
for var_name, exists in required_vars.items():
    status = "✅ Found" if exists else "❌ Missing"
    print(f"   {var_name}: {status}")

# Create material-level percentage heatmap (if data available)
if required_vars['citations_df'] and required_vars['material_patent_counts']:
    print("\n📊 Creating material-level percentage heatmap...")
    material_percentage_matrix, material_raw_matrix = create_material_level_percentage_heatmap(
        citations_df, material_patent_counts, NORMALIZATION_TYPE, SCALE_TYPE)
else:
    print("\n❌ Skipping material-level heatmap - missing required data")
    material_percentage_matrix = None
    material_raw_matrix = None

# Create structure family-level percentage heatmap (if data available)
if required_vars['structure_citations_df'] and required_vars['structure_family_patent_counts']:
    print("\n📊 Creating structure family-level percentage heatmap...")
    structure_percentage_matrix, structure_raw_matrix = create_structure_family_level_percentage_heatmap(
        structure_citations_df, structure_family_patent_counts, NORMALIZATION_TYPE, SCALE_TYPE)
else:
    print("\n❌ Skipping structure family-level heatmap - missing required data")
    structure_percentage_matrix = None
    structure_raw_matrix = None

# Print summary statistics (only if both datasets available)
if (material_percentage_matrix is not None and structure_percentage_matrix is not None):
    print_percentage_heatmap_summary_statistics(
        material_percentage_matrix, structure_percentage_matrix,
        material_raw_matrix, structure_raw_matrix, NORMALIZATION_TYPE, SCALE_TYPE)
elif material_percentage_matrix is not None:
    print("\n📈 Material-level summary available, but structure family data missing")
elif structure_percentage_matrix is not None:
    print("\n📈 Structure family-level summary available, but material data missing")
else:
    print("\n❌ No heatmaps created - missing required data")

# Print interpretation guide
if material_percentage_matrix is not None or structure_percentage_matrix is not None:
    print_percentage_interpretation_guide_corrected(NORMALIZATION_TYPE)

# Final status report
print(f"\n🎉 Heatmap creation process completed!")
print("\n📁 Generated files:")

if material_percentage_matrix is not None:
    print(f"✅ Material_Level_{NORMALIZATION_TYPE.capitalize()}_{SCALE_TYPE.capitalize()}_Citation_Heatmap.png")
    print(f"✅ material_level_{NORMALIZATION_TYPE}_{SCALE_TYPE}_citation_matrix.csv")

if structure_percentage_matrix is not None:
    print(f"✅ Structure_Family_Level_{NORMALIZATION_TYPE.capitalize()}_{SCALE_TYPE.capitalize()}_Citation_Heatmap.png")
    print(f"✅ structure_family_level_{NORMALIZATION_TYPE}_{SCALE_TYPE}_citation_matrix.csv")

print(f"\n📝 CURRENT CONFIGURATION:")
print(f"   NORMALIZATION_TYPE = '{NORMALIZATION_TYPE}'  # 'incoming' or 'outgoing'")
print(f"   SCALE_TYPE = '{SCALE_TYPE}'  # 'linear' or 'logarithmic'")

print("="*80)


#%%

#==============================================================
# DIAGONAL-BENCHMARKED HEATMAP EXTENSION
# Benchmarks all column values against diagonal (self-citation) values
# Creates new heatmaps showing relative strength vs self-referencing
#==============================================================

def create_diagonal_benchmarked_matrix(percentage_matrix, matrix_labels):
    """
    Create a matrix where each column value is benchmarked against the diagonal value of that column.
    
    Args:
        percentage_matrix: Original percentage matrix (N x N)
        matrix_labels: Labels for rows/columns
    
    Returns:
        benchmarked_matrix: Matrix where each value = original_value / diagonal_value * 100
        diagonal_values: The diagonal values used as benchmarks
        benchmark_stats: Statistics about the benchmarking process
    """
    print(f"Creating diagonal-benchmarked matrix for {percentage_matrix.shape[0]}x{percentage_matrix.shape[1]} matrix...")
    
    # Initialize benchmarked matrix
    benchmarked_matrix = np.zeros_like(percentage_matrix)
    
    # Extract diagonal values (self-citations) as benchmarks
    diagonal_values = np.diag(percentage_matrix)
    
    # Statistics tracking
    benchmark_stats = {
        'columns_with_diagonal': 0,
        'columns_without_diagonal': 0,
        'total_benchmarked_values': 0,
        'max_benchmark_ratio': 0,
        'min_benchmark_ratio': float('inf'),
        'diagonal_zero_columns': [],
        'highest_ratios': [],
        'benchmark_values': diagonal_values.copy()
    }
    
    # Benchmark each column against its diagonal value
    for col in range(percentage_matrix.shape[1]):
        diagonal_benchmark = diagonal_values[col]
        
        if diagonal_benchmark > 0:  # Only benchmark if diagonal value exists
            benchmark_stats['columns_with_diagonal'] += 1
            
            # Calculate benchmark ratios for this column
            for row in range(percentage_matrix.shape[0]):
                original_value = percentage_matrix[row, col]
                
                if original_value > 0:  # Only calculate for non-zero values
                    benchmark_ratio = (original_value / diagonal_benchmark) * 100
                    benchmarked_matrix[row, col] = benchmark_ratio
                    
                    # Update statistics
                    benchmark_stats['total_benchmarked_values'] += 1
                    benchmark_stats['max_benchmark_ratio'] = max(benchmark_stats['max_benchmark_ratio'], benchmark_ratio)
                    if benchmark_ratio < benchmark_stats['min_benchmark_ratio']:
                        benchmark_stats['min_benchmark_ratio'] = benchmark_ratio
                    
                    # Track highest ratios for analysis
                    if row != col:  # Exclude diagonal (which is always 100%)
                        benchmark_stats['highest_ratios'].append({
                            'provider': matrix_labels[row],
                            'receiver': matrix_labels[col], 
                            'ratio': benchmark_ratio,
                            'original_value': original_value,
                            'diagonal_benchmark': diagonal_benchmark
                        })
        else:
            # Column has no diagonal value to benchmark against
            benchmark_stats['columns_without_diagonal'] += 1
            benchmark_stats['diagonal_zero_columns'].append({
                'index': col,
                'label': matrix_labels[col]
            })
            # Keep original values when no diagonal benchmark exists
            benchmarked_matrix[:, col] = percentage_matrix[:, col]
    
    # Sort highest ratios for analysis
    benchmark_stats['highest_ratios'].sort(key=lambda x: x['ratio'], reverse=True)
    benchmark_stats['highest_ratios'] = benchmark_stats['highest_ratios'][:10]  # Keep top 10
    
    # Fix infinite min value if no benchmarking occurred
    if benchmark_stats['min_benchmark_ratio'] == float('inf'):
        benchmark_stats['min_benchmark_ratio'] = 0
    
    print(f"✅ Diagonal benchmarking completed:")
    print(f"   - Columns with diagonal benchmarks: {benchmark_stats['columns_with_diagonal']}")
    print(f"   - Columns without diagonal benchmarks: {benchmark_stats['columns_without_diagonal']}")
    print(f"   - Total benchmarked values: {benchmark_stats['total_benchmarked_values']}")
    print(f"   - Benchmark ratio range: {benchmark_stats['min_benchmark_ratio']:.1f}% - {benchmark_stats['max_benchmark_ratio']:.1f}%")
    
    return benchmarked_matrix, diagonal_values, benchmark_stats

def create_diagonal_benchmarked_heatmap(benchmarked_matrix, heatmap_labels, ordered_items, 
                                       diagonal_values, benchmark_stats, level_name, 
                                       normalization_type, scale_type="linear"):
    """
    Create heatmap visualization for diagonal-benchmarked citation matrix.
    UPDATED: Matches the layout from the first heatmap with horizontal colorbar 0-100%.
    """
    
    # Set up the figure - same size as original
    fig, ax = plt.subplots(figsize=(24, 22))
    
    # Prepare data based on scale type
    if scale_type == "logarithmic":
        # For logarithmic scale, add small epsilon to avoid log(0)
        epsilon = 0.1  # 0.1% minimum value for benchmark ratios
        log_matrix = np.where(benchmarked_matrix > 0, benchmarked_matrix, epsilon)
        log_matrix = np.log10(log_matrix)
        display_matrix = log_matrix
        
        # Set appropriate range for log scale (0.1% to 1000%)
        vmin = np.log10(epsilon)  # log10(0.1) = -1
        vmax = np.log10(1000)     # log10(1000) = 3
        
        # Create colormap for benchmark ratios
        from matplotlib.colors import LinearSegmentedColormap
        import matplotlib.colors as mcolors
        
        # Use blue-white-red colormap for benchmark ratios
        colors = ['#053061', '#2166AC', '#4393C3', '#92C5DE', '#D1E5F0', 
                 'white', '#FDBF6F', '#FF7F00', '#E31A1C', '#B10026', '#67000D']
        custom_cmap = mcolors.LinearSegmentedColormap.from_list("benchmark_log", colors, N=256)
        
        # Set NaN for original zero values
        display_matrix = np.where(benchmarked_matrix > 0, log_matrix, np.nan)
        
    else:  # linear scale - UPDATED to match original layout with 0-100% range
        from matplotlib.colors import LinearSegmentedColormap
        import matplotlib.colors as mcolors
        
        # MONOCHROMATISCHER GRADIENT: Weiß (0%) → Dunkelblau (100%)
        colors_monochrome_blue = ['#FFFFFF', '#E6F3FF', '#CCE7FF', '#B3DBFF', '#99CFFF', '#80C3FF', '#66B7FF', '#4DABFF', '#339FFF', '#1A93FF', '#0087FF', '#0066CC', '#004499']
        
        # Create smooth monochromatic colormap from white to dark blue
        custom_cmap = mcolors.LinearSegmentedColormap.from_list(
            "monochrome_blue", 
            colors_monochrome_blue,
            N=256
        )
        
        # Set range for display - LIMITED TO 0-100%
        vmin = 0
        vmax = 100  # Changed from 120 to 100
        
        # Set NaN for zero values, cap values at 100% for display
        display_matrix = np.where(benchmarked_matrix > 0, 
                                np.minimum(benchmarked_matrix, 100), 
                                np.nan)
    
    # Create heatmap
    im = ax.imshow(display_matrix, cmap=custom_cmap, aspect='equal', 
                   interpolation='nearest', vmin=vmin, vmax=vmax)
    
    # Add text annotations with benchmark percentage values
    for i in range(len(heatmap_labels)):
        for j in range(len(heatmap_labels)):
            value = benchmarked_matrix[i, j]
            
            if value > 0:  # Only show text for non-zero values
                # Choose text color based on value - UPDATED logic
                if scale_type == "logarithmic":
                    log_value = np.log10(value)
                    threshold_log = (vmin + vmax) / 2
                    text_color = 'white' if log_value > threshold_log else 'black'
                else:
                    # Black text for values 0-80%, white text for values > 80%
                    text_color = 'white' if value > 80 else 'black'
                
                # Format benchmark percentage value
                if i == j:  # Diagonal values are always 100%
                    text = '100%'
                elif value >= 100:
                    text = f'{value:.0f}%'
                elif value >= 10:
                    text = f'{value:.1f}%'
                else:
                    text = f'{value:.1f}%'
                
                base_fontsize = max(8, min(14, int(120 / len(heatmap_labels))))
                ax.text(j, i, text, ha='center', va='center', 
                        color=text_color, fontsize=base_fontsize, fontweight='bold')
    
    # Add diagonal benchmark values above the heatmap - MATCHING ORIGINAL LAYOUT
    base_fontsize = max(8, min(14, int(120 / len(heatmap_labels))))
    
    for j in range(len(heatmap_labels)):
        if diagonal_values[j] > 0:  # Only show for non-zero diagonal values
            # Show only the percentage value with reduced font size (matching original)
            diagonal_text = f'{diagonal_values[j]:.1f}%'
            reduced_fontsize = max(6, int(base_fontsize * 1))  # Same as original
            
            # Position text above the heatmap (matching original position)
            ax.text(j, -1, diagonal_text, 
                   ha='center', va='center', 
                   color='darkgreen', fontsize=reduced_fontsize, fontweight='bold',
                   bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen", alpha=0.7))
    
    # Add label for the benchmark row - MATCHING ORIGINAL LAYOUT AND POSITION
    ax.text(-1.8, -1, 'Diagonal benchmarks\n(self-citation %)\nused for column\nnormalization:', 
           ha='center', va='center', 
           color='darkgreen', fontsize=reduced_fontsize, fontweight='bold',
           bbox=dict(boxstyle="round,pad=0.3", facecolor="lightcyan", alpha=0.8))
    
    # Set the background color for NaN values
    ax.set_facecolor('lightgray')
    
    # Set ticks and labels - MATCHING ORIGINAL SIZES
    ax.set_xticks(range(len(heatmap_labels)))
    ax.set_yticks(range(len(heatmap_labels)))
    ax.set_xticklabels(heatmap_labels, rotation=45, ha='right', fontsize=12)
    ax.set_yticklabels(heatmap_labels, fontsize=12)
    
    # Add labels - MATCHING ORIGINAL LAYOUT
    ax.set_xlabel('Knowledge receivers', 
                  fontsize=24, fontweight='bold', labelpad=60)
    ax.set_ylabel('Knowledge providers', 
                  fontsize=24, fontweight='bold', labelpad=60)
    
    # Add horizontal colorbar at bottom - MATCHING ORIGINAL LAYOUT
    cbar = plt.colorbar(im, ax=ax, orientation='horizontal', shrink=0.6, pad=0.2, aspect=30)
    cbar.set_label('Benchmark ratio (% of self-citation)', fontsize=16, fontweight='bold', labelpad=15)
    
    if scale_type == "logarithmic":
        # Set logarithmic colorbar ticks
        log_ticks = [-1, 0, 1, 2, 3]  # 0.1%, 1%, 10%, 100%, 1000%
        tick_labels = ['0.1%', '1%', '10%', '100%', '1000%']
        cbar.set_ticks(log_ticks)
        cbar.set_ticklabels(tick_labels, fontsize=12)
    else:
        # Set 0-100% colorbar ticks - UPDATED to match new range
        cbar.set_ticks([0, 20, 40, 60, 80, 100])
        cbar.set_ticklabels(['0%', '20%', '40%', '60%', '80%', '100%'], fontsize=14)
    
    # Add grid lines - CORRECTED positions for new order (matching first heatmap)
    grid_positions = []
    
    # Add grid lines at specified positions:
    # Between LIB Spinel and SIB Layered Oxides (after position 2)
    grid_positions.append(2.5)
    # Between SIB PBA and LIB Graphite-based (after position 5) 
    grid_positions.append(5.5)
    # Between LIB LTO and SIB Hard Carbon (after position 7)
    grid_positions.append(7.5)
    
    print(f"Debug: Setting grid lines at positions: {grid_positions}")
    
    # Draw grid lines
    for pos in grid_positions:
        ax.axhline(y=pos, color='black', linewidth=2.5, alpha=0.8)
        ax.axvline(x=pos, color='black', linewidth=2.5, alpha=0.8)
    
    # Add section labels with NEW order - FIXED positions (matching first heatmap)
    # LIB Cathodes: positions 0,1,2 (LIB Layered Oxides, LIB Polyanionic, LIB Spinel)
    lib_cathode_positions = [0, 1, 2]
    # SIB Cathodes: positions 3,4,5 (SIB Layered Oxides, SIB Polyanionic, SIB PBA)  
    sib_cathode_positions = [3, 4, 5]
    # LIB Anodes: positions 6,7 (LIB Graphite-based, LIB LTO)
    lib_anode_positions = [6, 7]
    # SIB Anodes: position 8 (SIB Hard Carbon)
    sib_anode_positions = [8]
    
    def add_section_label_with_spacing(positions, label):
        if positions:
            center_pos = (min(positions) + max(positions)) / 2
            
            # SPEZIELLE ANPASSUNG für überlappende Anode-Labels
            if 'LIB' in label and 'negative' in label:
                # LIB negative electrode etwas nach links verschieben
                adjusted_center_pos_top = center_pos - 0.2
                adjusted_center_pos_left = center_pos - 0.2
            elif 'SIB' in label and 'negative' in label:
                # SIB negative electrode etwas nach rechts verschieben  
                adjusted_center_pos_top = center_pos + 0.2
                adjusted_center_pos_left = center_pos + 0.2
            else:
                adjusted_center_pos_top = center_pos
                adjusted_center_pos_left = center_pos
            
            # Top labels - MATCHING ORIGINAL POSITION
            top_position = len(heatmap_labels) + 1.2
            ax.text(adjusted_center_pos_top, top_position, label, ha='center', va='bottom', 
                    fontsize=12, fontweight='bold', rotation=0,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgrey", alpha=0.7))
            
            # Left labels - MATCHING ORIGINAL POSITION with same adjustment
            ax.text(-2.2, adjusted_center_pos_left, label, ha='center', va='center', 
                    fontsize=12, fontweight='bold', rotation=90,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgrey", alpha=0.7))
    
    # Add section labels with SWAPPED positions for LIB/SIB electrodes (matching first heatmap)
    add_section_label_with_spacing(lib_cathode_positions, 'LIB\npositive electrode')
    add_section_label_with_spacing(sib_cathode_positions, 'SIB\npositive electrode')  
    add_section_label_with_spacing(lib_anode_positions, 'LIB\nnegative electrode')
    add_section_label_with_spacing(sib_anode_positions, 'SIB\nnegative electrode')
    
    # Add coordinate labels (A1, A2, B1, etc.) in top-right corner of each cell
    for i in range(len(heatmap_labels)):
        for j in range(len(heatmap_labels)):
            # Generate coordinate label: row letter + column number
            row_letter = chr(65 + i)  # A=65, B=66, C=67, etc.
            col_number = j + 1
            coordinate_label = f"{row_letter}{col_number}"
            
            # Position in top-right corner of cell
            # Offset by 0.45 units from center towards top-right
            ax.text(j + 0.45, i - 0.45, coordinate_label, 
                   ha='right', va='top', 
                   color='black', fontsize=12, fontweight='bold',
                   bbox=dict(boxstyle="round,pad=0.1", facecolor="white", alpha=0.8, edgecolor='none'))
    
    # Set title
    # title = f'Diagonal-Benchmarked Knowledge Flow Heatmap - {level_name} Level\n'
    # title += f'Values Relative to Self-Citation Benchmark ({normalization_type.capitalize()}, {scale_type.capitalize()} Scale)\n'
    # title += f'100% = Equal to Self-Citation, >100% = Stronger than Self-Citation'
    
    # plt.title(title, fontsize=16, fontweight='bold', pad=100)
    
    # Adjust layout - MATCHING ORIGINAL LAYOUT
    plt.tight_layout()
    plt.subplots_adjust(left=0.20, bottom=0.20, top=0.85, right=0.95)
    
    # Save the figure
    filename = f'{level_name}_Level_Diagonal_Benchmarked_{normalization_type.capitalize()}_{scale_type.capitalize()}_Heatmap.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white', transparent=True)
    print(f"✅ Diagonal-benchmarked heatmap saved to {filename}")
    
    # Close figure to free memory
    plt.close(fig)
    
    return filename
    
def print_diagonal_benchmark_statistics(benchmark_stats, level_name):
    """Print comprehensive statistics about the diagonal benchmarking process."""
    
    print(f"\n" + "="*80)
    print(f"DIAGONAL BENCHMARK STATISTICS - {level_name.upper()} LEVEL")
    print("="*80)
    
    print(f"\n📊 BENCHMARKING OVERVIEW:")
    print(f"   Columns with diagonal benchmarks: {benchmark_stats['columns_with_diagonal']}")
    print(f"   Columns without diagonal benchmarks: {benchmark_stats['columns_without_diagonal']}")
    print(f"   Total values benchmarked: {benchmark_stats['total_benchmarked_values']}")
    
    if benchmark_stats['total_benchmarked_values'] > 0:
        print(f"   Benchmark ratio range: {benchmark_stats['min_benchmark_ratio']:.1f}% - {benchmark_stats['max_benchmark_ratio']:.1f}%")
    
    if benchmark_stats['diagonal_zero_columns']:
        print(f"\n⚠️  COLUMNS WITHOUT DIAGONAL BENCHMARKS:")
        for col_info in benchmark_stats['diagonal_zero_columns']:
            print(f"   {col_info['label']} (index {col_info['index']})")
        print(f"   → These columns retain original percentage values")
    
    if benchmark_stats['benchmark_values'] is not None:
        non_zero_benchmarks = benchmark_stats['benchmark_values'][benchmark_stats['benchmark_values'] > 0]
        if len(non_zero_benchmarks) > 0:
            print(f"\n📈 DIAGONAL BENCHMARK VALUES (Self-Citations):")
            print(f"   Average diagonal benchmark: {np.mean(non_zero_benchmarks):.2f}%")
            print(f"   Median diagonal benchmark: {np.median(non_zero_benchmarks):.2f}%")
            print(f"   Range: {np.min(non_zero_benchmarks):.2f}% - {np.max(non_zero_benchmarks):.2f}%")
    
    if benchmark_stats['highest_ratios']:
        print(f"\n🔥 TOP 5 CROSS-CITATIONS RELATIVE TO SELF-CITATION:")
        for i, ratio_info in enumerate(benchmark_stats['highest_ratios'][:5]):
            print(f"   {i+1}. {ratio_info['provider']} → {ratio_info['receiver']}: "
                  f"{ratio_info['ratio']:.1f}% of self-citation")
            print(f"      (Original: {ratio_info['original_value']:.2f}%, "
                  f"Diagonal: {ratio_info['diagonal_benchmark']:.2f}%)")
    
    print(f"\n💡 INTERPRETATION:")
    print(f"   100% = Citation strength equal to receiver's self-citation")
    print(f"   >100% = Citation stronger than receiver's self-referencing")
    print(f"   <100% = Citation weaker than receiver's self-referencing")
    print(f"   High ratios indicate unusually strong cross-technology dependencies")
    
    print("="*80)

#==============================================================
# MAIN EXECUTION FUNCTION FOR DIAGONAL BENCHMARKING
#==============================================================

def create_all_diagonal_benchmarked_heatmaps(normalization_type, scale_type):
    """
    Create diagonal-benchmarked versions of all available citation heatmaps.
    Reads the CSV matrices created by the original code and applies diagonal benchmarking.
    """
    
    print(f"\n" + "="*80)
    print(f"CREATING DIAGONAL-BENCHMARKED HEATMAPS")
    print(f"Configuration: {normalization_type.upper()} normalization, {scale_type.upper()} scale")
    print("="*80)
    
    # Define file paths based on configuration
    material_csv_path = f'material_level_{normalization_type}_{scale_type}_citation_matrix_CORRECTED.csv'
    structure_csv_path = f'structure_family_level_{normalization_type}_{scale_type}_citation_matrix_CORRECTED.csv'
    
    results = {}
    
    # Process Material Level Heatmap
    try:
        print(f"\n📊 Processing Material Level Heatmap...")
        
        # Read the CSV matrix
        material_df = pd.read_csv(material_csv_path, index_col=0)
        material_matrix = material_df.values
        material_labels = material_df.columns.tolist()
        
        print(f"   ✅ Loaded material matrix: {material_matrix.shape}")
        
        # Create diagonal-benchmarked matrix
        material_benchmarked, material_diagonal, material_stats = create_diagonal_benchmarked_matrix(
            material_matrix, material_labels)
        
        # Create ordered items for material level (reconstruct from labels)
        material_ordered_items = []
        for label in material_labels:
            # Parse label format like "LIB\nLCO" or "SIB\nHard Carbon"
            lines = label.split('\n')
            if len(lines) == 2:
                battery = lines[0]
                material = lines[1]
                
                # Determine electrode type based on material mapping
                if material in ['LCO', 'NMC', 'NCA', 'LFP', 'LMO', 'NFM', 'CFM', 'NMO', 'NVPF', 'NFPP/NFP', 'Fe-PBA', 'Mn-PBA']:
                    electrode = 'cathode'
                else:
                    electrode = 'anode'
                
                material_ordered_items.append((material, electrode, battery))
        
        # Create benchmarked heatmap
        material_filename = create_diagonal_benchmarked_heatmap(
            material_benchmarked, material_labels, material_ordered_items,
            material_diagonal, material_stats, "Material", normalization_type, scale_type)
        
        # Save benchmarked matrix as CSV
        material_benchmarked_df = pd.DataFrame(material_benchmarked, 
                                             index=material_labels, 
                                             columns=material_labels)
        benchmark_csv_path = f'material_level_diagonal_benchmarked_{normalization_type}_{scale_type}_matrix.csv'
        material_benchmarked_df.to_csv(benchmark_csv_path)
        
        # Print statistics
        print_diagonal_benchmark_statistics(material_stats, "Material")
        
        results['material'] = {
            'matrix': material_benchmarked,
            'stats': material_stats,
            'filename': material_filename,
            'csv_path': benchmark_csv_path
        }
        
    except FileNotFoundError:
        print(f"   ❌ Material CSV not found: {material_csv_path}")
        print(f"   ⏭️  Skipping material level heatmap")
    except Exception as e:
        print(f"   ❌ Error processing material level: {str(e)}")
    
    # Process Structure Family Level Heatmap
    try:
        print(f"\n📊 Processing Structure Family Level Heatmap...")
        
        # Read the CSV matrix
        structure_df = pd.read_csv(structure_csv_path, index_col=0)
        structure_matrix = structure_df.values
        structure_labels = structure_df.columns.tolist()
        
        print(f"   ✅ Loaded structure family matrix: {structure_matrix.shape}")
        
        # Create diagonal-benchmarked matrix
        structure_benchmarked, structure_diagonal, structure_stats = create_diagonal_benchmarked_matrix(
            structure_matrix, structure_labels)
        
        # Create ordered items for structure family level (reconstruct from labels)
        structure_ordered_items = []
        for label in structure_labels:
            # Parse label format like "LIB\nLayered Oxides"
            lines = label.split('\n')
            if len(lines) == 2:
                battery = lines[0]
                family_short = lines[1]
                
                # Map back to full family names and determine electrode type
                family_mapping = {
                    'Layered Oxides': ('LIB_Layered_Oxides', 'cathode'),
                    'Polyanionic': ('LIB_Polyanionic', 'cathode') if battery == 'LIB' else ('SIB_Polyanionic', 'cathode'),
                    'Spinel': ('LIB_Spinel', 'cathode'),
                    'Graphite-based': ('LIB_Anode_Graphite_based', 'anode'),
                    'LTO': ('LIB_Anode_LTO', 'anode'),
                    'PBA': ('SIB_PBA', 'cathode'),
                    'Hard Carbon': ('SIB_Anode_Hard_Carbon', 'anode')
                }
                
                if family_short in family_mapping:
                    family_full, electrode = family_mapping[family_short]
                    structure_ordered_items.append((family_full, electrode, battery))
        
        # Create benchmarked heatmap
        structure_filename = create_diagonal_benchmarked_heatmap(
            structure_benchmarked, structure_labels, structure_ordered_items,
            structure_diagonal, structure_stats, "Structure_Family", normalization_type, scale_type)
        
        # Save benchmarked matrix as CSV
        structure_benchmarked_df = pd.DataFrame(structure_benchmarked, 
                                              index=structure_labels, 
                                              columns=structure_labels)
        benchmark_csv_path = f'structure_family_level_diagonal_benchmarked_{normalization_type}_{scale_type}_matrix.csv'
        structure_benchmarked_df.to_csv(benchmark_csv_path)
        
        # Print statistics
        print_diagonal_benchmark_statistics(structure_stats, "Structure Family")
        
        results['structure_family'] = {
            'matrix': structure_benchmarked,
            'stats': structure_stats,
            'filename': structure_filename,
            'csv_path': benchmark_csv_path
        }
        
    except FileNotFoundError:
        print(f"   ❌ Structure family CSV not found: {structure_csv_path}")
        print(f"   ⏭️  Skipping structure family level heatmap")
    except Exception as e:
        print(f"   ❌ Error processing structure family level: {str(e)}")
    
    # Summary
    print(f"\n🎉 DIAGONAL BENCHMARKING COMPLETED!")
    print(f"\n📁 Generated files:")
    
    for level, result in results.items():
        print(f"✅ {result['filename']}")
        print(f"✅ {result['csv_path']}")
    
    if not results:
        print("❌ No files were generated. Please check that the original CSV matrices exist.")
        print("💡 Run the original heatmap creation code first to generate the required CSV files.")
    
    print("\n💡 INTERPRETATION GUIDE:")
    print("   📊 Each cell shows citation strength relative to receiver's self-citation")
    print("   🟦 Blue values (<100%): Weaker than self-referencing")
    print("   ⚪ White values (≈100%): Equal to self-referencing") 
    print("   🟥 Red values (>100%): Stronger than self-referencing")
    print("   🔥 High red values indicate exceptionally strong cross-technology dependencies")
    
    print("="*80)
    
    return results

#==============================================================
# EXECUTE DIAGONAL BENCHMARKING
# Uses the same configuration as the original code
#==============================================================

# Use the same configuration variables from the original code
try:
    normalization_config = NORMALIZATION_TYPE
    scale_config = SCALE_TYPE
except NameError:
    # Default configuration if variables not defined
    normalization_config = "incoming"
    scale_config = "linear"
    print("⚠️  Using default configuration: incoming normalization, linear scale")

# Create diagonal-benchmarked heatmaps
diagonal_results = create_all_diagonal_benchmarked_heatmaps(normalization_config, scale_config)


#%%

#==================================================================================
# Structure Family Level Sankey Diagram with Total Knowledge Normalization
# LIB Structure Families → SIB Structure Families with Product/Process Levels
#==================================================================================

import pandas as pd
import plotly.graph_objects as go
import numpy as np
import os
from plotly.subplots import make_subplots

def add_product_process_levels_to_structure_citations(structure_citations_df, combined_df):
    """
    Add product/process level information to structure_citations_df by matching patents.
    
    Args:
        structure_citations_df: DataFrame with structure family citation data
        combined_df: DataFrame with patent-level data including product/process levels
        
    Returns:
        DataFrame with added product/process level columns
    """
    print("Adding product/process level information to structure citations...")
    
    # Create lookup dictionaries from combined_df
    patent_to_product_process = combined_df.set_index('node_num')['product_process_level'].to_dict()
    
    # Add product/process levels to structure_citations_df
    structure_citations_enhanced = structure_citations_df.copy()
    
    # Add cited (provider) product/process level
    structure_citations_enhanced['cited_product_process_level'] = structure_citations_enhanced['cited_patent'].map(patent_to_product_process)
    
    # Add citing (receiver) product/process level  
    structure_citations_enhanced['citing_product_process_level'] = structure_citations_enhanced['citing_patent'].map(patent_to_product_process)
    
    # Standardize "Product, Process" to "both"
    structure_citations_enhanced['cited_product_process_level'] = structure_citations_enhanced['cited_product_process_level'].replace('Product, Process', 'both')
    structure_citations_enhanced['citing_product_process_level'] = structure_citations_enhanced['citing_product_process_level'].replace('Product, Process', 'both')
    
    # Check for missing values
    missing_cited = structure_citations_enhanced['cited_product_process_level'].isnull().sum()
    missing_citing = structure_citations_enhanced['citing_product_process_level'].isnull().sum()
    
    print(f"  Added product/process levels to {len(structure_citations_enhanced)} citations")
    print(f"  Missing cited levels: {missing_cited}")
    print(f"  Missing citing levels: {missing_citing}")
    
    # Remove rows with missing product/process information
    structure_citations_enhanced = structure_citations_enhanced.dropna(subset=['cited_product_process_level', 'citing_product_process_level'])
    
    print(f"  Final dataset: {len(structure_citations_enhanced)} citations with complete product/process information")
    
    return structure_citations_enhanced

def create_structure_family_total_knowledge_metrics(structure_citations_enhanced, family_aggregated_df):
    """
    Create citation metrics at structure family level using Total Knowledge Normalization.
    
    Args:
        structure_citations_enhanced: DataFrame with structure citations + product/process levels
        family_aggregated_df: DataFrame with total backward citations per structure family
        
    Returns:
        DataFrame with Total Knowledge Percentage metrics
    """
    print("Creating Structure Family level Total Knowledge Percentage metrics...")
    
    # Define the relevant columns for grouping
    cited_columns = ['cited_structure_family', 'cited_family_type', 'cited_battery_type', 'cited_product_process_level']
    citing_columns = ['citing_structure_family', 'citing_family_type', 'citing_battery_type', 'citing_product_process_level']
    
    # Group by all relevant columns and count occurrences
    metrics_df = structure_citations_enhanced.groupby(cited_columns + citing_columns).size().reset_index(name='citation_count')
    
    print(f"Found {len(metrics_df)} unique structure family-process combinations with citations")
    
    # Create lookup dictionary for total backward citations
    total_backward_citations = {}
    for _, row in family_aggregated_df.iterrows():
        family_key = row['structure_family']
        total_backward_citations[family_key] = row['backward_citations']
    
    print(f"Total backward citations available for {len(total_backward_citations)} structure families")
    
    # Calculate Total Knowledge Percentage for each row
    total_knowledge_percentages = []
    
    for _, row in metrics_df.iterrows():
        receiver_family = row['citing_structure_family']
        
        # Get total backward citations for the receiver family (from ALL technologies)
        total_backward = total_backward_citations.get(receiver_family, 1)
        
        if total_backward > 0:
            # Total Knowledge Percentage: What percentage of receiver's TOTAL knowledge 
            # (from all technologies) comes from this specific provider
            total_knowledge_percentage = (row['citation_count'] / total_backward) * 100
        else:
            total_knowledge_percentage = 0
        
        total_knowledge_percentages.append(total_knowledge_percentage)
    
    # Add the Total Knowledge Percentage column
    metrics_df['total_knowledge_percentage'] = total_knowledge_percentages
    
    # Add total backward citations for reference
    metrics_df['total_backward_citations'] = metrics_df['citing_structure_family'].map(total_backward_citations)
    
    # Sort by Total Knowledge Percentage (descending)
    metrics_df = metrics_df.sort_values(by='total_knowledge_percentage', ascending=False)
    
    # Display basic statistics
    print(f"Total Knowledge Percentage statistics:")
    print(f"  Maximum percentage: {metrics_df['total_knowledge_percentage'].max():.2f}%")
    print(f"  Mean percentage: {metrics_df['total_knowledge_percentage'].mean():.2f}%")
    print(f"  Median percentage: {metrics_df['total_knowledge_percentage'].median():.2f}%")
    
    # Calculate sum of LIB/SIB percentages for each receiver (should be < 100%)
    print(f"\nTotal Knowledge Verification:")
    receiver_totals = metrics_df.groupby(['citing_structure_family', 'citing_family_type', 'citing_battery_type', 'citing_product_process_level'])['total_knowledge_percentage'].sum()
    print(f"  Sample receiver totals from LIB/SIB (should be < 100%): {receiver_totals.head().round(2).tolist()}")
    print(f"  Mean LIB/SIB share: {receiver_totals.mean():.2f}%")
    print(f"  Max LIB/SIB share: {receiver_totals.max():.2f}%")
    
    # Save to CSV for reference
    metrics_df.to_csv('structure_family_total_knowledge_metrics.csv', index=False)
    print("Structure Family Total Knowledge metrics saved to structure_family_total_knowledge_metrics.csv")
    
    return metrics_df

def create_structure_family_colors():
    """Create color mappings for structure families."""
    return {
        # LIB Structure Families
        'LIB_Layered_Oxides': 'rgba(255,0,0,1)',      # Red
        'LIB_Polyanionic': 'rgba(0,0,255,1)',         # Blue  
        'LIB_Spinel': 'rgba(0,255,255,1)',            # Cyan
        'LIB_Anode_Graphite_based': 'rgba(102,102,102,1)', # Gray
        'LIB_Anode_LTO': 'rgba(255,165,0,1)',         # Orange
        
        # SIB Structure Families
        'SIB_Layered_Oxides': 'rgba(255,102,102,1)',  # Light Red
        'SIB_Polyanionic': 'rgba(0,102,255,1)',       # Medium Blue
        'SIB_PBA': 'rgba(0,204,0,1)',                 # Green
        'SIB_Anode_Hard_Carbon': 'rgba(153,153,153,1)' # Light Gray
    }

def create_structure_process_level_link_color(base_family_color, cited_process_level):
    """
    Create link color based on structure family color and cited (provider) process level.
    """
    # Extract RGB values from base color
    if base_family_color.startswith('rgba'):
        rgb_part = base_family_color.replace('rgba(', '').replace(')', '').split(',')
        r, g, b = int(rgb_part[0]), int(rgb_part[1]), int(rgb_part[2])
    else:
        r, g, b = 128, 128, 128
    
    # Define intensity levels based on cited (provider) process level
    if cited_process_level == 'Product':
        # Light intensity (40% opacity) for Product provider nodes
        alpha = 0.4
        # Lighten the color by mixing with white
        r = min(255, int(r + (255 - r) * 0.3))
        g = min(255, int(g + (255 - g) * 0.3))
        b = min(255, int(b + (255 - b) * 0.3))
    elif cited_process_level == 'Process':
        # Medium intensity (60% opacity) for Process provider nodes
        alpha = 0.6
    elif cited_process_level == 'both':
        # Dark intensity (80% opacity) for Both provider nodes
        alpha = 0.8
        # Darken the color slightly
        r = int(r * 0.8)
        g = int(g * 0.8)
        b = int(b * 0.8)
    else:
        alpha = 0.5
    
    return f'rgba({r},{g},{b},{alpha})'

def create_lib_to_sib_structure_family_sankey(metrics_df, title, filename, threshold=0.0):
    """
    Creates a Sankey diagram for LIB Structure Families → SIB Structure Families
    using Total Knowledge Percentage normalization.
    
    Args:
        metrics_df: DataFrame with structure family citation metrics including total_knowledge_percentage
        title: Title for the Sankey diagram
        filename: Filename to save the HTML output
        threshold: Minimum Total Knowledge Percentage to include (default: 0.0)
    """
    # Filter for LIB to SIB structure family flows
    lib_structures = ['LIB_Layered_Oxides', 'LIB_Polyanionic', 'LIB_Spinel', 'LIB_Anode_Graphite_based', 'LIB_Anode_LTO']
    sib_structures = ['SIB_Layered_Oxides', 'SIB_Polyanionic', 'SIB_PBA', 'SIB_Anode_Hard_Carbon']
    
    filtered_df = metrics_df[
        (metrics_df['cited_battery_type'] == 'LIB') & 
        (metrics_df['citing_battery_type'] == 'SIB') &
        (metrics_df['cited_structure_family'].isin(lib_structures)) &
        (metrics_df['citing_structure_family'].isin(sib_structures)) &
        (metrics_df['total_knowledge_percentage'] >= threshold)
    ]
    
    if filtered_df.empty:
        print(f"No data above threshold {threshold}% for {title}. Skipping.")
        return
    
    # Get color map
    color_map = create_structure_family_colors()
    
    # Get actual structure families present in the data
    unique_cited_families = set(filtered_df['cited_structure_family'].unique())
    unique_citing_families = set(filtered_df['citing_structure_family'].unique())
    
    # Filter to only include families that are actually present, maintaining order
    sorted_cited_families = [f for f in lib_structures if f in unique_cited_families]
    sorted_citing_families = [f for f in sib_structures if f in unique_citing_families]
    
    print(f"\nCreating LIB to SIB Structure Family Sankey:")
    print(f"  LIB structure providers: {sorted_cited_families}")
    print(f"  SIB structure receivers: {sorted_citing_families}")
    print(f"  Flows above threshold {threshold}%: {len(filtered_df)}")
    
    # The unique Product/Process level values
    process_levels = ['Product', 'Process', 'both']
    
    # Create a mapping from structure families and process levels to node indices
    node_mapping = {}
    node_labels = []
    node_colors = []
    
    current_index = 0
    
    # First the cited LIB structure families (providers - on the left side)
    for family in sorted_cited_families:
        for level in process_levels:
            node_key = f"cited_{family}_{level}"
            node_mapping[node_key] = current_index
            
            # Create readable labels
            family_short = family.replace('LIB_', '').replace('_', ' ')
            if family == 'LIB_Anode_Graphite_based':
                family_short = 'Graphite-based'
            elif family == 'LIB_Anode_LTO':
                family_short = 'LTO'
                
            node_labels.append(f"LIB {family_short}\n({level})")
            node_colors.append(color_map.get(family, 'rgba(128,128,128,1)'))
            current_index += 1
    
    # Then the citing SIB structure families (receivers - on the right side)
    for family in sorted_citing_families:
        for level in process_levels:
            node_key = f"citing_{family}_{level}"
            node_mapping[node_key] = current_index
            
            # Create readable labels
            family_short = family.replace('SIB_', '').replace('_', ' ')
            if family == 'SIB_Anode_Hard_Carbon':
                family_short = 'Hard Carbon'
                
            node_labels.append(f"SIB {family_short}\n({level})")
            node_colors.append(color_map.get(family, 'rgba(128,128,128,1)'))
            current_index += 1
    
    # Create lists for sources, targets and values for the Sankey diagram
    sources = []
    targets = []
    values = []
    link_colors = []
    
    # Process each row in the filtered DataFrame
    for _, row in filtered_df.iterrows():
        cited_family = row['cited_structure_family']      # LIB Provider
        citing_family = row['citing_structure_family']    # SIB Receiver
        cited_level = row['cited_product_process_level']   # Provider level
        citing_level = row['citing_product_process_level'] # Receiver level
        
        # Skip if families are not in our sorted lists
        if cited_family not in sorted_cited_families or citing_family not in sorted_citing_families:
            continue
        
        # Check if the levels are in our process level list
        if cited_level not in process_levels or citing_level not in process_levels:
            continue
        
        # Get source and target indices (Knowledge Flow: LIB Provider → SIB Receiver)
        source_key = f"cited_{cited_family}_{cited_level}"    # LIB Provider
        target_key = f"citing_{citing_family}_{citing_level}" # SIB Receiver
        
        if source_key not in node_mapping or target_key not in node_mapping:
            continue
            
        source_idx = node_mapping[source_key]
        target_idx = node_mapping[target_key]
        
        # Use Total Knowledge Percentage as the value
        value = row['total_knowledge_percentage']
        
        # Skip if value is not positive
        if value <= 0:
            continue
        
        # Use the color of the provider family (cited LIB family) for the link, with intensity based on provider process level
        base_color = color_map.get(cited_family, 'rgba(128,128,128,1)')
        link_color = create_structure_process_level_link_color(base_color, cited_level)
        
        sources.append(source_idx)
        targets.append(target_idx)
        values.append(value)
        link_colors.append(link_color)
    
    # Check if we have any data to plot
    if not sources:
        print(f"No valid links for {title}. Skipping.")
        return
    
    # Calculate statistics for subtitle
    max_percentage = filtered_df['total_knowledge_percentage'].max()
    mean_percentage = filtered_df['total_knowledge_percentage'].mean()
    
    # Create the Sankey diagram
    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",  # This forces Plotly to maintain node order
        node=dict(
            pad=15,
            thickness=20,
            line=dict(color="black", width=0.5),
            label=node_labels,
            color=node_colors
        ),
        link=dict(
            source=sources,
            target=targets,
            value=values,
            color=link_colors  # Color links based on provider family
        )
    )])
    
    # Create subtitle
    subtitle = f"LIB Structure Families → SIB Structure Families (Product/Process Levels)<br>Total Knowledge Percentage | Threshold: {threshold:.1f}% | Max: {max_percentage:.1f}% | Mean: {mean_percentage:.1f}%"
    
    # Update layout
    fig.update_layout(
        title_text=f"{title}<br><sub>{subtitle}</sub>",
        font_size=14,
        width=1800,    # Increased for better visibility with more nodes
        height=1400    # Increased for better visibility with more nodes
    )
    
    # Save the figure as an HTML file
    output_dir = "sankey_diagrams"
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"{filename}_total_knowledge.html")
    fig.write_html(output_path)
    
    print(f"LIB to SIB Structure Family Sankey diagram saved to {output_path}")
    
    return fig


def create_lib_to_lib_structure_family_sankey(metrics_df, title, filename, threshold=0.0):
    """
    Creates a Sankey diagram for LIB Structure Families → LIB Structure Families
    using Total Knowledge Percentage normalization.
    """
    # Filter for LIB to LIB structure family flows
    lib_structures = ['LIB_Layered_Oxides', 'LIB_Polyanionic', 'LIB_Spinel', 'LIB_Anode_Graphite_based', 'LIB_Anode_LTO']
    
    filtered_df = metrics_df[
        (metrics_df['cited_battery_type'] == 'LIB') & 
        (metrics_df['citing_battery_type'] == 'LIB') &
        (metrics_df['cited_structure_family'].isin(lib_structures)) &
        (metrics_df['citing_structure_family'].isin(lib_structures)) &
        (metrics_df['total_knowledge_percentage'] >= threshold)
    ]
    
    if filtered_df.empty:
        print(f"No data above threshold {threshold}% for {title}. Skipping.")
        return
    
    # Get color map
    color_map = create_structure_family_colors()
    
    # Get actual structure families present in the data
    unique_cited_families = set(filtered_df['cited_structure_family'].unique())
    unique_citing_families = set(filtered_df['citing_structure_family'].unique())
    
    # Filter to only include families that are actually present, maintaining order
    sorted_cited_families = [f for f in lib_structures if f in unique_cited_families]
    sorted_citing_families = [f for f in lib_structures if f in unique_citing_families]
    
    print(f"\nCreating LIB to LIB Structure Family Sankey:")
    print(f"  LIB structure providers: {sorted_cited_families}")
    print(f"  LIB structure receivers: {sorted_citing_families}")
    print(f"  Flows above threshold {threshold}%: {len(filtered_df)}")
    
    # The unique Product/Process level values
    process_levels = ['Product', 'Process', 'both']
    
    # Create a mapping from structure families and process levels to node indices
    node_mapping = {}
    node_labels = []
    node_colors = []
    
    current_index = 0
    
    # First the cited LIB structure families (providers - on the left side)
    for family in sorted_cited_families:
        for level in process_levels:
            node_key = f"cited_{family}_{level}"
            node_mapping[node_key] = current_index
            
            # Create readable labels
            family_short = family.replace('LIB_', '').replace('_', ' ')
            if family == 'LIB_Anode_Graphite_based':
                family_short = 'Graphite-based'
            elif family == 'LIB_Anode_LTO':
                family_short = 'LTO'
                
            node_labels.append(f"LIB {family_short}\nProvider ({level})")
            node_colors.append(color_map.get(family, 'rgba(128,128,128,1)'))
            current_index += 1
    
    # Then the citing LIB structure families (receivers - on the right side)
    for family in sorted_citing_families:
        for level in process_levels:
            node_key = f"citing_{family}_{level}"
            node_mapping[node_key] = current_index
            
            # Create readable labels
            family_short = family.replace('LIB_', '').replace('_', ' ')
            if family == 'LIB_Anode_Graphite_based':
                family_short = 'Graphite-based'
            elif family == 'LIB_Anode_LTO':
                family_short = 'LTO'
                
            node_labels.append(f"LIB {family_short}\nReceiver ({level})")
            node_colors.append(color_map.get(family, 'rgba(128,128,128,1)'))
            current_index += 1
    
    # Create lists for sources, targets and values for the Sankey diagram
    sources = []
    targets = []
    values = []
    link_colors = []
    
    # Process each row in the filtered DataFrame
    for _, row in filtered_df.iterrows():
        cited_family = row['cited_structure_family']      # LIB Provider
        citing_family = row['citing_structure_family']    # LIB Receiver
        cited_level = row['cited_product_process_level']   # Provider level
        citing_level = row['citing_product_process_level'] # Receiver level
        
        # Skip self-citations (same family and level)
        if cited_family == citing_family and cited_level == citing_level:
            continue
        
        # Skip if families are not in our sorted lists
        if cited_family not in sorted_cited_families or citing_family not in sorted_citing_families:
            continue
        
        # Check if the levels are in our process level list
        if cited_level not in process_levels or citing_level not in process_levels:
            continue
        
        # Get source and target indices (Knowledge Flow: LIB Provider → LIB Receiver)
        source_key = f"cited_{cited_family}_{cited_level}"    # LIB Provider
        target_key = f"citing_{citing_family}_{citing_level}" # LIB Receiver
        
        if source_key not in node_mapping or target_key not in node_mapping:
            continue
            
        source_idx = node_mapping[source_key]
        target_idx = node_mapping[target_key]
        
        # Use Total Knowledge Percentage as the value
        value = row['total_knowledge_percentage']
        
        # Skip if value is not positive
        if value <= 0:
            continue
        
        # Use the color of the provider family (cited LIB family) for the link
        base_color = color_map.get(cited_family, 'rgba(128,128,128,1)')
        link_color = create_structure_process_level_link_color(base_color, cited_level)
        
        sources.append(source_idx)
        targets.append(target_idx)
        values.append(value)
        link_colors.append(link_color)
    
    # Check if we have any data to plot
    if not sources:
        print(f"No valid links for {title}. Skipping.")
        return
    
    # Calculate statistics for subtitle
    max_percentage = filtered_df['total_knowledge_percentage'].max()
    mean_percentage = filtered_df['total_knowledge_percentage'].mean()
    
    # Create the Sankey diagram
    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=15,
            thickness=20,
            line=dict(color="black", width=0.5),
            label=node_labels,
            color=node_colors
        ),
        link=dict(
            source=sources,
            target=targets,
            value=values,
            color=link_colors
        )
    )])
    
    # Create subtitle
    subtitle = f"LIB Structure Families → LIB Structure Families (Product/Process Levels)<br>Total Knowledge Percentage | Threshold: {threshold:.1f}% | Max: {max_percentage:.1f}% | Mean: {mean_percentage:.1f}%"
    
    # Update layout
    fig.update_layout(
        title_text=f"{title}<br><sub>{subtitle}</sub>",
        font_size=14,
        width=1800,
        height=1400
    )
    
    # Save the figure as an HTML file
    output_dir = "sankey_diagrams"
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"{filename}_total_knowledge.html")
    fig.write_html(output_path)
    
    print(f"LIB to LIB Structure Family Sankey diagram saved to {output_path}")
    
    return fig


def create_sib_to_sib_structure_family_sankey(metrics_df, title, filename, threshold=0.0):
    """
    Creates a Sankey diagram for SIB Structure Families → SIB Structure Families
    using Total Knowledge Percentage normalization.
    """
    # Filter for SIB to SIB structure family flows
    sib_structures = ['SIB_Layered_Oxides', 'SIB_Polyanionic', 'SIB_PBA', 'SIB_Anode_Hard_Carbon']
    
    filtered_df = metrics_df[
        (metrics_df['cited_battery_type'] == 'SIB') & 
        (metrics_df['citing_battery_type'] == 'SIB') &
        (metrics_df['cited_structure_family'].isin(sib_structures)) &
        (metrics_df['citing_structure_family'].isin(sib_structures)) &
        (metrics_df['total_knowledge_percentage'] >= threshold)
    ]
    
    if filtered_df.empty:
        print(f"No data above threshold {threshold}% for {title}. Skipping.")
        return
    
    # Get color map
    color_map = create_structure_family_colors()
    
    # Get actual structure families present in the data
    unique_cited_families = set(filtered_df['cited_structure_family'].unique())
    unique_citing_families = set(filtered_df['citing_structure_family'].unique())
    
    # Filter to only include families that are actually present, maintaining order
    sorted_cited_families = [f for f in sib_structures if f in unique_cited_families]
    sorted_citing_families = [f for f in sib_structures if f in unique_citing_families]
    
    print(f"\nCreating SIB to SIB Structure Family Sankey:")
    print(f"  SIB structure providers: {sorted_cited_families}")
    print(f"  SIB structure receivers: {sorted_citing_families}")
    print(f"  Flows above threshold {threshold}%: {len(filtered_df)}")
    
    # The unique Product/Process level values
    process_levels = ['Product', 'Process', 'both']
    
    # Create a mapping from structure families and process levels to node indices
    node_mapping = {}
    node_labels = []
    node_colors = []
    
    current_index = 0
    
    # First the cited SIB structure families (providers - on the left side)
    for family in sorted_cited_families:
        for level in process_levels:
            node_key = f"cited_{family}_{level}"
            node_mapping[node_key] = current_index
            
            # Create readable labels
            family_short = family.replace('SIB_', '').replace('_', ' ')
            if family == 'SIB_Anode_Hard_Carbon':
                family_short = 'Hard Carbon'
                
            node_labels.append(f"SIB {family_short}\nProvider ({level})")
            node_colors.append(color_map.get(family, 'rgba(128,128,128,1)'))
            current_index += 1
    
    # Then the citing SIB structure families (receivers - on the right side)
    for family in sorted_citing_families:
        for level in process_levels:
            node_key = f"citing_{family}_{level}"
            node_mapping[node_key] = current_index
            
            # Create readable labels
            family_short = family.replace('SIB_', '').replace('_', ' ')
            if family == 'SIB_Anode_Hard_Carbon':
                family_short = 'Hard Carbon'
                
            node_labels.append(f"SIB {family_short}\nReceiver ({level})")
            node_colors.append(color_map.get(family, 'rgba(128,128,128,1)'))
            current_index += 1
    
    # Create lists for sources, targets and values for the Sankey diagram
    sources = []
    targets = []
    values = []
    link_colors = []
    
    # Process each row in the filtered DataFrame
    for _, row in filtered_df.iterrows():
        cited_family = row['cited_structure_family']      # SIB Provider
        citing_family = row['citing_structure_family']    # SIB Receiver
        cited_level = row['cited_product_process_level']   # Provider level
        citing_level = row['citing_product_process_level'] # Receiver level
        
        # Skip self-citations (same family and level)
        if cited_family == citing_family and cited_level == citing_level:
            continue
        
        # Skip if families are not in our sorted lists
        if cited_family not in sorted_cited_families or citing_family not in sorted_citing_families:
            continue
        
        # Check if the levels are in our process level list
        if cited_level not in process_levels or citing_level not in process_levels:
            continue
        
        # Get source and target indices (Knowledge Flow: SIB Provider → SIB Receiver)
        source_key = f"cited_{cited_family}_{cited_level}"    # SIB Provider
        target_key = f"citing_{citing_family}_{citing_level}" # SIB Receiver
        
        if source_key not in node_mapping or target_key not in node_mapping:
            continue
            
        source_idx = node_mapping[source_key]
        target_idx = node_mapping[target_key]
        
        # Use Total Knowledge Percentage as the value
        value = row['total_knowledge_percentage']
        
        # Skip if value is not positive
        if value <= 0:
            continue
        
        # Use the color of the provider family (cited SIB family) for the link
        base_color = color_map.get(cited_family, 'rgba(128,128,128,1)')
        link_color = create_structure_process_level_link_color(base_color, cited_level)
        
        sources.append(source_idx)
        targets.append(target_idx)
        values.append(value)
        link_colors.append(link_color)
    
    # Check if we have any data to plot
    if not sources:
        print(f"No valid links for {title}. Skipping.")
        return
    
    # Calculate statistics for subtitle
    max_percentage = filtered_df['total_knowledge_percentage'].max()
    mean_percentage = filtered_df['total_knowledge_percentage'].mean()
    
    # Create the Sankey diagram
    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=15,
            thickness=20,
            line=dict(color="black", width=0.5),
            label=node_labels,
            color=node_colors
        ),
        link=dict(
            source=sources,
            target=targets,
            value=values,
            color=link_colors
        )
    )])
    
    # Create subtitle
    subtitle = f"SIB Structure Families → SIB Structure Families (Product/Process Levels)<br>Total Knowledge Percentage | Threshold: {threshold:.1f}% | Max: {max_percentage:.1f}% | Mean: {mean_percentage:.1f}%"
    
    # Update layout
    fig.update_layout(
        title_text=f"{title}<br><sub>{subtitle}</sub>",
        font_size=14,
        width=1800,
        height=1400
    )
    
    # Save the figure as an HTML file
    output_dir = "sankey_diagrams"
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"{filename}_total_knowledge.html")
    fig.write_html(output_path)
    
    print(f"SIB to SIB Structure Family Sankey diagram saved to {output_path}")
    
    return fig


def print_structure_family_total_knowledge_summary(metrics_df):
    """Print summary statistics for the Structure Family Total Knowledge analysis."""
    
    print("\n" + "="*80)
    print("STRUCTURE FAMILY TOTAL KNOWLEDGE ANALYSIS SUMMARY")
    print("="*80)
    
    print(f"\n📊 OVERALL STATISTICS:")
    print(f"   Total structure family-process pairs with citations: {len(metrics_df):4d}")
    print(f"   Maximum Total Knowledge Percentage: {metrics_df['total_knowledge_percentage'].max():7.2f}%")
    print(f"   Mean Total Knowledge Percentage: {metrics_df['total_knowledge_percentage'].mean():7.2f}%")
    print(f"   Median Total Knowledge Percentage: {metrics_df['total_knowledge_percentage'].median():7.2f}%")
    
    # LIB to SIB flows specifically
    lib_to_sib = metrics_df[
        (metrics_df['cited_battery_type'] == 'LIB') & 
        (metrics_df['citing_battery_type'] == 'SIB')
    ]
    
    if not lib_to_sib.empty:
        print(f"\n🔄 LIB → SIB STRUCTURE FAMILY FLOWS:")
        print(f"   Total LIB → SIB flows: {len(lib_to_sib):4d}")
        print(f"   Maximum LIB → SIB percentage: {lib_to_sib['total_knowledge_percentage'].max():7.2f}%")
        print(f"   Mean LIB → SIB percentage: {lib_to_sib['total_knowledge_percentage'].mean():7.2f}%")
        
        # Top LIB → SIB flows
        print(f"\n🔝 TOP 10 LIB → SIB KNOWLEDGE FLOWS:")
        top_lib_to_sib = lib_to_sib.head(10)
        for i, (_, row) in enumerate(top_lib_to_sib.iterrows(), 1):
            provider = row['cited_structure_family'].replace('LIB_', '')
            receiver = row['citing_structure_family'].replace('SIB_', '')
            print(f"   {i:2d}. {provider:15} ({row['cited_product_process_level']:7}) → "
                  f"{receiver:15} ({row['citing_product_process_level']:7}) | "
                  f"Share: {row['total_knowledge_percentage']:6.2f}% | Citations: {row['citation_count']:3d}")
    
    # Structure family coverage
    print(f"\n📈 STRUCTURE FAMILY COVERAGE:")
    lib_providers = metrics_df[metrics_df['cited_battery_type'] == 'LIB']['cited_structure_family'].unique()
    sib_receivers = metrics_df[metrics_df['citing_battery_type'] == 'SIB']['citing_structure_family'].unique()
    
    print(f"   LIB provider families: {len(lib_providers)} - {sorted(lib_providers)}")
    print(f"   SIB receiver families: {len(sib_receivers)} - {sorted(sib_receivers)}")
    
    # Process level distribution
    print(f"\n🏭 PRODUCT/PROCESS LEVEL DISTRIBUTION:")
    cited_levels = metrics_df['cited_product_process_level'].value_counts()
    citing_levels = metrics_df['citing_product_process_level'].value_counts()
    
    print(f"   Provider process levels:")
    for level, count in cited_levels.items():
        print(f"     {level:10}: {count:4d} pairs")
    
    print(f"   Receiver process levels:")
    for level, count in citing_levels.items():
        print(f"     {level:10}: {count:4d} pairs")
    
    print("="*80)

# Main execution function
def main_structure_family_total_knowledge_analysis():
    """
    Main function to create all Structure Family level Sankey diagrams with Total Knowledge normalization.
    """
    print("Starting Comprehensive Structure Family Total Knowledge Analysis...")
    
    try:
        # Check if required variables exist
        print("Checking for required data...")
        
        # Test if variables exist
        test_structure_citations = structure_citations_df
        test_combined = combined_df
        print("✅ Found required datasets")
        
        # Step 1: Add product/process levels to structure citations
        print("\n📊 Step 1: Adding product/process levels to structure citations...")
        structure_citations_enhanced = add_product_process_levels_to_structure_citations(
            structure_citations_df, combined_df
        )
        
        # Step 2: Create structure family aggregated data for total backward citations
        print("\n📊 Step 2: Creating structure family aggregated data...")
        family_aggregated_df = create_structure_family_aggregated_df(combined_df)
        
        # Step 3: Create total knowledge metrics
        print("\n📊 Step 3: Creating Total Knowledge Percentage metrics...")
        structure_total_knowledge_metrics = create_structure_family_total_knowledge_metrics(
            structure_citations_enhanced, family_aggregated_df
        )
        
        # Step 4: Print summary statistics
        print_structure_family_total_knowledge_summary(structure_total_knowledge_metrics)
        
        # Step 5: Create all three Sankey diagrams
        print("\n📊 Step 5: Creating Structure Family Sankey diagrams...")
        
        # 5a: LIB → SIB Sankey diagram
        print("\n🔄 Creating LIB → SIB Structure Family Sankey diagram...")
        fig_lib_to_sib = create_lib_to_sib_structure_family_sankey(
            structure_total_knowledge_metrics,
            "LIB Structure Families to SIB Structure Families Knowledge Transfer",
            "LIB_to_SIB_Structure_Families_Total_Knowledge_Sankey",
            threshold=0.0
        )
        
        # 5b: LIB → LIB Sankey diagram
        print("\n🔄 Creating LIB → LIB Structure Family Sankey diagram...")
        fig_lib_to_lib = create_lib_to_lib_structure_family_sankey(
            structure_total_knowledge_metrics,
            "LIB Structure Families Internal Knowledge Transfer",
            "LIB_to_LIB_Structure_Families_Total_Knowledge_Sankey",
            threshold=0.0
        )
        
        # 5c: SIB → SIB Sankey diagram
        print("\n🔄 Creating SIB → SIB Structure Family Sankey diagram...")
        fig_sib_to_sib = create_sib_to_sib_structure_family_sankey(
            structure_total_knowledge_metrics,
            "SIB Structure Families Internal Knowledge Transfer",
            "SIB_to_SIB_Structure_Families_Total_Knowledge_Sankey",
            threshold=0.0
        )
        
        print(f"\n✅ COMPREHENSIVE ANALYSIS COMPLETE!")
        print(f"📁 Files created:")
        print(f"   - structure_family_total_knowledge_metrics.csv")
        print(f"   - sankey_diagrams/LIB_to_SIB_Structure_Families_Total_Knowledge_Sankey_total_knowledge.html")
        print(f"   - sankey_diagrams/LIB_to_LIB_Structure_Families_Total_Knowledge_Sankey_total_knowledge.html")
        print(f"   - sankey_diagrams/SIB_to_SIB_Structure_Families_Total_Knowledge_Sankey_total_knowledge.html")
        
    except NameError as e:
        print(f"❌ Error: Required data not found - {e}")
        print("Please ensure the following variables are available:")
        print("   - structure_citations_df")
        print("   - combined_df")
        print("   - create_structure_family_aggregated_df function")
        
    except Exception as e:
        print(f"❌ Error during analysis: {e}")
        import traceback
        traceback.print_exc()

# Run the analysis
if __name__ == "__main__":
    main_structure_family_total_knowledge_analysis()

print("\n" + "="*80)
print("STRUCTURE FAMILY TOTAL KNOWLEDGE INTERPRETATION GUIDE")
print("="*80)

print("""
🎯 STRUCTURE FAMILY TOTAL KNOWLEDGE ANALYSIS:
   This analysis shows knowledge flows from LIB Structure Families to SIB Structure Families
   using Total Knowledge Normalization (consistent with corrected heatmap methodology).

📊 METHODOLOGY - TOTAL KNOWLEDGE NORMALIZATION:
   Formula: Total_Knowledge_Percentage(i→j) = Citations(i→j) / Total_Backward_Citations(j) × 100%
   Where:
   • i = Knowledge Provider (LIB structure family + process level)
   • j = Knowledge Receiver (SIB structure family + process level)  
   • Total_Backward_Citations(j) = ALL citations received by j from ALL technologies

🏗️ STRUCTURE FAMILY DEFINITIONS:
   LIB Providers (5 families):
   • LIB Layered Oxides (LCO, NMC, NCA cathodes)
   • LIB Polyanionic (LFP cathodes)
   • LIB Spinel (LMO cathodes)
   • LIB Graphite-based (Graphite, Silicon/Carbon anodes)
   • LIB LTO (Lithium Titanate anodes)
   
   SIB Receivers (4 families):
   • SIB Layered Oxides (NFM, CFM, NMO cathodes)
   • SIB Polyanionic (NVPF, NFPP/NFP cathodes)
   • SIB PBA (Fe-PBA, Mn-PBA cathodes)
   • SIB Hard Carbon (Hard Carbon anodes)

🎨 SANKEY DIAGRAM ELEMENTS:
   • Left Side: 15 LIB provider nodes (5 families × 3 process levels)
   • Right Side: 12 SIB receiver nodes (4 families × 3 process levels)
   • Links: Proportional to Total Knowledge Percentage
   • Node Colors: Based on structure family type
   • Link Colors: Provider family color with process-level intensity

🔍 LINK COLOR INTENSITY (Provider Process Level):
   • Light flows: Product-focused LIB providers (40% opacity + lightened)
   • Medium flows: Process-focused LIB providers (60% opacity + original)
   • Dark flows: Comprehensive LIB providers (80% opacity + darkened)

📈 PERCENTAGE INTERPRETATION:
   HIGH PERCENTAGES (>5%):
   • Strong dependency of SIB family on specific LIB knowledge
   • Core technological relationship in battery innovation
   • Critical knowledge transfer pathway

   MODERATE PERCENTAGES (1-5%):
   • Significant but not dominant knowledge relationship
   • Secondary technological connection
   • Balanced knowledge integration

   LOW PERCENTAGES (<1%):
   • Minor knowledge relationship
   • SIB family primarily builds on non-LIB knowledge
   • Emerging or weak technological connection

💡 STRATEGIC PATTERNS:

CATHODE-TO-CATHODE FLOWS:
• LIB Layered Oxides → SIB Layered Oxides: Direct structural knowledge transfer
• LIB Polyanionic → SIB Polyanionic: Chemistry-specific knowledge sharing
• Cross-chemistry flows: Novel material development approaches

ANODE-TO-ANODE FLOWS:
• LIB Graphite-based → SIB Hard Carbon: Carbon-based material knowledge
• LIB LTO → SIB Hard Carbon: Advanced anode material insights

CROSS-ELECTRODE FLOWS:
• LIB Cathodes → SIB Anodes: System integration knowledge
• LIB Anodes → SIB Cathodes: Cell-level design insights

PROCESS-LEVEL INSIGHTS:
• Product → Product: Material design and composition knowledge
• Process → Process: Manufacturing and scaling expertise
• Both → Any: Comprehensive technological knowledge transfer

🎯 BUSINESS APPLICATIONS:

TECHNOLOGY STRATEGY:
• High-percentage flows: Monitor critical LIB knowledge dependencies
• Low SIB percentages: Opportunities for unique SIB innovation paths
• Process-level patterns: Guide R&D focus areas (product vs. process)

COMPETITIVE INTELLIGENCE:
• Strong LIB → SIB flows: Areas where LIB experience provides advantage
• Weak flows: Potential SIB differentiation opportunities
• Cross-level flows: Integration and scaling insights

PARTNERSHIP STRATEGY:
• High-percentage relationships: Strategic collaboration targets
• Balanced dependencies: Robust, diversified knowledge sourcing
• Process-specific flows: Operational partnership opportunities

IP STRATEGY:
• Critical knowledge flows: High-value patent landscape areas
• Emerging flows: Early-stage IP positioning opportunities
• Cross-family flows: Broad technological IP portfolios

INNOVATION ROADMAP:
• Product-heavy flows: Focus on material innovation
• Process-heavy flows: Emphasize manufacturing development
• Balanced flows: Integrated product-process innovation

This analysis reveals how SIB technologies leverage LIB knowledge foundations
while identifying opportunities for distinct SIB innovation pathways.
""")

print("="*80)



#%%

#==================================================================================
# Structure Family Level Sankey Diagram with Total Knowledge Normalization
# LIB Structure Families → SIB Structure Families with Product/Process Levels
#==================================================================================

import pandas as pd
import plotly.graph_objects as go
import numpy as np
import os
from plotly.subplots import make_subplots

def add_product_process_levels_to_structure_citations(structure_citations_df, combined_df):
    """
    Add product/process level information to structure_citations_df by matching patents.
    
    Args:
        structure_citations_df: DataFrame with structure family citation data
        combined_df: DataFrame with patent-level data including product/process levels
        
    Returns:
        DataFrame with added product/process level columns
    """
    print("Adding product/process level information to structure citations...")
    
    # Create lookup dictionaries from combined_df using correct column name
    patent_to_product_process = combined_df.set_index('node_num')['Product-Process-Level'].to_dict()
    
    # Add product/process levels to structure_citations_df
    structure_citations_enhanced = structure_citations_df.copy()
    
    # Add cited (provider) product/process level
    structure_citations_enhanced['cited_product_process_level'] = structure_citations_enhanced['cited_patent'].map(patent_to_product_process)
    
    # Add citing (receiver) product/process level  
    structure_citations_enhanced['citing_product_process_level'] = structure_citations_enhanced['citing_patent'].map(patent_to_product_process)
    
    # Standardize "Product, Process" to "both"
    structure_citations_enhanced['cited_product_process_level'] = structure_citations_enhanced['cited_product_process_level'].replace('Product, Process', 'both')
    structure_citations_enhanced['citing_product_process_level'] = structure_citations_enhanced['citing_product_process_level'].replace('Product, Process', 'both')
    
    # Check for missing values
    missing_cited = structure_citations_enhanced['cited_product_process_level'].isnull().sum()
    missing_citing = structure_citations_enhanced['citing_product_process_level'].isnull().sum()
    
    print(f"  Added product/process levels to {len(structure_citations_enhanced)} citations")
    print(f"  Missing cited levels: {missing_cited}")
    print(f"  Missing citing levels: {missing_citing}")
    
    # Remove rows with missing product/process information
    structure_citations_enhanced = structure_citations_enhanced.dropna(subset=['cited_product_process_level', 'citing_product_process_level'])
    
    print(f"  Final dataset: {len(structure_citations_enhanced)} citations with complete product/process information")
    
    return structure_citations_enhanced

def create_structure_family_total_knowledge_metrics(structure_citations_enhanced, family_aggregated_df):
    """
    Create citation metrics at structure family level using Total Knowledge Normalization.
    
    Args:
        structure_citations_enhanced: DataFrame with structure citations + product/process levels
        family_aggregated_df: DataFrame with total backward citations per structure family
        
    Returns:
        DataFrame with Total Knowledge Percentage metrics
    """
    print("Creating Structure Family level Total Knowledge Percentage metrics...")
    
    # Define the relevant columns for grouping
    cited_columns = ['cited_structure_family', 'cited_family_type', 'cited_battery_type', 'cited_product_process_level']
    citing_columns = ['citing_structure_family', 'citing_family_type', 'citing_battery_type', 'citing_product_process_level']
    
    # Group by all relevant columns and count occurrences
    metrics_df = structure_citations_enhanced.groupby(cited_columns + citing_columns).size().reset_index(name='citation_count')
    
    print(f"Found {len(metrics_df)} unique structure family-process combinations with citations")
    
    # Create lookup dictionary for total backward citations
    total_backward_citations = {}
    for _, row in family_aggregated_df.iterrows():
        family_key = row['structure_family']
        total_backward_citations[family_key] = row['backward_citations']
    
    print(f"Total backward citations available for {len(total_backward_citations)} structure families")
    
    # Calculate Total Knowledge Percentage for each row
    total_knowledge_percentages = []
    
    for _, row in metrics_df.iterrows():
        receiver_family = row['citing_structure_family']
        
        # Get total backward citations for the receiver family (from ALL technologies)
        total_backward = total_backward_citations.get(receiver_family, 1)
        
        if total_backward > 0:
            # Total Knowledge Percentage: What percentage of receiver's TOTAL knowledge 
            # (from all technologies) comes from this specific provider
            total_knowledge_percentage = (row['citation_count'] / total_backward) * 100
        else:
            total_knowledge_percentage = 0
        
        total_knowledge_percentages.append(total_knowledge_percentage)
    
    # Add the Total Knowledge Percentage column
    metrics_df['total_knowledge_percentage'] = total_knowledge_percentages
    
    # Add total backward citations for reference
    metrics_df['total_backward_citations'] = metrics_df['citing_structure_family'].map(total_backward_citations)
    
    # Sort by Total Knowledge Percentage (descending)
    metrics_df = metrics_df.sort_values(by='total_knowledge_percentage', ascending=False)
    
    # Display basic statistics
    print(f"Total Knowledge Percentage statistics:")
    print(f"  Maximum percentage: {metrics_df['total_knowledge_percentage'].max():.2f}%")
    print(f"  Mean percentage: {metrics_df['total_knowledge_percentage'].mean():.2f}%")
    print(f"  Median percentage: {metrics_df['total_knowledge_percentage'].median():.2f}%")
    
    # Calculate sum of LIB/SIB percentages for each receiver (should be < 100%)
    print(f"\nTotal Knowledge Verification:")
    receiver_totals = metrics_df.groupby(['citing_structure_family', 'citing_family_type', 'citing_battery_type', 'citing_product_process_level'])['total_knowledge_percentage'].sum()
    print(f"  Sample receiver totals from LIB/SIB (should be < 100%): {receiver_totals.head().round(2).tolist()}")
    print(f"  Mean LIB/SIB share: {receiver_totals.mean():.2f}%")
    print(f"  Max LIB/SIB share: {receiver_totals.max():.2f}%")
    
    # Save to CSV for reference
    metrics_df.to_csv('structure_family_total_knowledge_metrics.csv', index=False)
    print("Structure Family Total Knowledge metrics saved to structure_family_total_knowledge_metrics.csv")
    
    return metrics_df

def create_structure_family_colors():
    """Create color mappings for structure families."""
    return {
        # LIB Structure Families
        'LIB_Layered_Oxides': 'rgba(255,0,0,1)',      # Red
        'LIB_Polyanionic': 'rgba(0,0,255,1)',         # Blue  
        'LIB_Spinel': 'rgba(0,255,255,1)',            # Cyan
        'LIB_Anode_Graphite_based': 'rgba(102,102,102,1)', # Gray
        'LIB_Anode_LTO': 'rgba(255,165,0,1)',         # Orange
        
        # SIB Structure Families
        'SIB_Layered_Oxides': 'rgba(255,102,102,1)',  # Light Red
        'SIB_Polyanionic': 'rgba(0,102,255,1)',       # Medium Blue
        'SIB_PBA': 'rgba(0,204,0,1)',                 # Green
        'SIB_Anode_Hard_Carbon': 'rgba(153,153,153,1)' # Light Gray
    }

def create_structure_process_level_link_color(base_family_color, cited_process_level):
    """
    Create link color based on structure family color and cited (provider) process level.
    Enhanced color differences for better distinction, especially for LTO.
    """
    # Extract RGB values from base color
    if base_family_color.startswith('rgba'):
        rgb_part = base_family_color.replace('rgba(', '').replace(')', '').split(',')
        r, g, b = int(rgb_part[0]), int(rgb_part[1]), int(rgb_part[2])
    else:
        r, g, b = 128, 128, 128
    
    # Enhanced intensity levels with stronger color differences
    if cited_process_level == 'Product':
        # Light intensity (35% opacity) for Product provider nodes - MORE LIGHTENING
        alpha = 0.35
        # Stronger lightening by mixing more with white
        r = min(255, int(r + (255 - r) * 0.5))  # Increased from 0.3 to 0.5
        g = min(255, int(g + (255 - g) * 0.5))
        b = min(255, int(b + (255 - b) * 0.5))
    elif cited_process_level == 'Process':
        # Medium intensity (65% opacity) for Process provider nodes
        alpha = 0.65
        # Keep original color intensity but slightly adjusted
        pass
    elif cited_process_level == 'both':
        # Dark intensity (85% opacity) for Both provider nodes - STRONGER DARKENING
        alpha = 0.85
        # Stronger darkening for better contrast
        r = int(r * 0.65)  # Reduced from 0.8 to 0.65
        g = int(g * 0.65)
        b = int(b * 0.65)
    else:
        alpha = 0.5
    
    return f'rgba({r},{g},{b},{alpha})'

def create_lib_to_sib_structure_family_sankey(metrics_df, title, filename, threshold=0.0, show_node_labels=True):
    """
    Creates a Sankey diagram for LIB Structure Families → SIB Structure Families
    using Total Knowledge Percentage normalization.
    
    Args:
        metrics_df: DataFrame with structure family citation metrics including total_knowledge_percentage
        title: Title for the Sankey diagram
        filename: Filename to save the HTML output
        threshold: Minimum Total Knowledge Percentage to include (default: 0.0)
        show_node_labels: Boolean to control whether node labels are displayed (default: True)
    """
    # Filter for LIB to SIB structure family flows
    lib_structures = ['LIB_Layered_Oxides', 'LIB_Polyanionic', 'LIB_Spinel', 'LIB_Anode_Graphite_based', 'LIB_Anode_LTO']
    sib_structures = ['SIB_Layered_Oxides', 'SIB_Polyanionic', 'SIB_PBA', 'SIB_Anode_Hard_Carbon']
    
    filtered_df = metrics_df[
        (metrics_df['cited_battery_type'] == 'LIB') & 
        (metrics_df['citing_battery_type'] == 'SIB') &
        (metrics_df['cited_structure_family'].isin(lib_structures)) &
        (metrics_df['citing_structure_family'].isin(sib_structures)) &
        (metrics_df['total_knowledge_percentage'] >= threshold)
    ]
    
    if filtered_df.empty:
        print(f"No data above threshold {threshold}% for {title}. Skipping.")
        return
    
    # Get color map
    color_map = create_structure_family_colors()
    
    # Get actual structure families present in the data
    unique_cited_families = set(filtered_df['cited_structure_family'].unique())
    unique_citing_families = set(filtered_df['citing_structure_family'].unique())
    
    # Filter to only include families that are actually present, maintaining order
    sorted_cited_families = [f for f in lib_structures if f in unique_cited_families]
    sorted_citing_families = [f for f in sib_structures if f in unique_citing_families]
    
    print(f"\nCreating LIB to SIB Structure Family Sankey:")
    print(f"  LIB structure providers: {sorted_cited_families}")
    print(f"  SIB structure receivers: {sorted_citing_families}")
    print(f"  Flows above threshold {threshold}%: {len(filtered_df)}")
    print(f"  Node labels displayed: {show_node_labels}")
    
    # The unique Product/Process level values
    process_levels = ['Product', 'Process', 'both']
    
    # Create a mapping from structure families and process levels to node indices
    node_mapping = {}
    node_labels = []
    node_colors = []
    
    current_index = 0
    
    # First the cited LIB structure families (providers - on the left side)
    for family in sorted_cited_families:
        for level in process_levels:
            node_key = f"cited_{family}_{level}"
            node_mapping[node_key] = current_index
            
            if show_node_labels:
                # Create readable labels
                family_short = family.replace('LIB_', '').replace('_', ' ')
                if family == 'LIB_Anode_Graphite_based':
                    family_short = 'Graphite-based'
                elif family == 'LIB_Anode_LTO':
                    family_short = 'LTO'
                    
                node_labels.append(f"LIB {family_short}\n({level})")
            else:
                # Empty labels when show_node_labels is False
                node_labels.append("")
                
            node_colors.append(color_map.get(family, 'rgba(128,128,128,1)'))
            current_index += 1
    
    # Then the citing SIB structure families (receivers - on the right side)
    for family in sorted_citing_families:
        for level in process_levels:
            node_key = f"citing_{family}_{level}"
            node_mapping[node_key] = current_index
            
            if show_node_labels:
                # Create readable labels
                family_short = family.replace('SIB_', '').replace('_', ' ')
                if family == 'SIB_Anode_Hard_Carbon':
                    family_short = 'Hard Carbon'
                    
                node_labels.append(f"SIB {family_short}\n({level})")
            else:
                # Empty labels when show_node_labels is False
                node_labels.append("")
                
            node_colors.append(color_map.get(family, 'rgba(128,128,128,1)'))
            current_index += 1
    
    # Create lists for sources, targets and values for the Sankey diagram
    sources = []
    targets = []
    values = []
    link_colors = []
    
    # Process each row in the filtered DataFrame
    for _, row in filtered_df.iterrows():
        cited_family = row['cited_structure_family']      # LIB Provider
        citing_family = row['citing_structure_family']    # SIB Receiver
        cited_level = row['cited_product_process_level']   # Provider level
        citing_level = row['citing_product_process_level'] # Receiver level
        
        # Skip if families are not in our sorted lists
        if cited_family not in sorted_cited_families or citing_family not in sorted_citing_families:
            continue
        
        # Check if the levels are in our process level list
        if cited_level not in process_levels or citing_level not in process_levels:
            continue
        
        # Get source and target indices (Knowledge Flow: LIB Provider → SIB Receiver)
        source_key = f"cited_{cited_family}_{cited_level}"    # LIB Provider
        target_key = f"citing_{citing_family}_{citing_level}" # SIB Receiver
        
        if source_key not in node_mapping or target_key not in node_mapping:
            continue
            
        source_idx = node_mapping[source_key]
        target_idx = node_mapping[target_key]
        
        # Use Total Knowledge Percentage as the value
        value = row['total_knowledge_percentage']
        
        # Skip if value is not positive
        if value <= 0:
            continue
        
        # Use the color of the provider family (cited LIB family) for the link, with ENHANCED intensity based on provider process level
        base_color = color_map.get(cited_family, 'rgba(128,128,128,1)')
        link_color = create_structure_process_level_link_color(base_color, cited_level)
        
        sources.append(source_idx)
        targets.append(target_idx)
        values.append(value)
        link_colors.append(link_color)
    
    # Check if we have any data to plot
    if not sources:
        print(f"No valid links for {title}. Skipping.")
        return
    
    # Calculate statistics for subtitle
    max_percentage = filtered_df['total_knowledge_percentage'].max()
    mean_percentage = filtered_df['total_knowledge_percentage'].mean()
    
    # Create the Sankey diagram
    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",  # This forces Plotly to maintain node order
        node=dict(
            pad=15,
            thickness=20,
            line=dict(color="black", width=0.5),
            label=node_labels,
            color=node_colors
        ),
        link=dict(
            source=sources,
            target=targets,
            value=values,
            color=link_colors  # Color links based on provider family
        )
    )])
    
    # Create subtitle with label information
    label_info = "with labels" if show_node_labels else "without labels"
    # subtitle = f"LIB Structure Families → SIB Structure Families (Product/Process Levels)<br>Total Knowledge Percentage | {label_info} | Threshold: {threshold:.1f}% | Max: {max_percentage:.1f}% | Mean: {mean_percentage:.1f}%"
    subtitle = f"LIB Structure Families → SIB Structure Families (Product/Process Levels)"

    # Update layout
    fig.update_layout(
        title_text=f"{title}<br><sub>{subtitle}</sub>",
        font_size=14,
        width=1800,    # Increased for better visibility with more nodes
        height=1400    # Increased for better visibility with more nodes
    )
    
    # Save the figure as an HTML file
    output_dir = "sankey_diagrams"
    os.makedirs(output_dir, exist_ok=True)
    
    # Include label setting in filename
    label_suffix = "_with_labels" if show_node_labels else "_without_labels"
    output_path = os.path.join(output_dir, f"{filename}{label_suffix}_total_knowledge.html")
    fig.write_html(output_path)
    
    print(f"LIB to SIB Structure Family Sankey diagram saved to {output_path}")
    
    return fig


def create_lib_to_lib_structure_family_sankey(metrics_df, title, filename, threshold=0.0):
    """
    Creates a Sankey diagram for LIB Structure Families → LIB Structure Families
    using Total Knowledge Percentage normalization.
    """
    # Filter for LIB to LIB structure family flows
    lib_structures = ['LIB_Layered_Oxides', 'LIB_Polyanionic', 'LIB_Spinel', 'LIB_Anode_Graphite_based', 'LIB_Anode_LTO']
    
    filtered_df = metrics_df[
        (metrics_df['cited_battery_type'] == 'LIB') & 
        (metrics_df['citing_battery_type'] == 'LIB') &
        (metrics_df['cited_structure_family'].isin(lib_structures)) &
        (metrics_df['citing_structure_family'].isin(lib_structures)) &
        (metrics_df['total_knowledge_percentage'] >= threshold)
    ]
    
    if filtered_df.empty:
        print(f"No data above threshold {threshold}% for {title}. Skipping.")
        return
    
    # Get color map
    color_map = create_structure_family_colors()
    
    # Get actual structure families present in the data
    unique_cited_families = set(filtered_df['cited_structure_family'].unique())
    unique_citing_families = set(filtered_df['citing_structure_family'].unique())
    
    # Filter to only include families that are actually present, maintaining order
    sorted_cited_families = [f for f in lib_structures if f in unique_cited_families]
    sorted_citing_families = [f for f in lib_structures if f in unique_citing_families]
    
    print(f"\nCreating LIB to LIB Structure Family Sankey:")
    print(f"  LIB structure providers: {sorted_cited_families}")
    print(f"  LIB structure receivers: {sorted_citing_families}")
    print(f"  Flows above threshold {threshold}%: {len(filtered_df)}")
    
    # The unique Product/Process level values
    process_levels = ['Product', 'Process', 'both']
    
    # Create a mapping from structure families and process levels to node indices
    node_mapping = {}
    node_labels = []
    node_colors = []
    
    current_index = 0
    
    # First the cited LIB structure families (providers - on the left side)
    for family in sorted_cited_families:
        for level in process_levels:
            node_key = f"cited_{family}_{level}"
            node_mapping[node_key] = current_index
            
            # Empty labels for cleaner display
            node_labels.append("")
            node_colors.append(color_map.get(family, 'rgba(128,128,128,1)'))
            current_index += 1
    
    # Then the citing LIB structure families (receivers - on the right side)
    for family in sorted_citing_families:
        for level in process_levels:
            node_key = f"citing_{family}_{level}"
            node_mapping[node_key] = current_index
            
            # Empty labels for cleaner display
            node_labels.append("")
            node_colors.append(color_map.get(family, 'rgba(128,128,128,1)'))
            current_index += 1
    
   
    # Create lists for sources, targets and values for the Sankey diagram
    sources = []
    targets = []
    values = []
    link_colors = []
    
    # Process each row in the filtered DataFrame
    for _, row in filtered_df.iterrows():
        cited_family = row['cited_structure_family']      # LIB Provider
        citing_family = row['citing_structure_family']    # LIB Receiver
        cited_level = row['cited_product_process_level']   # Provider level
        citing_level = row['citing_product_process_level'] # Receiver level
        
        # Skip self-citations (same family and level)
        if cited_family == citing_family and cited_level == citing_level:
            continue
        
        # Skip if families are not in our sorted lists
        if cited_family not in sorted_cited_families or citing_family not in sorted_citing_families:
            continue
        
        # Check if the levels are in our process level list
        if cited_level not in process_levels or citing_level not in process_levels:
            continue
        
        # Get source and target indices (Knowledge Flow: LIB Provider → LIB Receiver)
        source_key = f"cited_{cited_family}_{cited_level}"    # LIB Provider
        target_key = f"citing_{citing_family}_{citing_level}" # LIB Receiver
        
        if source_key not in node_mapping or target_key not in node_mapping:
            continue
            
        source_idx = node_mapping[source_key]
        target_idx = node_mapping[target_key]
        
        # Use Total Knowledge Percentage as the value
        value = row['total_knowledge_percentage']
        
        # Skip if value is not positive
        if value <= 0:
            continue
        
        # Use the color of the provider family (cited LIB family) for the link
        base_color = color_map.get(cited_family, 'rgba(128,128,128,1)')
        link_color = create_structure_process_level_link_color(base_color, cited_level)
        
        sources.append(source_idx)
        targets.append(target_idx)
        values.append(value)
        link_colors.append(link_color)
    
    # Check if we have any data to plot
    if not sources:
        print(f"No valid links for {title}. Skipping.")
        return
    
    # Calculate statistics for subtitle
    max_percentage = filtered_df['total_knowledge_percentage'].max()
    mean_percentage = filtered_df['total_knowledge_percentage'].mean()
    
    # Create the Sankey diagram
    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=15,
            thickness=20,
            line=dict(color="black", width=0.5),
            label=node_labels,
            color=node_colors
        ),
        link=dict(
            source=sources,
            target=targets,
            value=values,
            color=link_colors
        )
    )])
    
    # Create subtitle
    # subtitle = f"SIB Structure Families → SIB Structure Families (Product/Process Levels)<br>Total Knowledge Percentage | Threshold: {threshold:.1f}% | Max: {max_percentage:.1f}% | Mean: {mean_percentage:.1f}%"
    subtitle = f"SIB Structure Families → SIB Structure Families (Product/Process Levels)"
    
    # Update layout
    fig.update_layout(
        title_text=f"{title}<br><sub>{subtitle}</sub>",
        font_size=14,
        width=1800,
        height=1400
    )
    
    # Save the figure as an HTML file
    output_dir = "sankey_diagrams"
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"{filename}_total_knowledge.html")
    fig.write_html(output_path)
    
    print(f"LIB to LIB Structure Family Sankey diagram saved to {output_path}")
    
    return fig


def create_sib_to_sib_structure_family_sankey(metrics_df, title, filename, threshold=0.0):
    """
    Creates a Sankey diagram for SIB Structure Families → SIB Structure Families
    using Total Knowledge Percentage normalization.
    """
    # Filter for SIB to SIB structure family flows
    sib_structures = ['SIB_Layered_Oxides', 'SIB_Polyanionic', 'SIB_PBA', 'SIB_Anode_Hard_Carbon']
    
    filtered_df = metrics_df[
        (metrics_df['cited_battery_type'] == 'SIB') & 
        (metrics_df['citing_battery_type'] == 'SIB') &
        (metrics_df['cited_structure_family'].isin(sib_structures)) &
        (metrics_df['citing_structure_family'].isin(sib_structures)) &
        (metrics_df['total_knowledge_percentage'] >= threshold)
    ]
    
    if filtered_df.empty:
        print(f"No data above threshold {threshold}% for {title}. Skipping.")
        return
    
    # Get color map
    color_map = create_structure_family_colors()
    
    # Get actual structure families present in the data
    unique_cited_families = set(filtered_df['cited_structure_family'].unique())
    unique_citing_families = set(filtered_df['citing_structure_family'].unique())
    
    # Filter to only include families that are actually present, maintaining order
    sorted_cited_families = [f for f in sib_structures if f in unique_cited_families]
    sorted_citing_families = [f for f in sib_structures if f in unique_citing_families]
    
    print(f"\nCreating SIB to SIB Structure Family Sankey:")
    print(f"  SIB structure providers: {sorted_cited_families}")
    print(f"  SIB structure receivers: {sorted_citing_families}")
    print(f"  Flows above threshold {threshold}%: {len(filtered_df)}")
    
    # The unique Product/Process level values
    process_levels = ['Product', 'Process', 'both']
    
    # Create a mapping from structure families and process levels to node indices
    node_mapping = {}
    node_labels = []
    node_colors = []
    
    current_index = 0
    
    # First the cited SIB structure families (providers - on the left side)
    for family in sorted_cited_families:
        for level in process_levels:
            node_key = f"cited_{family}_{level}"
            node_mapping[node_key] = current_index
            
            # Empty labels for cleaner display
            node_labels.append("")
            node_colors.append(color_map.get(family, 'rgba(128,128,128,1)'))
            current_index += 1
    
    # Then the citing SIB structure families (receivers - on the right side)
    for family in sorted_citing_families:
        for level in process_levels:
            node_key = f"citing_{family}_{level}"
            node_mapping[node_key] = current_index
            
            # Empty labels for cleaner display
            node_labels.append("")
            node_colors.append(color_map.get(family, 'rgba(128,128,128,1)'))
            current_index += 1
    
    # Create lists for sources, targets and values for the Sankey diagram
    sources = []
    targets = []
    values = []
    link_colors = []
    
    # Process each row in the filtered DataFrame
    for _, row in filtered_df.iterrows():
        cited_family = row['cited_structure_family']      # SIB Provider
        citing_family = row['citing_structure_family']    # SIB Receiver
        cited_level = row['cited_product_process_level']   # Provider level
        citing_level = row['citing_product_process_level'] # Receiver level
        
        # Skip self-citations (same family and level)
        if cited_family == citing_family and cited_level == citing_level:
            continue
        
        # Skip if families are not in our sorted lists
        if cited_family not in sorted_cited_families or citing_family not in sorted_citing_families:
            continue
        
        # Check if the levels are in our process level list
        if cited_level not in process_levels or citing_level not in process_levels:
            continue
        
        # Get source and target indices (Knowledge Flow: SIB Provider → SIB Receiver)
        source_key = f"cited_{cited_family}_{cited_level}"    # SIB Provider
        target_key = f"citing_{citing_family}_{citing_level}" # SIB Receiver
        
        if source_key not in node_mapping or target_key not in node_mapping:
            continue
            
        source_idx = node_mapping[source_key]
        target_idx = node_mapping[target_key]
        
        # Use Total Knowledge Percentage as the value
        value = row['total_knowledge_percentage']
        
        # Skip if value is not positive
        if value <= 0:
            continue
        
        # Use the color of the provider family (cited SIB family) for the link
        base_color = color_map.get(cited_family, 'rgba(128,128,128,1)')
        link_color = create_structure_process_level_link_color(base_color, cited_level)
        
        sources.append(source_idx)
        targets.append(target_idx)
        values.append(value)
        link_colors.append(link_color)
    
    # Check if we have any data to plot
    if not sources:
        print(f"No valid links for {title}. Skipping.")
        return
    
    # Calculate statistics for subtitle
    max_percentage = filtered_df['total_knowledge_percentage'].max()
    mean_percentage = filtered_df['total_knowledge_percentage'].mean()
    
    # Create the Sankey diagram
    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=15,
            thickness=20,
            line=dict(color="black", width=0.5),
            label=node_labels,
            color=node_colors
        ),
        link=dict(
            source=sources,
            target=targets,
            value=values,
            color=link_colors
        )
    )])
    
    # Create subtitle
    # subtitle = f"SIB Structure Families → SIB Structure Families (Product/Process Levels)<br>Total Knowledge Percentage | Threshold: {threshold:.1f}% | Max: {max_percentage:.1f}% | Mean: {mean_percentage:.1f}%"
    subtitle = f"SIB Structure Families → SIB Structure Families (Product/Process Levels)"

    # Update layout
    fig.update_layout(
        title_text=f"{title}<br><sub>{subtitle}</sub>",
        font_size=14,
        width=1800,
        height=1400
    )
    
    # Save the figure as an HTML file
    output_dir = "sankey_diagrams"
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"{filename}_total_knowledge.html")
    fig.write_html(output_path)
    
    print(f"SIB to SIB Structure Family Sankey diagram saved to {output_path}")
    
    return fig


def print_structure_family_total_knowledge_summary(metrics_df):
    """Print summary statistics for the Structure Family Total Knowledge analysis."""
    
    print("\n" + "="*80)
    print("STRUCTURE FAMILY TOTAL KNOWLEDGE ANALYSIS SUMMARY")
    print("="*80)
    
    print(f"\n📊 OVERALL STATISTICS:")
    print(f"   Total structure family-process pairs with citations: {len(metrics_df):4d}")
    print(f"   Maximum Total Knowledge Percentage: {metrics_df['total_knowledge_percentage'].max():7.2f}%")
    print(f"   Mean Total Knowledge Percentage: {metrics_df['total_knowledge_percentage'].mean():7.2f}%")
    print(f"   Median Total Knowledge Percentage: {metrics_df['total_knowledge_percentage'].median():7.2f}%")
    
    # LIB to SIB flows specifically
    lib_to_sib = metrics_df[
        (metrics_df['cited_battery_type'] == 'LIB') & 
        (metrics_df['citing_battery_type'] == 'SIB')
    ]
    
    if not lib_to_sib.empty:
        print(f"\n🔄 LIB → SIB STRUCTURE FAMILY FLOWS:")
        print(f"   Total LIB → SIB flows: {len(lib_to_sib):4d}")
        print(f"   Maximum LIB → SIB percentage: {lib_to_sib['total_knowledge_percentage'].max():7.2f}%")
        print(f"   Mean LIB → SIB percentage: {lib_to_sib['total_knowledge_percentage'].mean():7.2f}%")
        
        # Top LIB → SIB flows
        print(f"\n🔝 TOP 10 LIB → SIB KNOWLEDGE FLOWS:")
        top_lib_to_sib = lib_to_sib.head(10)
        for i, (_, row) in enumerate(top_lib_to_sib.iterrows(), 1):
            provider = row['cited_structure_family'].replace('LIB_', '')
            receiver = row['citing_structure_family'].replace('SIB_', '')
            print(f"   {i:2d}. {provider:15} ({row['cited_product_process_level']:7}) → "
                  f"{receiver:15} ({row['citing_product_process_level']:7}) | "
                  f"Share: {row['total_knowledge_percentage']:6.2f}% | Citations: {row['citation_count']:3d}")
    
    # Structure family coverage
    print(f"\n📈 STRUCTURE FAMILY COVERAGE:")
    lib_providers = metrics_df[metrics_df['cited_battery_type'] == 'LIB']['cited_structure_family'].unique()
    sib_receivers = metrics_df[metrics_df['citing_battery_type'] == 'SIB']['citing_structure_family'].unique()
    
    print(f"   LIB provider families: {len(lib_providers)} - {sorted(lib_providers)}")
    print(f"   SIB receiver families: {len(sib_receivers)} - {sorted(sib_receivers)}")
    
    # Process level distribution
    print(f"\n🏭 PRODUCT/PROCESS LEVEL DISTRIBUTION:")
    cited_levels = metrics_df['cited_product_process_level'].value_counts()
    citing_levels = metrics_df['citing_product_process_level'].value_counts()
    
    print(f"   Provider process levels:")
    for level, count in cited_levels.items():
        print(f"     {level:10}: {count:4d} pairs")
    
    print(f"   Receiver process levels:")
    for level, count in citing_levels.items():
        print(f"     {level:10}: {count:4d} pairs")
    
    print("="*80)

# Main execution function
def main_structure_family_total_knowledge_analysis():
    """
    Main function to create all Structure Family level Sankey diagrams with Total Knowledge normalization.
    """
    print("Starting Comprehensive Structure Family Total Knowledge Analysis...")
    
    try:
        # Check if required variables exist
        print("Checking for required data...")
        
        # Test if variables exist
        test_structure_citations = structure_citations_df
        test_combined = combined_df
        print("✅ Found required datasets")
        
        # Step 1: Add product/process levels to structure citations
        print("\n📊 Step 1: Adding product/process levels to structure citations...")
        structure_citations_enhanced = add_product_process_levels_to_structure_citations(
            structure_citations_df, combined_df
        )
        
        # Step 2: Create structure family aggregated data for total backward citations
        print("\n📊 Step 2: Creating structure family aggregated data...")
        family_aggregated_df = create_structure_family_aggregated_df(combined_df)
        
        # Step 3: Create total knowledge metrics
        print("\n📊 Step 3: Creating Total Knowledge Percentage metrics...")
        structure_total_knowledge_metrics = create_structure_family_total_knowledge_metrics(
            structure_citations_enhanced, family_aggregated_df
        )
        
        # Step 4: Print summary statistics
        print_structure_family_total_knowledge_summary(structure_total_knowledge_metrics)
        
        # Step 5: Create all three Sankey diagrams
        print("\n📊 Step 5: Creating Structure Family Sankey diagrams...")
        
        # 5a: LIB → SIB Sankey diagram
        print("\n🔄 Creating LIB → SIB Structure Family Sankey diagram...")
        fig_lib_to_sib = create_lib_to_sib_structure_family_sankey(
            structure_total_knowledge_metrics,
            "LIB Structure Families to SIB Structure Families Knowledge Transfer",
            "LIB_to_SIB_Structure_Families_Total_Knowledge_Sankey",
            threshold=0.0
        )
        
        # 5b: LIB → LIB Sankey diagram
        print("\n🔄 Creating LIB → LIB Structure Family Sankey diagram...")
        fig_lib_to_lib = create_lib_to_lib_structure_family_sankey(
            structure_total_knowledge_metrics,
            "LIB Structure Families Internal Knowledge Transfer",
            "LIB_to_LIB_Structure_Families_Total_Knowledge_Sankey",
            threshold=0.0
        )
        
        # 5c: SIB → SIB Sankey diagram
        print("\n🔄 Creating SIB → SIB Structure Family Sankey diagram...")
        fig_sib_to_sib = create_sib_to_sib_structure_family_sankey(
            structure_total_knowledge_metrics,
            "SIB Structure Families Internal Knowledge Transfer",
            "SIB_to_SIB_Structure_Families_Total_Knowledge_Sankey",
            threshold=0.0
        )
        
        print(f"\n✅ COMPREHENSIVE ANALYSIS COMPLETE!")
        print(f"📁 Files created:")
        print(f"   - structure_family_total_knowledge_metrics.csv")
        print(f"   - sankey_diagrams/LIB_to_SIB_Structure_Families_Total_Knowledge_Sankey_total_knowledge.html")
        print(f"   - sankey_diagrams/LIB_to_LIB_Structure_Families_Total_Knowledge_Sankey_total_knowledge.html")
        print(f"   - sankey_diagrams/SIB_to_SIB_Structure_Families_Total_Knowledge_Sankey_total_knowledge.html")
        
    except NameError as e:
        print(f"❌ Error: Required data not found - {e}")
        print("Please ensure the following variables are available:")
        print("   - structure_citations_df")
        print("   - combined_df")
        print("   - create_structure_family_aggregated_df function")
        
    except Exception as e:
        print(f"❌ Error during analysis: {e}")
        import traceback
        traceback.print_exc()

# Run the analysis
if __name__ == "__main__":
    main_structure_family_total_knowledge_analysis()

print("\n" + "="*80)
print("STRUCTURE FAMILY TOTAL KNOWLEDGE INTERPRETATION GUIDE")
print("="*80)

print("""
🎯 STRUCTURE FAMILY TOTAL KNOWLEDGE ANALYSIS:
   This analysis shows knowledge flows from LIB Structure Families to SIB Structure Families
   using Total Knowledge Normalization (consistent with corrected heatmap methodology).

📊 METHODOLOGY - TOTAL KNOWLEDGE NORMALIZATION:
   Formula: Total_Knowledge_Percentage(i→j) = Citations(i→j) / Total_Backward_Citations(j) × 100%
   Where:
   • i = Knowledge Provider (LIB structure family + process level)
   • j = Knowledge Receiver (SIB structure family + process level)  
   • Total_Backward_Citations(j) = ALL citations received by j from ALL technologies

🏗️ STRUCTURE FAMILY DEFINITIONS:
   LIB Providers (5 families):
   • LIB Layered Oxides (LCO, NMC, NCA cathodes)
   • LIB Polyanionic (LFP cathodes)
   • LIB Spinel (LMO cathodes)
   • LIB Graphite-based (Graphite, Silicon/Carbon anodes)
   • LIB LTO (Lithium Titanate anodes)
   
   SIB Receivers (4 families):
   • SIB Layered Oxides (NFM, CFM, NMO cathodes)
   • SIB Polyanionic (NVPF, NFPP/NFP cathodes)
   • SIB PBA (Fe-PBA, Mn-PBA cathodes)
   • SIB Hard Carbon (Hard Carbon anodes)

🎨 SANKEY DIAGRAM ELEMENTS:
   • Left Side: 15 LIB provider nodes (5 families × 3 process levels)
   • Right Side: 12 SIB receiver nodes (4 families × 3 process levels)
   • Links: Proportional to Total Knowledge Percentage
   • Node Colors: Based on structure family type
   • Link Colors: Provider family color with process-level intensity

🔍 LINK COLOR INTENSITY (Provider Process Level):
   • Light flows: Product-focused LIB providers (40% opacity + lightened)
   • Medium flows: Process-focused LIB providers (60% opacity + original)
   • Dark flows: Comprehensive LIB providers (80% opacity + darkened)

📈 PERCENTAGE INTERPRETATION:
   HIGH PERCENTAGES (>5%):
   • Strong dependency of SIB family on specific LIB knowledge
   • Core technological relationship in battery innovation
   • Critical knowledge transfer pathway

   MODERATE PERCENTAGES (1-5%):
   • Significant but not dominant knowledge relationship
   • Secondary technological connection
   • Balanced knowledge integration

   LOW PERCENTAGES (<1%):
   • Minor knowledge relationship
   • SIB family primarily builds on non-LIB knowledge
   • Emerging or weak technological connection

💡 STRATEGIC PATTERNS:

CATHODE-TO-CATHODE FLOWS:
• LIB Layered Oxides → SIB Layered Oxides: Direct structural knowledge transfer
• LIB Polyanionic → SIB Polyanionic: Chemistry-specific knowledge sharing
• Cross-chemistry flows: Novel material development approaches

ANODE-TO-ANODE FLOWS:
• LIB Graphite-based → SIB Hard Carbon: Carbon-based material knowledge
• LIB LTO → SIB Hard Carbon: Advanced anode material insights

CROSS-ELECTRODE FLOWS:
• LIB Cathodes → SIB Anodes: System integration knowledge
• LIB Anodes → SIB Cathodes: Cell-level design insights

PROCESS-LEVEL INSIGHTS:
• Product → Product: Material design and composition knowledge
• Process → Process: Manufacturing and scaling expertise
• Both → Any: Comprehensive technological knowledge transfer

🎯 BUSINESS APPLICATIONS:

TECHNOLOGY STRATEGY:
• High-percentage flows: Monitor critical LIB knowledge dependencies
• Low SIB percentages: Opportunities for unique SIB innovation paths
• Process-level patterns: Guide R&D focus areas (product vs. process)

COMPETITIVE INTELLIGENCE:
• Strong LIB → SIB flows: Areas where LIB experience provides advantage
• Weak flows: Potential SIB differentiation opportunities
• Cross-level flows: Integration and scaling insights

PARTNERSHIP STRATEGY:
• High-percentage relationships: Strategic collaboration targets
• Balanced dependencies: Robust, diversified knowledge sourcing
• Process-specific flows: Operational partnership opportunities

IP STRATEGY:
• Critical knowledge flows: High-value patent landscape areas
• Emerging flows: Early-stage IP positioning opportunities
• Cross-family flows: Broad technological IP portfolios

INNOVATION ROADMAP:
• Product-heavy flows: Focus on material innovation
• Process-heavy flows: Emphasize manufacturing development
• Balanced flows: Integrated product-process innovation

This analysis reveals how SIB technologies leverage LIB knowledge foundations
while identifying opportunities for distinct SIB innovation pathways.
""")

print("="*80)
#%%


#==================================================================================
# Modified Structure Family Level Sankey Diagram with Correct Normalization
# Provider side: Aggregated structure families (no process level split)
# Receiver side: Structure families split by product/process levels
# Correct backward citations calculation per structure + process level
#==================================================================================

import pandas as pd
import plotly.graph_objects as go
import numpy as np
import os
from plotly.subplots import make_subplots

def calculate_structure_process_backward_citations(combined_df):
    """
    Calculate total backward citations per structure family + process level combination
    from combined_df using material mappings.
    """
    print("Calculating structure + process level backward citations...")
    
    # Define structure family mappings
    structure_mappings = {
        'LIB_Layered_Oxides': ['LCO', 'NMC', 'NCA'],
        'LIB_Polyanionic': ['LFP'],
        'LIB_Spinel': ['LMO'],
        'LIB_Anode_Graphite_based': ['Graphite', 'Silicon/Carbon'],
        'LIB_Anode_LTO': ['LTO'],
        'SIB_Layered_Oxides': ['NMO', 'CFM', 'NFM'],
        'SIB_Polyanionic': ['NFPP/NFP', 'NVPF'],
        'SIB_PBA': ['Fe-PBA', 'Mn-PBA'],
        'SIB_Anode_Hard_Carbon': ['Hard Carbon']
    }
    
    # Prepare combined_df
    combined_enhanced = combined_df.copy()
    
    # Standardize Product-Process-Level
    combined_enhanced['Product-Process-Level'] = combined_enhanced['Product-Process-Level'].replace('Product, Process', 'both')
    
    # Create structure family assignments
    structure_assignments = []
    
    for idx, row in combined_enhanced.iterrows():
        if idx % 1000 == 0:
            print(f"Processing patent {idx+1}/{len(combined_enhanced)}")
        
        node_num = row['node_num']
        battery_type = row['battery_type']
        process_level = row['Product-Process-Level']
        backward_citations = row['family_level_cited_patent_counts']
        
        # Get materials from both cathode and anode columns
        cathode_materials = row.get('Cathode Material', [])
        anode_materials = row.get('Anode Material', [])
        
        # Convert to lists if they're strings or other types
        if isinstance(cathode_materials, str):
            cathode_materials = [cathode_materials]
        elif not isinstance(cathode_materials, list):
            cathode_materials = []
            
        if isinstance(anode_materials, str):
            anode_materials = [anode_materials]
        elif not isinstance(anode_materials, list):
            anode_materials = []
        
        # Combine all materials for this patent
        all_materials = set(cathode_materials + anode_materials)
        
        # Find matching structure families
        assigned_structures = []
        for structure_family, structure_materials in structure_mappings.items():
            # Check if any of the structure's materials are in this patent's materials
            if any(material in all_materials for material in structure_materials):
                assigned_structures.append(structure_family)
        
        # Create entries for each assigned structure
        for structure_family in assigned_structures:
            structure_assignments.append({
                'node_num': node_num,
                'battery_type': battery_type,
                'structure_family': structure_family,
                'process_level': process_level,
                'backward_citations': backward_citations
            })
    
    # Convert to DataFrame
    structure_df = pd.DataFrame(structure_assignments)
    
    # Group by structure family + process level and sum backward citations
    backward_citations_summary = structure_df.groupby(['structure_family', 'process_level']).agg({
        'backward_citations': 'sum',
        'node_num': 'count'  # Count of patents
    }).reset_index()
    
    backward_citations_summary.rename(columns={'node_num': 'patent_count'}, inplace=True)
    
    print(f"\nBackward citations summary:")
    print(f"Total structure-process combinations: {len(backward_citations_summary)}")
    print(f"Sample entries:")
    print(backward_citations_summary.head(10))
    
    # Create lookup dictionary
    backward_citations_lookup = {}
    for _, row in backward_citations_summary.iterrows():
        key = (row['structure_family'], row['process_level'])
        backward_citations_lookup[key] = row['backward_citations']
    
    # Save detailed results
    structure_df.to_csv('structure_patent_assignments.csv', index=False)
    backward_citations_summary.to_csv('structure_process_backward_citations.csv', index=False)
    
    print(f"Detailed assignments saved to: structure_patent_assignments.csv")
    print(f"Summary saved to: structure_process_backward_citations.csv")
    
    return backward_citations_lookup, backward_citations_summary

def add_product_process_levels_to_structure_citations(structure_citations_df, combined_df):
    """Add product/process level information to structure_citations_df by matching patents."""
    print("Adding product/process level information to structure citations...")
    
    # Create lookup dictionaries from combined_df
    patent_to_product_process = combined_df.set_index('node_num')['Product-Process-Level'].to_dict()
    
    # Add product/process levels to structure_citations_df
    structure_citations_enhanced = structure_citations_df.copy()
    
    # Add cited (provider) product/process level
    structure_citations_enhanced['cited_product_process_level'] = structure_citations_enhanced['cited_patent'].map(patent_to_product_process)
    
    # Add citing (receiver) product/process level  
    structure_citations_enhanced['citing_product_process_level'] = structure_citations_enhanced['citing_patent'].map(patent_to_product_process)
    
    # Standardize "Product, Process" to "both"
    structure_citations_enhanced['cited_product_process_level'] = structure_citations_enhanced['cited_product_process_level'].replace('Product, Process', 'both')
    structure_citations_enhanced['citing_product_process_level'] = structure_citations_enhanced['citing_product_process_level'].replace('Product, Process', 'both')
    
    # Remove rows with missing product/process information
    structure_citations_enhanced = structure_citations_enhanced.dropna(subset=['cited_product_process_level', 'citing_product_process_level'])
    
    print(f"Final dataset: {len(structure_citations_enhanced)} citations with complete product/process information")
    
    return structure_citations_enhanced

def create_corrected_receiver_level_knowledge_metrics(structure_citations_enhanced, backward_citations_lookup):
    """
    Create citation metrics with correct normalization per receiver structure + process level.
    """
    print("Creating corrected receiver-level knowledge metrics...")
    
    # Group by receiver (structure + process level) and provider (structure only)
    metrics_data = []
    
    # Get unique receiver combinations (structure + process level)
    receiver_combinations = structure_citations_enhanced[['citing_structure_family', 'citing_battery_type', 'citing_product_process_level']].drop_duplicates()
    
    print(f"Processing {len(receiver_combinations)} unique receiver combinations...")
    
    for _, receiver_row in receiver_combinations.iterrows():
        receiver_family = receiver_row['citing_structure_family']
        receiver_battery_type = receiver_row['citing_battery_type']
        receiver_process_level = receiver_row['citing_product_process_level']
        
        # Get total backward citations for this specific receiver combination from lookup
        receiver_key = (receiver_family, receiver_process_level)
        total_backward_citations = backward_citations_lookup.get(receiver_key, 0)
        
        if total_backward_citations == 0:
            print(f"Warning: No backward citations found for {receiver_family} ({receiver_process_level})")
            continue
        
        # Get ALL citations for this specific receiver combination
        receiver_citations = structure_citations_enhanced[
            (structure_citations_enhanced['citing_structure_family'] == receiver_family) &
            (structure_citations_enhanced['citing_product_process_level'] == receiver_process_level)
        ]
        
        if len(receiver_citations) == 0:
            continue
        
        # Group by provider structure family, excluding same structure for intra-battery flows
        if receiver_battery_type == 'LIB':
            # For LIB receivers, exclude same LIB structure
            other_citations = receiver_citations[
                ~((receiver_citations['cited_battery_type'] == 'LIB') & 
                  (receiver_citations['cited_structure_family'] == receiver_family))
            ]
        else:  # SIB
            # For SIB receivers, exclude same SIB structure
            other_citations = receiver_citations[
                ~((receiver_citations['cited_battery_type'] == 'SIB') & 
                  (receiver_citations['cited_structure_family'] == receiver_family))
            ]
        
        # Group by provider structure family (aggregated across all process levels)
        provider_groups = other_citations.groupby(['cited_structure_family', 'cited_battery_type']).agg({
            'individual_citation_count': 'sum'
        }).reset_index()
        
        # Create metrics for each provider
        for _, provider_row in provider_groups.iterrows():
            provider_family = provider_row['cited_structure_family']
            provider_battery_type = provider_row['cited_battery_type']
            citation_count = provider_row['individual_citation_count']
            
            # Calculate knowledge percentage with CORRECT normalization
            knowledge_percentage = (citation_count / total_backward_citations) * 100
            
            metrics_data.append({
                'provider_structure_family': provider_family,
                'provider_battery_type': provider_battery_type,
                'receiver_structure_family': receiver_family,
                'receiver_battery_type': receiver_battery_type,
                'receiver_process_level': receiver_process_level,
                'citation_count': citation_count,
                'total_backward_citations': total_backward_citations,
                'knowledge_percentage': knowledge_percentage
            })
    
    metrics_df = pd.DataFrame(metrics_data)
    
    print(f"Created {len(metrics_df)} provider-receiver relationships")
    
    # Calculate summary percentages for each receiver node
    receiver_summary = metrics_df.groupby(['receiver_structure_family', 'receiver_battery_type', 'receiver_process_level']).agg({
        'knowledge_percentage': 'sum',
        'citation_count': 'sum',
        'total_backward_citations': 'first'
    }).reset_index()
    
    receiver_summary.rename(columns={'knowledge_percentage': 'total_from_others_percentage'}, inplace=True)
    
    print(f"\nReceiver summary statistics:")
    print(f"Max percentage from others: {receiver_summary['total_from_others_percentage'].max():.2f}%")
    print(f"Mean percentage from others: {receiver_summary['total_from_others_percentage'].mean():.2f}%")
    
    # Save results
    metrics_df.to_csv('corrected_receiver_level_knowledge_metrics.csv', index=False)
    receiver_summary.to_csv('receiver_summary_percentages.csv', index=False)
    
    print("Corrected metrics saved to: corrected_receiver_level_knowledge_metrics.csv")
    print("Receiver summary saved to: receiver_summary_percentages.csv")
    
    return metrics_df, receiver_summary

def create_filtered_receiver_summary(metrics_df, battery_filter_provider, battery_filter_receiver):
    """
    Create receiver summary with only specific provider-receiver combinations.
    """
    # Filter für spezifischen Flow-Typ
    if battery_filter_provider == battery_filter_receiver:
        # Intra-battery flow (z.B. SIB→SIB): Exclude same structure
        filtered_metrics = metrics_df[
            (metrics_df['provider_battery_type'] == battery_filter_provider) & 
            (metrics_df['receiver_battery_type'] == battery_filter_receiver)
        ]
    else:
        # Inter-battery flow (z.B. LIB→SIB): Include all
        filtered_metrics = metrics_df[
            (metrics_df['provider_battery_type'] == battery_filter_provider) & 
            (metrics_df['receiver_battery_type'] == battery_filter_receiver)
        ]
    
    # Gruppiere und summiere nur gefilterte Flüsse
    filtered_summary = filtered_metrics.groupby([
        'receiver_structure_family', 
        'receiver_battery_type', 
        'receiver_process_level'
    ]).agg({
        'knowledge_percentage': 'sum',
        'total_backward_citations': 'first'
    }).reset_index()
    
    filtered_summary.rename(columns={'knowledge_percentage': 'filtered_percentage'}, inplace=True)
    
    return filtered_summary

def create_structure_family_colors():
    """Create color mappings for structure families."""
    return {
        # LIB Structure Families
        'LIB_Layered_Oxides': 'rgba(255,0,0,0.8)',      # Red
        'LIB_Polyanionic': 'rgba(0,0,255,0.8)',         # Blue  
        'LIB_Spinel': 'rgba(0,255,255,0.8)',            # Cyan
        'LIB_Anode_Graphite_based': 'rgba(102,102,102,0.8)', # Gray
        'LIB_Anode_LTO': 'rgba(255,165,0,0.8)',         # Orange
        
        # SIB Structure Families
        'SIB_Layered_Oxides': 'rgba(255,102,102,0.8)',  # Light Red
        'SIB_Polyanionic': 'rgba(0,102,255,0.8)',       # Medium Blue
        'SIB_PBA': 'rgba(0,204,0,0.8)',                 # Green
        'SIB_Anode_Hard_Carbon': 'rgba(153,153,153,0.8)' # Light Gray
    }

def print_receiver_percentage_tables(metrics_df):
    """
    Print percentage tables for each receiver by flow type in console.
    """
    print("\n" + "="*80)
    print("RECEIVER PERCENTAGE TABLES")
    print("="*80)
    
    # Define flow types
    flow_types = [
        ('LIB', 'LIB', 'LIB → LIB Knowledge Transfer'),
        ('LIB', 'SIB', 'LIB → SIB Knowledge Transfer'), 
        ('SIB', 'SIB', 'SIB → SIB Knowledge Transfer')
    ]
    
    for provider_type, receiver_type, flow_name in flow_types:
        print(f"\n🔄 {flow_name.upper()}:")
        print("-" * 60)
        
        # Create filtered summary for this flow type
        filtered_summary = create_filtered_receiver_summary(metrics_df, provider_type, receiver_type)
        
        if filtered_summary.empty:
            print("   No data available for this flow type.")
            continue
        
        # Sort by percentage (descending)
        filtered_summary_sorted = filtered_summary.sort_values('filtered_percentage', ascending=False)
        
        print(f"{'Receiver Structure':<25} {'Process Level':<12} {'Percentage':<12} {'Citations':<10}")
        print("-" * 60)
        
        for _, row in filtered_summary_sorted.iterrows():
            structure = row['receiver_structure_family'].replace('_', ' ')
            process_level = row['receiver_process_level']
            percentage = row['filtered_percentage']
            citations = row['total_backward_citations']
            
            print(f"{structure:<25} {process_level:<12} {percentage:>8.1f}%    {citations:>8.0f}")
        
        # Summary statistics
        max_pct = filtered_summary['filtered_percentage'].max()
        mean_pct = filtered_summary['filtered_percentage'].mean()
        print("-" * 60)
        print(f"Maximum: {max_pct:.1f}% | Average: {mean_pct:.1f}% | Total entries: {len(filtered_summary)}")
    
    print("\n" + "="*80)


def create_lib_to_sib_corrected_sankey(metrics_df, receiver_summary, title, filename, threshold=0.0):
   """
   Creates corrected Sankey: LIB Structure Families (aggregated) → SIB Structure Families (by process level)
   with percentage labels next to receiver nodes.
   """
   # Filter for LIB to SIB flows
   lib_structures = ['LIB_Layered_Oxides', 'LIB_Polyanionic', 'LIB_Spinel', 'LIB_Anode_Graphite_based', 'LIB_Anode_LTO']
   sib_structures = ['SIB_Layered_Oxides', 'SIB_Polyanionic', 'SIB_PBA', 'SIB_Anode_Hard_Carbon']
   
   filtered_df = metrics_df[
       (metrics_df['provider_battery_type'] == 'LIB') & 
       (metrics_df['receiver_battery_type'] == 'SIB') &
       (metrics_df['provider_structure_family'].isin(lib_structures)) &
       (metrics_df['receiver_structure_family'].isin(sib_structures)) &
       (metrics_df['knowledge_percentage'] >= threshold)
   ]
   
   if filtered_df.empty:
       print(f"No data above threshold {threshold}% for {title}. Skipping.")
       return
   
   # Create filtered summary for LIB→SIB flows only
   filtered_summary = create_filtered_receiver_summary(metrics_df, 'LIB', 'SIB')
   
   color_map = create_structure_family_colors()
   process_levels = ['Product', 'Process', 'both']
   
   # Get unique families present in data
   unique_providers = sorted([f for f in lib_structures if f in filtered_df['provider_structure_family'].unique()])
   unique_receivers = sorted([f for f in sib_structures if f in filtered_df['receiver_structure_family'].unique()])
   
   print(f"\nCreating LIB → SIB Corrected Sankey:")
   print(f"  LIB providers (aggregated): {unique_providers}")
   print(f"  SIB receivers (by process level): {unique_receivers}")
   print(f"  Flows above threshold {threshold}%: {len(filtered_df)}")
   
   # Create node mapping
   node_mapping = {}
   node_labels = []
   node_colors = []
   current_index = 0
   
   # Left side: LIB providers (aggregated, no process level split)
   for family in unique_providers:
       node_mapping[f"provider_{family}"] = current_index
       node_labels.append("")
       node_colors.append(color_map.get(family, 'rgba(128,128,128,0.8)'))
       current_index += 1
   
   # Right side: SIB receivers (split by process level) with LIB-only percentage labels
   for family in unique_receivers:
       for level in process_levels:
           node_mapping[f"receiver_{family}_{level}"] = current_index
           
           # Get LIB-only percentage from filtered_summary
           summary_row = filtered_summary[
               (filtered_summary['receiver_structure_family'] == family) &
               (filtered_summary['receiver_process_level'] == level)
           ]
           
           if not summary_row.empty:
               percentage = summary_row.iloc[0]['filtered_percentage']
               # node_labels.append(f"[{percentage:.1f}%]")
               node_labels.append("")
           else:
               # node_labels.append("[0.0%]")
               node_labels.append("")
           
           node_colors.append(color_map.get(family, 'rgba(128,128,128,0.8)'))
           current_index += 1
   
   # Create links
   sources = []
   targets = []
   values = []
   link_colors = []
   
   for _, row in filtered_df.iterrows():
       provider_family = row['provider_structure_family']
       receiver_family = row['receiver_structure_family']
       receiver_level = row['receiver_process_level']
       
       if receiver_level not in process_levels:
           continue
       
       provider_key = f"provider_{provider_family}"
       receiver_key = f"receiver_{receiver_family}_{receiver_level}"
       
       if provider_key not in node_mapping or receiver_key not in node_mapping:
           continue
       
       source_idx = node_mapping[provider_key]
       target_idx = node_mapping[receiver_key]
       value = row['knowledge_percentage']
       
       if value <= 0:
           continue
       
       sources.append(source_idx)
       targets.append(target_idx)
       values.append(value)
       link_colors.append(color_map.get(provider_family, 'rgba(128,128,128,0.6)'))
   
   if not sources:
       print(f"No valid links for {title}. Skipping.")
       return
   
   # Create Sankey diagram
   fig = go.Figure(data=[go.Sankey(
       arrangement="snap",
       node=dict(
           pad=15,
           thickness=20,
           line=dict(color="black", width=0.5),
           label=node_labels,
           color=node_colors
       ),
       link=dict(
           source=sources,
           target=targets,
           value=values,
           color=link_colors
       )
   )])
   
   subtitle = f"LIB Structure Families → SIB Structure Families (by Process Level) - Corrected Normalization"
   
   fig.update_layout(
       title_text=f"{title}<br><sub>{subtitle}</sub>",
       font_size=14,
       width=1800,
       height=1200
   )
   
   # Save figure
   output_dir = "sankey_diagrams"
   os.makedirs(output_dir, exist_ok=True)
   output_path = os.path.join(output_dir, f"{filename}_corrected.html")
   fig.write_html(output_path)
   
   print(f"LIB → SIB Corrected Sankey saved to {output_path}")
   return fig

def create_lib_to_lib_corrected_sankey(metrics_df, receiver_summary, title, filename, threshold=0.0):
   """
   Creates corrected Sankey: LIB Structure Families (aggregated) → LIB Structure Families (by process level)
   """
   lib_structures = ['LIB_Layered_Oxides', 'LIB_Polyanionic', 'LIB_Spinel', 'LIB_Anode_Graphite_based', 'LIB_Anode_LTO']
   
   filtered_df = metrics_df[
       (metrics_df['provider_battery_type'] == 'LIB') & 
       (metrics_df['receiver_battery_type'] == 'LIB') &
       (metrics_df['provider_structure_family'].isin(lib_structures)) &
       (metrics_df['receiver_structure_family'].isin(lib_structures)) &
       (metrics_df['knowledge_percentage'] >= threshold)
   ]
   
   if filtered_df.empty:
       print(f"No data above threshold {threshold}% for {title}. Skipping.")
       return
   
   # Create filtered summary for LIB→LIB flows only
   filtered_summary = create_filtered_receiver_summary(metrics_df, 'LIB', 'LIB')
   
   color_map = create_structure_family_colors()
   process_levels = ['Product', 'Process', 'both']
   
   unique_providers = sorted([f for f in lib_structures if f in filtered_df['provider_structure_family'].unique()])
   unique_receivers = sorted([f for f in lib_structures if f in filtered_df['receiver_structure_family'].unique()])
   
   print(f"\nCreating LIB → LIB Corrected Sankey:")
   print(f"  LIB providers (aggregated): {unique_providers}")
   print(f"  LIB receivers (by process level): {unique_receivers}")
   print(f"  Flows above threshold {threshold}%: {len(filtered_df)}")
   
   # Create node mapping with percentage labels
   node_mapping = {}
   node_labels = []
   node_colors = []
   current_index = 0
   
   # Left side: LIB providers (aggregated)
   for family in unique_providers:
       node_mapping[f"provider_{family}"] = current_index
       node_labels.append("")
       node_colors.append(color_map.get(family, 'rgba(128,128,128,0.8)'))
       current_index += 1
   
   # Right side: LIB receivers (split by process level) with LIB-only percentage labels
   for family in unique_receivers:
       for level in process_levels:
           node_mapping[f"receiver_{family}_{level}"] = current_index
           
           # Get LIB-only percentage from filtered_summary
           summary_row = filtered_summary[
               (filtered_summary['receiver_structure_family'] == family) &
               (filtered_summary['receiver_process_level'] == level) &
               (filtered_summary['receiver_battery_type'] == 'LIB')
           ]
           
           if not summary_row.empty:
               percentage = summary_row.iloc[0]['filtered_percentage']
               # node_labels.append(f"[{percentage:.1f}%]")
               node_labels.append("")
           else:
               # node_labels.append("[0.0%]")
               node_labels.append("")
           
           node_colors.append(color_map.get(family, 'rgba(128,128,128,0.8)'))
           current_index += 1
   
   # Create links
   sources = []
   targets = []
   values = []
   link_colors = []
   
   for _, row in filtered_df.iterrows():
       provider_family = row['provider_structure_family']
       receiver_family = row['receiver_structure_family']
       receiver_level = row['receiver_process_level']
       
       if receiver_level not in process_levels:
           continue
       
       provider_key = f"provider_{provider_family}"
       receiver_key = f"receiver_{receiver_family}_{receiver_level}"
       
       if provider_key not in node_mapping or receiver_key not in node_mapping:
           continue
       
       source_idx = node_mapping[provider_key]
       target_idx = node_mapping[receiver_key]
       value = row['knowledge_percentage']
       
       if value <= 0:
           continue
       
       sources.append(source_idx)
       targets.append(target_idx)
       values.append(value)
       link_colors.append(color_map.get(provider_family, 'rgba(128,128,128,0.6)'))
   
   if not sources:
       print(f"No valid links for {title}. Skipping.")
       return
   
   # Create Sankey diagram
   fig = go.Figure(data=[go.Sankey(
       arrangement="snap",
       node=dict(
           pad=15,
           thickness=20,
           line=dict(color="black", width=0.5),
           label=node_labels,
           color=node_colors
       ),
       link=dict(
           source=sources,
           target=targets,
           value=values,
           color=link_colors
       )
   )])
   
   subtitle = f"LIB Structure Families → LIB Structure Families (by Process Level) - Corrected Normalization"
   
   fig.update_layout(
       title_text=f"{title}<br><sub>{subtitle}</sub>",
       font_size=14,
       width=1800,
       height=1200
   )
   
   # Save figure
   output_dir = "sankey_diagrams"
   os.makedirs(output_dir, exist_ok=True)
   output_path = os.path.join(output_dir, f"{filename}_corrected.html")
   fig.write_html(output_path)
   
   print(f"LIB → LIB Corrected Sankey saved to {output_path}")
   return fig

def create_sib_to_sib_corrected_sankey(metrics_df, receiver_summary, title, filename, threshold=0.0):
   """
   Creates corrected Sankey: SIB Structure Families (aggregated) → SIB Structure Families (by process level)
   """
   sib_structures = ['SIB_Layered_Oxides', 'SIB_Polyanionic', 'SIB_PBA', 'SIB_Anode_Hard_Carbon']
   
   filtered_df = metrics_df[
       (metrics_df['provider_battery_type'] == 'SIB') & 
       (metrics_df['receiver_battery_type'] == 'SIB') &
       (metrics_df['provider_structure_family'].isin(sib_structures)) &
       (metrics_df['receiver_structure_family'].isin(sib_structures)) &
       (metrics_df['knowledge_percentage'] >= threshold)
   ]
   
   if filtered_df.empty:
       print(f"No data above threshold {threshold}% for {title}. Skipping.")
       return
   
   # Create filtered summary for SIB→SIB flows only
   filtered_summary = create_filtered_receiver_summary(metrics_df, 'SIB', 'SIB')
   
   color_map = create_structure_family_colors()
   process_levels = ['Product', 'Process', 'both']
   
   unique_providers = sorted([f for f in sib_structures if f in filtered_df['provider_structure_family'].unique()])
   unique_receivers = sorted([f for f in sib_structures if f in filtered_df['receiver_structure_family'].unique()])
   
   print(f"\nCreating SIB → SIB Corrected Sankey:")
   print(f"  SIB providers (aggregated): {unique_providers}")
   print(f"  SIB receivers (by process level): {unique_receivers}")
   print(f"  Flows above threshold {threshold}%: {len(filtered_df)}")
   
   # Create node mapping with percentage labels
   node_mapping = {}
   node_labels = []
   node_colors = []
   current_index = 0
   
   # Left side: SIB providers (aggregated)
   for family in unique_providers:
       node_mapping[f"provider_{family}"] = current_index
       node_labels.append("")
       node_colors.append(color_map.get(family, 'rgba(128,128,128,0.8)'))
       current_index += 1
   
   # Right side: SIB receivers (split by process level) with SIB-only percentage labels
   for family in unique_receivers:
       for level in process_levels:
           node_mapping[f"receiver_{family}_{level}"] = current_index
           
           # Get SIB-only percentage from filtered_summary
           summary_row = filtered_summary[
               (filtered_summary['receiver_structure_family'] == family) &
               (filtered_summary['receiver_process_level'] == level) &
               (filtered_summary['receiver_battery_type'] == 'SIB')
           ]
           
           if not summary_row.empty:
               percentage = summary_row.iloc[0]['filtered_percentage']
               # node_labels.append(f"[{percentage:.1f}%]")
               node_labels.append("")
           else:
               # node_labels.append("[0.0%]")
               node_labels.append("")
           
           node_colors.append(color_map.get(family, 'rgba(128,128,128,0.8)'))
           current_index += 1
   
   # Create links
   sources = []
   targets = []
   values = []
   link_colors = []
   
   for _, row in filtered_df.iterrows():
       provider_family = row['provider_structure_family']
       receiver_family = row['receiver_structure_family']
       receiver_level = row['receiver_process_level']
       
       if receiver_level not in process_levels:
           continue
       
       provider_key = f"provider_{provider_family}"
       receiver_key = f"receiver_{receiver_family}_{receiver_level}"
       
       if provider_key not in node_mapping or receiver_key not in node_mapping:
           continue
       
       source_idx = node_mapping[provider_key]
       target_idx = node_mapping[receiver_key]
       value = row['knowledge_percentage']
       
       if value <= 0:
           continue
       
       sources.append(source_idx)
       targets.append(target_idx)
       values.append(value)
       link_colors.append(color_map.get(provider_family, 'rgba(128,128,128,0.6)'))
   
   if not sources:
       print(f"No valid links for {title}. Skipping.")
       return
   
   # Create Sankey diagram
   fig = go.Figure(data=[go.Sankey(
       arrangement="snap",
       node=dict(
           pad=15,
           thickness=20,
           line=dict(color="black", width=0.5),
           label=node_labels,
           color=node_colors
       ),
       link=dict(
           source=sources,
           target=targets,
           value=values,
           color=link_colors
       )
   )])
   
   subtitle = f"SIB Structure Families → SIB Structure Families (by Process Level) - Corrected Normalization"
   
   fig.update_layout(
       title_text=f"{title}<br><sub>{subtitle}</sub>",
       font_size=14,
       width=1800,
       height=1200
   )
   
   # Save figure
   output_dir = "sankey_diagrams"
   os.makedirs(output_dir, exist_ok=True)
   output_path = os.path.join(output_dir, f"{filename}_corrected.html")
   fig.write_html(output_path)
   
   print(f"SIB → SIB Corrected Sankey saved to {output_path}")
   return fig

def main_corrected_sankey_analysis():
    """
    Main function to create corrected Sankey diagrams with proper normalization.
    """
    print("Starting Corrected Sankey Analysis with Proper Normalization...")
    
    try:
        # Step 1: Calculate structure + process level backward citations
        print("\n📊 Step 1: Calculating structure + process level backward citations...")
        backward_citations_lookup, backward_citations_summary = calculate_structure_process_backward_citations(combined_df)
        
        # Step 2: Add product/process levels to structure citations
        print("\n📊 Step 2: Adding product/process levels to structure citations...")
        structure_citations_enhanced = add_product_process_levels_to_structure_citations(
            structure_citations_df, combined_df
        )
        
        # Step 3: Create corrected receiver-level metrics
        print("\n📊 Step 3: Creating corrected receiver-level knowledge metrics...")
        corrected_metrics, receiver_summary = create_corrected_receiver_level_knowledge_metrics(
            structure_citations_enhanced, backward_citations_lookup
        )
        
        # Step 4: Create corrected Sankey diagrams
        print("\n📊 Step 4: Creating corrected Sankey diagrams...")
        
        # LIB → SIB
        fig_lib_to_sib = create_lib_to_sib_corrected_sankey(
            corrected_metrics,
            receiver_summary,
            "LIB to SIB Knowledge Transfer (Corrected)",
            "LIB_to_SIB_Corrected_Sankey",
            threshold=0.0
        )
        
        # LIB → LIB
        fig_lib_to_lib = create_lib_to_lib_corrected_sankey(
            corrected_metrics,
            receiver_summary,
            "LIB Internal Knowledge Transfer (Corrected)",
            "LIB_to_LIB_Corrected_Sankey",
            threshold=0.0
        )
        
        # SIB → SIB
        fig_sib_to_sib = create_sib_to_sib_corrected_sankey(
            corrected_metrics,
            receiver_summary,
            "SIB Internal Knowledge Transfer (Corrected)",
            "SIB_to_SIB_Corrected_Sankey",
            threshold=0.0
        )
        
        print("\n📊 Step 5: Printing receiver percentage tables...")
        print_receiver_percentage_tables(corrected_metrics)
        
        print(f"\n✅ CORRECTED SANKEY ANALYSIS COMPLETE!")
        print(f"\n📁 Files created:")
        print(f"   - structure_patent_assignments.csv")
        print(f"   - structure_process_backward_citations.csv")
        print(f"   - corrected_receiver_level_knowledge_metrics.csv")
        print(f"   - receiver_summary_percentages.csv")
        print(f"   - sankey_diagrams/LIB_to_SIB_Corrected_Sankey_corrected.html")
        print(f"   - sankey_diagrams/LIB_to_LIB_Corrected_Sankey_corrected.html")
        print(f"   - sankey_diagrams/SIB_to_SIB_Corrected_Sankey_corrected.html")
        
        # Print summary statistics
        print(f"\n📈 SUMMARY STATISTICS:")
        print(f"   Total backward citations calculated: {len(backward_citations_summary)}")
        print(f"   Total receiver combinations: {len(receiver_summary)}")
        print(f"   Max percentage from others: {receiver_summary['total_from_others_percentage'].max():.2f}%")
        print(f"   Mean percentage from others: {receiver_summary['total_from_others_percentage'].mean():.2f}%")
        
        # Show top receivers by percentage
        print(f"\n🔝 TOP 10 RECEIVERS BY PERCENTAGE FROM OTHERS:")
        top_receivers = receiver_summary.nlargest(10, 'total_from_others_percentage')
        for i, (_, row) in enumerate(top_receivers.iterrows(), 1):
            print(f"   {i:2d}. {row['receiver_structure_family'].replace('_', ' '):20} "
                  f"({row['receiver_process_level']:7}) | {row['total_from_others_percentage']:6.2f}% | "
                  f"Citations: {row['citation_count']:4d} | Total: {row['total_backward_citations']:5d}")
        
    except Exception as e:
        print(f"❌ Error during analysis: {e}")
        import traceback
        traceback.print_exc()

# Run the analysis
if __name__ == "__main__":
    main_corrected_sankey_analysis()

#%%



#==================================================================================
# Modified Structure Family Level Sankey Diagram with Aggregated Receivers
# Both provider and receiver side: Aggregated structure families (no process level split)
# Simplified structure for better overview
#==================================================================================

import pandas as pd
import plotly.graph_objects as go
import numpy as np
import os
from plotly.subplots import make_subplots



def calculate_structure_backward_citations(combined_df):
    """
    Calculate total backward citations per structure family (aggregated across all process levels)
    from combined_df using material mappings.
    """
    print("Calculating aggregated structure family backward citations...")
    
    # Define structure family mappings
    structure_mappings = {
        'LIB_Layered_Oxides': ['LCO', 'NMC', 'NCA'],
        'LIB_Polyanionic': ['LFP'],
        'LIB_Spinel': ['LMO'],
        'LIB_Anode_Graphite_based': ['Graphite', 'Silicon/Carbon'],
        'LIB_Anode_LTO': ['LTO'],
        'SIB_Layered_Oxides': ['NMO', 'CFM', 'NFM'],
        'SIB_Polyanionic': ['NFPP/NFP', 'NVPF'],
        'SIB_PBA': ['Fe-PBA', 'Mn-PBA'],
        'SIB_Anode_Hard_Carbon': ['Hard Carbon']
    }
    
    # Prepare combined_df
    combined_enhanced = combined_df.copy()
    
    # Standardize Product-Process-Level
    combined_enhanced['Product-Process-Level'] = combined_enhanced['Product-Process-Level'].replace('Product, Process', 'both')
    
    # Create structure family assignments
    structure_assignments = []
    
    for idx, row in combined_enhanced.iterrows():
        if idx % 1000 == 0:
            print(f"Processing patent {idx+1}/{len(combined_enhanced)}")
        
        node_num = row['node_num']
        battery_type = row['battery_type']
        backward_citations = row['family_level_cited_patent_counts']
        
        # Get materials from both cathode and anode columns
        cathode_materials = row.get('Cathode Material', [])
        anode_materials = row.get('Anode Material', [])
        
        # Convert to lists if they're strings or other types
        if isinstance(cathode_materials, str):
            cathode_materials = [cathode_materials]
        elif not isinstance(cathode_materials, list):
            cathode_materials = []
            
        if isinstance(anode_materials, str):
            anode_materials = [anode_materials]
        elif not isinstance(anode_materials, list):
            anode_materials = []
        
        # Combine all materials for this patent
        all_materials = set(cathode_materials + anode_materials)
        
        # Find matching structure families
        assigned_structures = []
        for structure_family, structure_materials in structure_mappings.items():
            # Check if any of the structure's materials are in this patent's materials
            if any(material in all_materials for material in structure_materials):
                assigned_structures.append(structure_family)
        
        # Create entries for each assigned structure
        for structure_family in assigned_structures:
            structure_assignments.append({
                'node_num': node_num,
                'battery_type': battery_type,
                'structure_family': structure_family,
                'backward_citations': backward_citations
            })
    
    # Convert to DataFrame
    structure_df = pd.DataFrame(structure_assignments)
    
    # Group by structure family only (aggregated across process levels) and sum backward citations
    backward_citations_summary = structure_df.groupby(['structure_family']).agg({
        'backward_citations': 'sum',
        'node_num': 'count'  # Count of patents
    }).reset_index()
    
    backward_citations_summary.rename(columns={'node_num': 'patent_count'}, inplace=True)
    
    print(f"\nBackward citations summary:")
    print(f"Total structure families: {len(backward_citations_summary)}")
    print(f"Sample entries:")
    print(backward_citations_summary.head(10))
    
    # Create lookup dictionary
    backward_citations_lookup = {}
    for _, row in backward_citations_summary.iterrows():
        key = row['structure_family']
        backward_citations_lookup[key] = row['backward_citations']
    
    # Save detailed results
    structure_df.to_csv('aggregated_structure_patent_assignments.csv', index=False)
    backward_citations_summary.to_csv('aggregated_structure_backward_citations.csv', index=False)
    
    print(f"Detailed assignments saved to: aggregated_structure_patent_assignments.csv")
    print(f"Summary saved to: aggregated_structure_backward_citations.csv")
    
    return backward_citations_lookup, backward_citations_summary

def add_product_process_levels_to_structure_citations(structure_citations_df, combined_df):
    """Add product/process level information to structure_citations_df by matching patents."""
    print("Adding product/process level information to structure citations...")
    
    # Create lookup dictionaries from combined_df
    patent_to_product_process = combined_df.set_index('node_num')['Product-Process-Level'].to_dict()
    
    # Add product/process levels to structure_citations_df
    structure_citations_enhanced = structure_citations_df.copy()
    
    # Add cited (provider) product/process level
    structure_citations_enhanced['cited_product_process_level'] = structure_citations_enhanced['cited_patent'].map(patent_to_product_process)
    
    # Add citing (receiver) product/process level  
    structure_citations_enhanced['citing_product_process_level'] = structure_citations_enhanced['citing_patent'].map(patent_to_product_process)
    
    # Standardize "Product, Process" to "both"
    structure_citations_enhanced['cited_product_process_level'] = structure_citations_enhanced['cited_product_process_level'].replace('Product, Process', 'both')
    structure_citations_enhanced['citing_product_process_level'] = structure_citations_enhanced['citing_product_process_level'].replace('Product, Process', 'both')
    
    # Remove rows with missing product/process information
    structure_citations_enhanced = structure_citations_enhanced.dropna(subset=['cited_product_process_level', 'citing_product_process_level'])
    
    print(f"Final dataset: {len(structure_citations_enhanced)} citations with complete product/process information")
    
    return structure_citations_enhanced

def create_aggregated_receiver_knowledge_metrics(structure_citations_enhanced, backward_citations_lookup):
    """
    Create citation metrics with aggregated receivers (no process level split).
    """
    print("Creating aggregated receiver-level knowledge metrics...")
    
    # Group by aggregated receiver (structure only) and provider (structure only)
    metrics_data = []
    
    # Get unique receiver structure families
    receiver_families = structure_citations_enhanced['citing_structure_family'].unique()
    
    print(f"Processing {len(receiver_families)} unique receiver structure families...")
    
    for receiver_family in receiver_families:
        # Get total backward citations for this receiver structure family from lookup
        total_backward_citations = backward_citations_lookup.get(receiver_family, 0)
        
        if total_backward_citations == 0:
            print(f"Warning: No backward citations found for {receiver_family}")
            continue
        
        # Get ALL citations for this receiver structure family (across all process levels)
        receiver_citations = structure_citations_enhanced[
            structure_citations_enhanced['citing_structure_family'] == receiver_family
        ]
        
        if len(receiver_citations) == 0:
            continue
        
        # Get receiver battery type
        receiver_battery_type = receiver_citations['citing_battery_type'].iloc[0]
        
        # Group by provider structure family, excluding same structure for intra-battery flows
        if receiver_battery_type == 'LIB':
            # For LIB receivers, exclude same LIB structure
            other_citations = receiver_citations[
                ~((receiver_citations['cited_battery_type'] == 'LIB') & 
                  (receiver_citations['cited_structure_family'] == receiver_family))
            ]
        else:  # SIB
            # For SIB receivers, exclude same SIB structure
            other_citations = receiver_citations[
                ~((receiver_citations['cited_battery_type'] == 'SIB') & 
                  (receiver_citations['cited_structure_family'] == receiver_family))
            ]
        
        # Group by provider structure family (aggregated across all process levels)
        provider_groups = other_citations.groupby(['cited_structure_family', 'cited_battery_type']).agg({
            'individual_citation_count': 'sum'
        }).reset_index()
        
        # Create metrics for each provider
        for _, provider_row in provider_groups.iterrows():
            provider_family = provider_row['cited_structure_family']
            provider_battery_type = provider_row['cited_battery_type']
            citation_count = provider_row['individual_citation_count']
            
            # Calculate knowledge percentage with aggregated normalization
            knowledge_percentage = (citation_count / total_backward_citations) * 100
            
            metrics_data.append({
                'provider_structure_family': provider_family,
                'provider_battery_type': provider_battery_type,
                'receiver_structure_family': receiver_family,
                'receiver_battery_type': receiver_battery_type,
                'citation_count': citation_count,
                'total_backward_citations': total_backward_citations,
                'knowledge_percentage': knowledge_percentage
            })
    
    metrics_df = pd.DataFrame(metrics_data)
    
    print(f"Created {len(metrics_df)} provider-receiver relationships")
    
    # Calculate summary percentages for each receiver node
    receiver_summary = metrics_df.groupby(['receiver_structure_family', 'receiver_battery_type']).agg({
        'knowledge_percentage': 'sum',
        'citation_count': 'sum',
        'total_backward_citations': 'first'
    }).reset_index()
    
    receiver_summary.rename(columns={'knowledge_percentage': 'total_from_others_percentage'}, inplace=True)
    
    print(f"\nReceiver summary statistics:")
    print(f"Max percentage from others: {receiver_summary['total_from_others_percentage'].max():.2f}%")
    print(f"Mean percentage from others: {receiver_summary['total_from_others_percentage'].mean():.2f}%")
    
    # Save results
    metrics_df.to_csv('aggregated_receiver_knowledge_metrics.csv', index=False)
    receiver_summary.to_csv('aggregated_receiver_summary_percentages.csv', index=False)
    
    print("Aggregated metrics saved to: aggregated_receiver_knowledge_metrics.csv")
    print("Receiver summary saved to: aggregated_receiver_summary_percentages.csv")
    
    return metrics_df, receiver_summary

def create_filtered_receiver_summary(metrics_df, battery_filter_provider, battery_filter_receiver):
    """
    Create receiver summary with only specific provider-receiver combinations.
    """
    # Filter for specific flow type
    if battery_filter_provider == battery_filter_receiver:
        # Intra-battery flow (e.g. SIB→SIB): Exclude same structure
        filtered_metrics = metrics_df[
            (metrics_df['provider_battery_type'] == battery_filter_provider) & 
            (metrics_df['receiver_battery_type'] == battery_filter_receiver)
        ]
    else:
        # Inter-battery flow (e.g. LIB→SIB): Include all
        filtered_metrics = metrics_df[
            (metrics_df['provider_battery_type'] == battery_filter_provider) & 
            (metrics_df['receiver_battery_type'] == battery_filter_receiver)
        ]
    
    # Group and sum only filtered flows
    filtered_summary = filtered_metrics.groupby([
        'receiver_structure_family', 
        'receiver_battery_type'
    ]).agg({
        'knowledge_percentage': 'sum',
        'total_backward_citations': 'first'
    }).reset_index()
    
    filtered_summary.rename(columns={'knowledge_percentage': 'filtered_percentage'}, inplace=True)
    
    return filtered_summary

def create_structure_family_colors():
    """Create color mappings for structure families."""
    return {
        # LIB Structure Families
        'LIB_Layered_Oxides': 'rgba(255,0,0,0.8)',      # Red
        'LIB_Polyanionic': 'rgba(0,0,255,0.8)',         # Blue  
        'LIB_Spinel': 'rgba(0,255,255,0.8)',            # Cyan
        'LIB_Anode_Graphite_based': 'rgba(102,102,102,0.8)', # Gray
        'LIB_Anode_LTO': 'rgba(255,165,0,0.8)',         # Orange
        
        # SIB Structure Families
        'SIB_Layered_Oxides': 'rgba(255,102,102,0.8)',  # Light Red
        'SIB_Polyanionic': 'rgba(0,102,255,0.8)',       # Medium Blue
        'SIB_PBA': 'rgba(0,204,0,0.8)',                 # Green
        'SIB_Anode_Hard_Carbon': 'rgba(153,153,153,0.8)' # Light Gray
    }

def print_aggregated_receiver_percentage_tables(metrics_df):
    """
    Print percentage tables for each aggregated receiver by flow type in console.
    """
    print("\n" + "="*80)
    print("AGGREGATED RECEIVER PERCENTAGE TABLES")
    print("="*80)
    
    # Define flow types
    flow_types = [
        ('LIB', 'LIB', 'LIB → LIB Knowledge Transfer'),
        ('LIB', 'SIB', 'LIB → SIB Knowledge Transfer'), 
        ('SIB', 'SIB', 'SIB → SIB Knowledge Transfer')
    ]
    
    for provider_type, receiver_type, flow_name in flow_types:
        print(f"\n🔄 {flow_name.upper()}:")
        print("-" * 60)
        
        # Create filtered summary for this flow type
        filtered_summary = create_filtered_receiver_summary(metrics_df, provider_type, receiver_type)
        
        if filtered_summary.empty:
            print("   No data available for this flow type.")
            continue
        
        # Sort by percentage (descending)
        filtered_summary_sorted = filtered_summary.sort_values('filtered_percentage', ascending=False)
        
        print(f"{'Receiver Structure':<35} {'Percentage':<12} {'Citations':<10}")
        print("-" * 60)
        
        for _, row in filtered_summary_sorted.iterrows():
            structure = row['receiver_structure_family'].replace('_', ' ')
            percentage = row['filtered_percentage']
            citations = row['total_backward_citations']
            
            print(f"{structure:<35} {percentage:>8.1f}%    {citations:>8.0f}")
        
        # Summary statistics
        max_pct = filtered_summary['filtered_percentage'].max()
        mean_pct = filtered_summary['filtered_percentage'].mean()
        print("-" * 60)
        print(f"Maximum: {max_pct:.1f}% | Average: {mean_pct:.1f}% | Total entries: {len(filtered_summary)}")
    
    print("\n" + "="*80)

def create_lib_to_sib_aggregated_sankey(metrics_df, receiver_summary, title, filename, threshold=0.0):
    """
    Creates aggregated Sankey: LIB Structure Families → SIB Structure Families
    Both sides aggregated (no process level split).
    """
    # Filter for LIB to SIB flows
    lib_structures = ['LIB_Layered_Oxides', 'LIB_Polyanionic', 'LIB_Spinel', 'LIB_Anode_Graphite_based', 'LIB_Anode_LTO']
    sib_structures = ['SIB_Layered_Oxides', 'SIB_Polyanionic', 'SIB_PBA', 'SIB_Anode_Hard_Carbon']
    
    filtered_df = metrics_df[
        (metrics_df['provider_battery_type'] == 'LIB') & 
        (metrics_df['receiver_battery_type'] == 'SIB') &
        (metrics_df['provider_structure_family'].isin(lib_structures)) &
        (metrics_df['receiver_structure_family'].isin(sib_structures)) &
        (metrics_df['knowledge_percentage'] >= threshold)
    ]
    
    if filtered_df.empty:
        print(f"No data above threshold {threshold}% for {title}. Skipping.")
        return
    
    # Create filtered summary for LIB→SIB flows only
    filtered_summary = create_filtered_receiver_summary(metrics_df, 'LIB', 'SIB')
    
    color_map = create_structure_family_colors()
    
    # Get unique families present in data
    unique_providers = sorted([f for f in lib_structures if f in filtered_df['provider_structure_family'].unique()])
    unique_receivers = sorted([f for f in sib_structures if f in filtered_df['receiver_structure_family'].unique()])
    
    print(f"\nCreating LIB → SIB Aggregated Sankey:")
    print(f"  LIB providers: {unique_providers}")
    print(f"  SIB receivers: {unique_receivers}")
    print(f"  Flows above threshold {threshold}%: {len(filtered_df)}")
    
    # Create node mapping
    node_mapping = {}
    node_labels = []
    node_colors = []
    current_index = 0
    
    # Left side: LIB providers
    for family in unique_providers:
        node_mapping[f"provider_{family}"] = current_index
        clean_name = family.replace('LIB_', '').replace('_', ' ')
        node_labels.append("")
        node_colors.append(color_map.get(family, 'rgba(128,128,128,0.8)'))
        current_index += 1
    
    # Right side: SIB receivers with percentage labels
    for family in unique_receivers:
        node_mapping[f"receiver_{family}"] = current_index
        
        # Get LIB-only percentage from filtered_summary
        summary_row = filtered_summary[
            filtered_summary['receiver_structure_family'] == family
        ]
        
        clean_name = family.replace('SIB_', '').replace('_', ' ')
        if not summary_row.empty:
            percentage = summary_row.iloc[0]['filtered_percentage']
            node_labels.append("")
        else:
            node_labels.append("")
        
        node_colors.append(color_map.get(family, 'rgba(128,128,128,0.8)'))
        current_index += 1
    
    # Create links
    sources = []
    targets = []
    values = []
    link_colors = []
    
    for _, row in filtered_df.iterrows():
        provider_family = row['provider_structure_family']
        receiver_family = row['receiver_structure_family']
        
        provider_key = f"provider_{provider_family}"
        receiver_key = f"receiver_{receiver_family}"
        
        if provider_key not in node_mapping or receiver_key not in node_mapping:
            continue
        
        source_idx = node_mapping[provider_key]
        target_idx = node_mapping[receiver_key]
        value = row['knowledge_percentage']
        
        if value <= 0:
            continue
        
        sources.append(source_idx)
        targets.append(target_idx)
        values.append(value)
        link_colors.append(color_map.get(provider_family, 'rgba(128,128,128,0.6)'))
    
    if not sources:
        print(f"No valid links for {title}. Skipping.")
        return
    
    # Create Sankey diagram
    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=15,
            thickness=25,
            line=dict(color="black", width=0.5),
            label=node_labels,
            color=node_colors
        ),
        link=dict(
            source=sources,
            target=targets,
            value=values,
            color=link_colors
        )
    )])
    
    subtitle = f"LIB Structure Families → SIB Structure Families (Aggregated)"
    
    fig.update_layout(
        title_text=f"{title}<br><sub>{subtitle}</sub>",
        font_size=14,
        width=1400,
        height=800
    )
    
    # Save figure
    output_dir = "sankey_diagrams"
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"{filename}_aggregated.html")
    fig.write_html(output_path)
    
    print(f"LIB → SIB Aggregated Sankey saved to {output_path}")
    return fig

def create_lib_to_lib_aggregated_sankey(metrics_df, receiver_summary, title, filename, threshold=0.0):
    """
    Creates aggregated Sankey: LIB Structure Families → LIB Structure Families
    Both sides aggregated (no process level split).
    """
    lib_structures = ['LIB_Layered_Oxides', 'LIB_Polyanionic', 'LIB_Spinel', 'LIB_Anode_Graphite_based', 'LIB_Anode_LTO']
    
    filtered_df = metrics_df[
        (metrics_df['provider_battery_type'] == 'LIB') & 
        (metrics_df['receiver_battery_type'] == 'LIB') &
        (metrics_df['provider_structure_family'].isin(lib_structures)) &
        (metrics_df['receiver_structure_family'].isin(lib_structures)) &
        (metrics_df['knowledge_percentage'] >= threshold)
    ]
    
    if filtered_df.empty:
        print(f"No data above threshold {threshold}% for {title}. Skipping.")
        return
    
    # Create filtered summary for LIB→LIB flows only
    filtered_summary = create_filtered_receiver_summary(metrics_df, 'LIB', 'LIB')
    
    color_map = create_structure_family_colors()
    
    unique_providers = sorted([f for f in lib_structures if f in filtered_df['provider_structure_family'].unique()])
    unique_receivers = sorted([f for f in lib_structures if f in filtered_df['receiver_structure_family'].unique()])
    
    print(f"\nCreating LIB → LIB Aggregated Sankey:")
    print(f"  LIB providers: {unique_providers}")
    print(f"  LIB receivers: {unique_receivers}")
    print(f"  Flows above threshold {threshold}%: {len(filtered_df)}")
    
    # Create node mapping
    node_mapping = {}
    node_labels = []
    node_colors = []
    current_index = 0
    
    # Left side: LIB providers
    for family in unique_providers:
        node_mapping[f"provider_{family}"] = current_index
        clean_name = family.replace('LIB_', '').replace('_', ' ')
        node_labels.append("")
        node_colors.append(color_map.get(family, 'rgba(128,128,128,0.8)'))
        current_index += 1
    
    # Right side: LIB receivers with percentage labels
    for family in unique_receivers:
        node_mapping[f"receiver_{family}"] = current_index
        
        # Get LIB-only percentage from filtered_summary
        summary_row = filtered_summary[
            (filtered_summary['receiver_structure_family'] == family) &
            (filtered_summary['receiver_battery_type'] == 'LIB')
        ]
        
        clean_name = family.replace('LIB_', '').replace('_', ' ')
        if not summary_row.empty:
            percentage = summary_row.iloc[0]['filtered_percentage']
            node_labels.append("")
        else:
            node_labels.append("")
        
        node_colors.append(color_map.get(family, 'rgba(128,128,128,0.8)'))
        current_index += 1
    
    # Create links
    sources = []
    targets = []
    values = []
    link_colors = []
    
    for _, row in filtered_df.iterrows():
        provider_family = row['provider_structure_family']
        receiver_family = row['receiver_structure_family']
        
        provider_key = f"provider_{provider_family}"
        receiver_key = f"receiver_{receiver_family}"
        
        if provider_key not in node_mapping or receiver_key not in node_mapping:
            continue
        
        source_idx = node_mapping[provider_key]
        target_idx = node_mapping[receiver_key]
        value = row['knowledge_percentage']
        
        if value <= 0:
            continue
        
        sources.append(source_idx)
        targets.append(target_idx)
        values.append(value)
        link_colors.append(color_map.get(provider_family, 'rgba(128,128,128,0.6)'))
    
    if not sources:
        print(f"No valid links for {title}. Skipping.")
        return
    
    # Create Sankey diagram
    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=15,
            thickness=25,
            line=dict(color="black", width=0.5),
            label=node_labels,
            color=node_colors
        ),
        link=dict(
            source=sources,
            target=targets,
            value=values,
            color=link_colors
        )
    )])
    
    subtitle = f"LIB Structure Families → LIB Structure Families (Aggregated)"
    
    fig.update_layout(
        title_text=f"{title}<br><sub>{subtitle}</sub>",
        font_size=14,
        width=1400,
        height=800
    )
    
    # Save figure
    output_dir = "sankey_diagrams"
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"{filename}_aggregated.html")
    fig.write_html(output_path)
    
    print(f"LIB → LIB Aggregated Sankey saved to {output_path}")
    return fig

def create_sib_to_sib_aggregated_sankey(metrics_df, receiver_summary, title, filename, threshold=0.0):
    """
    Creates aggregated Sankey: SIB Structure Families → SIB Structure Families
    Both sides aggregated (no process level split).
    """
    sib_structures = ['SIB_Layered_Oxides', 'SIB_Polyanionic', 'SIB_PBA', 'SIB_Anode_Hard_Carbon']
    
    filtered_df = metrics_df[
        (metrics_df['provider_battery_type'] == 'SIB') & 
        (metrics_df['receiver_battery_type'] == 'SIB') &
        (metrics_df['provider_structure_family'].isin(sib_structures)) &
        (metrics_df['receiver_structure_family'].isin(sib_structures)) &
        (metrics_df['knowledge_percentage'] >= threshold)
    ]
    
    if filtered_df.empty:
        print(f"No data above threshold {threshold}% for {title}. Skipping.")
        return
    
    # Create filtered summary for SIB→SIB flows only
    filtered_summary = create_filtered_receiver_summary(metrics_df, 'SIB', 'SIB')
    
    color_map = create_structure_family_colors()
    
    unique_providers = sorted([f for f in sib_structures if f in filtered_df['provider_structure_family'].unique()])
    unique_receivers = sorted([f for f in sib_structures if f in filtered_df['receiver_structure_family'].unique()])
    
    print(f"\nCreating SIB → SIB Aggregated Sankey:")
    print(f"  SIB providers: {unique_providers}")
    print(f"  SIB receivers: {unique_receivers}")
    print(f"  Flows above threshold {threshold}%: {len(filtered_df)}")
    
    # Create node mapping
    node_mapping = {}
    node_labels = []
    node_colors = []
    current_index = 0
    
    # Left side: SIB providers
    for family in unique_providers:
        node_mapping[f"provider_{family}"] = current_index
        clean_name = family.replace('SIB_', '').replace('_', ' ')
        node_labels.append("")
        node_colors.append(color_map.get(family, 'rgba(128,128,128,0.8)'))
        current_index += 1
    
    # Right side: SIB receivers with percentage labels
    for family in unique_receivers:
        node_mapping[f"receiver_{family}"] = current_index
        
        # Get SIB-only percentage from filtered_summary
        summary_row = filtered_summary[
            (filtered_summary['receiver_structure_family'] == family) &
            (filtered_summary['receiver_battery_type'] == 'SIB')
        ]
        
        clean_name = family.replace('SIB_', '').replace('_', ' ')
        if not summary_row.empty:
            percentage = summary_row.iloc[0]['filtered_percentage']
            node_labels.append("")
        else:
            node_labels.append("")
        
        node_colors.append(color_map.get(family, 'rgba(128,128,128,0.8)'))
        current_index += 1
    
    # Create links
    sources = []
    targets = []
    values = []
    link_colors = []
    
    for _, row in filtered_df.iterrows():
        provider_family = row['provider_structure_family']
        receiver_family = row['receiver_structure_family']
        
        provider_key = f"provider_{provider_family}"
        receiver_key = f"receiver_{receiver_family}"
        
        if provider_key not in node_mapping or receiver_key not in node_mapping:
            continue
        
        source_idx = node_mapping[provider_key]
        target_idx = node_mapping[receiver_key]
        value = row['knowledge_percentage']
        
        if value <= 0:
            continue
        
        sources.append(source_idx)
        targets.append(target_idx)
        values.append(value)
        link_colors.append(color_map.get(provider_family, 'rgba(128,128,128,0.6)'))
    
    if not sources:
        print(f"No valid links for {title}. Skipping.")
        return
    
    # Create Sankey diagram
    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=15,
            thickness=25,
            line=dict(color="black", width=0.5),
            label=node_labels,
            color=node_colors
        ),
        link=dict(
            source=sources,
            target=targets,
            value=values,
            color=link_colors
        )
    )])
    
    subtitle = f"SIB Structure Families → SIB Structure Families (Aggregated)"
    
    fig.update_layout(
        title_text=f"{title}<br><sub>{subtitle}</sub>",
        font_size=14,
        width=1400,
        height=800
    )
    
    # Save figure
    output_dir = "sankey_diagrams"
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"{filename}_aggregated.html")
    fig.write_html(output_path)
    
    print(f"SIB → SIB Aggregated Sankey saved to {output_path}")
    return fig

def main_aggregated_sankey_analysis():
    """
    Main function to create aggregated Sankey diagrams with both providers and receivers aggregated.
    """
    print("Starting Aggregated Sankey Analysis (Both Sides Aggregated)...")
    
    try:
        # Step 1: Calculate aggregated structure family backward citations
        print("\n📊 Step 1: Calculating aggregated structure family backward citations...")
        backward_citations_lookup, backward_citations_summary = calculate_structure_backward_citations(combined_df)
        
        # Step 2: Add product/process levels to structure citations
        print("\n📊 Step 2: Adding product/process levels to structure citations...")
        structure_citations_enhanced = add_product_process_levels_to_structure_citations(
            structure_citations_df, combined_df
        )
        
        # Step 3: Create aggregated receiver-level metrics
        print("\n📊 Step 3: Creating aggregated receiver-level knowledge metrics...")
        aggregated_metrics, receiver_summary = create_aggregated_receiver_knowledge_metrics(
            structure_citations_enhanced, backward_citations_lookup
        )
        
        # Step 4: Create aggregated Sankey diagrams
        print("\n📊 Step 4: Creating aggregated Sankey diagrams...")
        
        # LIB → SIB
        fig_lib_to_sib = create_lib_to_sib_aggregated_sankey(
            aggregated_metrics,
            receiver_summary,
            "LIB to SIB Knowledge Transfer (Aggregated)",
            "LIB_to_SIB_Aggregated_Sankey",
            threshold=0.0
        )
        
        # LIB → LIB
        fig_lib_to_lib = create_lib_to_lib_aggregated_sankey(
            aggregated_metrics,
            receiver_summary,
            "LIB Internal Knowledge Transfer (Aggregated)",
            "LIB_to_LIB_Aggregated_Sankey",
            threshold=0.0
        )
        
        # SIB → SIB
        fig_sib_to_sib = create_sib_to_sib_aggregated_sankey(
            aggregated_metrics,
            receiver_summary,
            "SIB Internal Knowledge Transfer (Aggregated)",
            "SIB_to_SIB_Aggregated_Sankey",
            threshold=0.0
        )
        
        print("\n📊 Step 5: Printing aggregated receiver percentage tables...")
        print_aggregated_receiver_percentage_tables(aggregated_metrics)
        
        print(f"\n✅ AGGREGATED SANKEY ANALYSIS COMPLETE!")
        print(f"\n📁 Files created:")
        print(f"   - aggregated_structure_patent_assignments.csv")
        print(f"   - aggregated_structure_backward_citations.csv")
        print(f"   - aggregated_receiver_knowledge_metrics.csv")
        print(f"   - aggregated_receiver_summary_percentages.csv")
        print(f"   - sankey_diagrams/LIB_to_SIB_Aggregated_Sankey_aggregated.html")
        print(f"   - sankey_diagrams/LIB_to_LIB_Aggregated_Sankey_aggregated.html")
        print(f"   - sankey_diagrams/SIB_to_SIB_Aggregated_Sankey_aggregated.html")
        
        # Print summary statistics
        print(f"\n📈 SUMMARY STATISTICS:")
        print(f"   Total backward citations calculated: {len(backward_citations_summary)}")
        print(f"   Total receiver combinations: {len(receiver_summary)}")
        print(f"   Max percentage from others: {receiver_summary['total_from_others_percentage'].max():.2f}%")
        print(f"   Mean percentage from others: {receiver_summary['total_from_others_percentage'].mean():.2f}%")
        
        # Show top receivers by percentage
        print(f"\n🔝 TOP 10 RECEIVERS BY PERCENTAGE FROM OTHERS:")
        top_receivers = receiver_summary.nlargest(10, 'total_from_others_percentage')
        for i, (_, row) in enumerate(top_receivers.iterrows(), 1):
            print(f"   {i:2d}. {row['receiver_structure_family'].replace('_', ' '):25} "
                  f"| {row['total_from_others_percentage']:6.2f}% | "
                  f"Citations: {row['citation_count']:4d} | Total: {row['total_backward_citations']:5d}")
        
        # Print comparison of flows
        print(f"\n🔄 FLOW COMPARISON:")
        
        # LIB → SIB flows
        lib_to_sib = aggregated_metrics[
            (aggregated_metrics['provider_battery_type'] == 'LIB') & 
            (aggregated_metrics['receiver_battery_type'] == 'SIB')
        ]
        print(f"   LIB → SIB flows: {len(lib_to_sib)} connections")
        if len(lib_to_sib) > 0:
            print(f"     Max flow: {lib_to_sib['knowledge_percentage'].max():.2f}%")
            print(f"     Avg flow: {lib_to_sib['knowledge_percentage'].mean():.2f}%")
        
        # LIB → LIB flows
        lib_to_lib = aggregated_metrics[
            (aggregated_metrics['provider_battery_type'] == 'LIB') & 
            (aggregated_metrics['receiver_battery_type'] == 'LIB')
        ]
        print(f"   LIB → LIB flows: {len(lib_to_lib)} connections")
        if len(lib_to_lib) > 0:
            print(f"     Max flow: {lib_to_lib['knowledge_percentage'].max():.2f}%")
            print(f"     Avg flow: {lib_to_lib['knowledge_percentage'].mean():.2f}%")
        
        # SIB → SIB flows
        sib_to_sib = aggregated_metrics[
            (aggregated_metrics['provider_battery_type'] == 'SIB') & 
            (aggregated_metrics['receiver_battery_type'] == 'SIB')
        ]
        print(f"   SIB → SIB flows: {len(sib_to_sib)} connections")
        if len(sib_to_sib) > 0:
            print(f"     Max flow: {sib_to_sib['knowledge_percentage'].max():.2f}%")
            print(f"     Avg flow: {sib_to_sib['knowledge_percentage'].mean():.2f}%")
        
    except Exception as e:
        print(f"❌ Error during analysis: {e}")
        import traceback
        traceback.print_exc()

# Run the analysis
if __name__ == "__main__":
    main_aggregated_sankey_analysis()
